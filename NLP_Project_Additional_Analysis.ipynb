{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "gpuType": "L4", "collapsed_sections": ["Q4qrTqYjmUkW", "OiFKn3R2mrtF", "KN12uHaWJbwY", "_dVDrC1a3elY"], "machine_shape": "hm"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "accelerator": "GPU", "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}}}}, "cells": [{"cell_type": "markdown", "source": ["# Phase 0: Environment Setup & Verification"], "metadata": {"id": "Q4qrTqYjmUkW"}}, {"cell_type": "markdown", "source": ["## 0.2: Mount Google Drive"], "metadata": {"id": "OiFKn3R2mrtF"}}, {"cell_type": "code", "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7UIp6ZhWvlvD", "outputId": "881ffb54-3a4e-4455-8126-d26a2dcd8814"}, "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Mounted at /content/drive\n"]}]}, {"cell_type": "code", "source": ["from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Create project directory in Drive\n", "import os\n", "project_path = '/content/drive/MyDrive/NLP_Project'\n", "os.makedirs(project_path, exist_ok=True)\n", "os.makedirs(f'{project_path}/data', exist_ok=True)\n", "os.makedirs(f'{project_path}/models', exist_ok=True)\n", "os.makedirs(f'{project_path}/results', exist_ok=True)\n", "os.makedirs(f'{project_path}/checkpoints', exist_ok=True)\n", "\n", "print(f\"\u2713 Project directory created at: {project_path}\")\n", "print(f\"\\nDirectory structure:\")\n", "!ls -la /content/drive/MyDrive/NLP_Project/"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Wkgqw1THg0DS", "outputId": "2edf061d-4fab-4b35-dbf8-f42e1f843b51"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n", "\u2713 Project directory created at: /content/drive/MyDrive/NLP_Project\n", "\n", "Directory structure:\n", "total 16\n", "drwx------ 2 root root 4096 Dec 10 00:42 checkpoints\n", "drwx------ 2 root root 4096 Nov 19 01:28 data\n", "drwx------ 2 root root 4096 Dec 10 00:13 models\n", "drwx------ 2 root root 4096 Nov 19 01:28 results\n"]}]}, {"cell_type": "markdown", "source": ["## 0.3: Install Required Libraries"], "metadata": {"id": "VG4GckODms6r"}}, {"cell_type": "code", "source": ["print(\"Installing required packages...\")\n", "!pip install -q peft accelerate bitsandbytes\n", "!pip install -q sentence-transformers faiss-cpu\n", "!pip install -q rouge-score bert-score\n", "!pip install -q datasets\n", "!pip install -U bitsandbytes accelerate\n", "\n", "print(\"\\n\" + \"=\"*50)\n", "print(\"VERIFYING INSTALLATIONS\")\n", "print(\"=\"*50)\n", "\n", "# Verify installations\n", "import pandas as pd\n", "import numpy as np\n", "import torch\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "import transformers\n", "import peft\n", "import sentence_transformers\n", "from sentence_transformers import SentenceTransformer\n", "from datasets import load_dataset\n", "\n", "print(\"\u2713 All core libraries imported successfully!\")\n", "print(f\"\\nLibrary versions:\")\n", "print(f\"  PyTorch: {torch.__version__}\")\n", "print(f\"  Transformers: {transformers.__version__}\")\n", "print(f\"  PEFT: {peft.__version__}\")\n", "print(f\"  Sentence Transformers: {sentence_transformers.__version__}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lsxHbzmThMMb", "outputId": "5cb3b463-c50b-471d-f241-74faa855e3ac"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Installing required packages...\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n", "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n", "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n", "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n", "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n", "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n", "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n", "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n", "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n", "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n", "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n", "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n", "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n", "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n", "\n", "==================================================\n", "VERIFYING INSTALLATIONS\n", "==================================================\n", "\u2713 All core libraries imported successfully!\n", "\n", "Library versions:\n", "  PyTorch: 2.9.0+cu126\n", "  Transformers: 4.57.3\n", "  PEFT: 0.18.0\n", "  Sentence Transformers: 5.2.0\n"]}]}, {"cell_type": "markdown", "source": ["# Hybrid System"], "metadata": {"id": "KN12uHaWJbwY"}}, {"cell_type": "code", "source": ["import torch\n", "import pandas as pd\n", "import time\n", "import pickle\n", "import faiss\n", "from sentence_transformers import SentenceTransformer\n", "from peft import PeftModel\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "\n", "print(\"=\"*60)\n", "print(\"4.1: End-to-End Pipeline System\")\n", "print(\"=\"*60)\n", "\n", "# Build complete pipeline class\n", "class HybridChatbot:\n", "    def __init__(self):\n", "        print(\"\\n1. Loading all components...\")\n", "\n", "        # Classifier\n", "        print(\"   - Loading classifier...\")\n", "        with open('/content/drive/MyDrive/NLP_Project/models/classifier/logistic_regression.pkl', 'rb') as f:\n", "            self.classifier = pickle.load(f)\n", "        with open('/content/drive/MyDrive/NLP_Project/models/classifier/tfidf_vectorizer.pkl', 'rb') as f:\n", "            self.tfidf = pickle.load(f)\n", "\n", "        # Retrieval system\n", "        print(\"   - Loading retrieval system...\")\n", "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n", "        self.retrieval_index = faiss.read_index('/content/drive/MyDrive/NLP_Project/models/retrieval/faiss_index.bin')\n", "        self.retrieval_data = pd.read_csv('/content/drive/MyDrive/NLP_Project/models/retrieval/deterministic_qa_pairs.csv')\n", "\n", "        # LLM\n", "        print(\"   - Loading fine-tuned LLM...\")\n", "        base_model = AutoModelForCausalLM.from_pretrained(\n", "            \"microsoft/phi-2\",\n", "            device_map=\"auto\",\n", "            trust_remote_code=True,\n", "            torch_dtype=torch.float16\n", "        )\n", "        self.llm_model = PeftModel.from_pretrained(\n", "            base_model,\n", "            \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", "        )\n", "        self.llm_tokenizer = AutoTokenizer.from_pretrained(\n", "            \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", "        )\n", "\n", "        print(\"   \u2713 All components loaded!\\n\")\n", "\n", "    def classify_query(self, query):\n", "        \"\"\"Returns 0 for deterministic, 1 for indeterministic\"\"\"\n", "        query_tfidf = self.tfidf.transform([query])\n", "        return self.classifier.predict(query_tfidf)[0]\n", "\n", "    def retrieve_response(self, query, k=1):\n", "        \"\"\"Semantic search for deterministic queries\"\"\"\n", "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n", "        distances, indices = self.retrieval_index.search(query_embedding.astype('float32'), k)\n", "        return self.retrieval_data.iloc[indices[0][0]]['response'], distances[0][0]\n", "\n", "    def generate_response(self, query, max_tokens=150):\n", "        \"\"\"LLM generation for indeterministic queries\"\"\"\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n", "\n", "        with torch.no_grad():\n", "            outputs = self.llm_model.generate(\n", "                **inputs,\n", "                max_new_tokens=max_tokens,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=self.llm_tokenizer.eos_token_id\n", "            )\n", "\n", "        response = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        return response.split(\"Assistant:\")[-1].strip()\n", "\n", "    def respond(self, query):\n", "        \"\"\"Main pipeline: classify \u2192 route \u2192 respond\"\"\"\n", "        start_time = time.time()\n", "\n", "        # Step 1: Classify\n", "        prediction = self.classify_query(query)\n", "        route = \"RETRIEVAL\" if prediction == 0 else \"LLM_GENERATION\"\n", "\n", "        # Step 2: Get response\n", "        if prediction == 0:  # Deterministic\n", "            response, distance = self.retrieve_response(query)\n", "            confidence = 1.0 / (1.0 + distance)  # Convert distance to confidence\n", "        else:  # Indeterministic\n", "            response = self.generate_response(query)\n", "            confidence = None\n", "\n", "        latency = (time.time() - start_time) * 1000\n", "\n", "        return {\n", "            'query': query,\n", "            'route': route,\n", "            'response': response,\n", "            'latency_ms': latency,\n", "            'confidence': confidence\n", "        }\n", "\n", "# Initialize chatbot\n", "chatbot = HybridChatbot()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 0, "referenced_widgets": ["0736692735b84edcae0e054df8c48399", "35a5f8a56dbf466c86d8f81da81d6523", "f41691687d174adf8b8c3a0d4191e20d", "0577e64a32fd4a05b29f09b90d185245", "2272e15b507c46e39054b031ec2e8be5", "4b5b8a24316342c19770f17bd6dc2ea9", "3976bcba735d4593b814f753e68ed91e", "8d240b7bef7b4b33853f6d6bdc82bde4", "f88cd7b6d0b24654b2761b88009ff949", "b2d29c6440bf4fef968538683063064c", "1cdd621e388e4a64bdc27736de7c7af4", "5a38e272a81b4347b607785db9228f9c", "3b2415d92a304d579a99cea78ab335c4", "c97f3bf22a874bd39483b37ca8195a18", "31cfa781f42f4a099fd8748797dbcf99", "c593699d5a5c451bb0ec6c7795ce27df", "6629f9beff61493dad16e172f623f765", "d1c9aaa3411241b7aac547bddfe092f6", "5f5a20631f1e4bcda978d901b5cee4a5", "4550f5e8679b4101882184e355bd48f0", "ebe0a7097aff4d02b27d7749dd6fa1e3", "3a89c6fb09194155b058978548f2f51d", "3742aaa4114a48139c05fd23153ba410", "1eb1da8acf084c399464f716f7c820bc", "25dbec20cddd4074a1bccccecbc93b95", "e62f4588bb1244a1a3a5cc0c133dd523", "9fc72d2b9de0489b9dafce5d09e0288b", "6349017f4b6345b180b79515a45ab8a5", "9944a01a1d0647c9bab46549ce7511e6", "a897c54ace8048b0a617ac6b8e926761", "457047ffe0e146e88585947ec8f5bd4e", "ca43e0b9786e49998bac41ee47b89bc5", "96a548956d8f4007a9f7e272a4cf3f37", "0e1eaa5ea0854e39a2cf9a45e40251a7", "bb523195d1b64000badb7a2c82c89d56", "88eb1106bd6a46cba124bdc41b83a843", "cf8d7b11e7cc4c8bb3c853e3798f4605", "c2ad226539344f4bb70c8c43a447ff52", "6d0f74b5351840d391d9e1024004b485", "c4a2e914540e4148b9029d0174a52ad8", "97f94013794248c09240a0d69bffed4d", "3fd23656a1d94f5fa6f30687bc8f9f0d", "dbfe06379fac4bacbcf234f0508f3a39", "7445af72a85743efbe42d4a7114d9858", "7fc8c7cac07445018c14447ac525f26d", "e14120878fb0451ebd4bcd886a365832", "9ae8cef3ab4f4637b558844b1b09b1f5", "254b31fac02b4f59996e305a77d9cc72", "3b7688ddc8d242b790595cfd6e7a4f87", "7a0482bf2e864989b6cc8984efb2ea9e", "e60efa804ea64bff82db13e4cac8ac26", "a93e966a543742c4b7ff104813c49084", "b0489357db454816931b28d940de2391", "9dc55f6a932643efba353ef0cc5d34fd", "3d3a86fc66f045d1a2e6988757f18547", "7947335b6d5844fbb9723e05886b38c7", "1be3b4b54aaa4914a4a1826e8ff1225f", "4fd3fcbbe7064dc9b4c56f50fdfcf876", "1066cbb511804a469eee3f54a1a815e0", "1a0510b909124c069bae200e057e9d2b", "598ebaf2ca114411a55a7cba213f4beb", "c6c0f814a98f448aaf3b6b3614e5f084", "be1c8cab84204cb180550bb9691a9af3", "040c424a3c194f4ba73396fd4e81f237", "bff7d62297fc41e78e929c0a9c1e4de2", "46eb9f9bfaa7462dba89902772d4345b", "8303a1d2db5d443e901b7ab265da1d99", "ea2edf65b6f7434ab690e2549779a9d5", "c74b94e59459490896796ea1fa3676e1", "c210642b0fa949e683359e7ad62e5509", "e7f294b93d0d4c9599401cf2a93ac11a", "269c7c5411374e43a79e4099a5d31e44", "525a5e957d6b4b4c908b33fa13ea1237", "a0f33b89c8074587833b6b70e4cc7801", "4d3913a0a1284f02bc462e510af53c31", "c5bba6381f0f4a4f9ba5930b33f7e809", "65a41fff4afa4949a022759209c3fbe8", "e14d2ece481942efac66ded7a7d26550", "28b80c31ab834daca52e8e4c17eecab5", "2b76bcb03ccf42fc924850295675eac8", "2a9c63f8280d44e0996b19c1214e9d37", "c49e66c151c440a497de9b9cdca5e4db", "48579917fa3043d09c263e4410f2c145", "1e19458982834da7b8bd27510d9b8350", "1658298eb5eb4d04a5336ec3330d76be", "fb0b04912f45439088d8332a03266b20", "0e55f1ddfd9e460d860faa07d4cd8d43", "d991dbea1fb04250926f42ee5931d612", "e2df5fc8dbe140849c5492c8a26e2298", "d406d43b33384947965919110e81694d", "b3d54de014d441ef96c181fbd258f0b0", "3644a52f30ad4a838d19c0b7ec2a9d5b", "ac2146f1927e4ada96565876530bacad", "38ca86088e0b47c2808b0c11cb1929c3", "de030b0de3cc421f9e7ef8e013cbff0e", "84db52cc6c2d4ee483ae679236f376a1", "795030fc3f14486582dfefb4893d5aed", "aa6abdd490c847f9a12b7845240a7e41", "e8ad978be29848ab8db938ff8785c41d", "515ca81da63040438850138e6fbca4cc", "829baa7a11fa4b3786a249365dc65bc7", "b17da578214241f899b99b5f3b5dcedf", "20654dc525844ff69e928ac02960a183", "db06ba300d1a42099ee5630f7a25ad7e", "5220de22b50c441cb260386bf0597b3d", "a773bd024c724726bc569e63d9556967", "ae1a6be0a44d44b78d5e8e2680e1b646", "96f8a0e629ce4f21b017f2c45600627d", "668783aecb54476b807f05d1a701eff2", "327f075bdab44ef2a51f0054dbafa38f", "5f6cc30e52314db8b3b799839a936d2b", "1cbddc3a06a142d484b8369597a4d7c0", "a6165b35907942e68238cae91eae36dc", "216ee3aa707d4479b7f5792ec2058da0", "e6525dd9c76847038915252a9a2c786c", "561f139d12c24cc486cc110b38422543", "9db4ba78c8bd4cefb05b06885c3894ce", "c7fd6d42a2994906975f79bc18e2f3fe", "6fbb8f67efc5426596250f389132248e", "59b03098ae294309af8f1aa9bbf0d154", "9974c04c219e4eb497e30159a1a59cfc", "dda91b883f224087b23b9a7b4df3b018", "17a76940e3d84043ac0fa5f4080f0f93", "8260600dfc534f42b5831be77361d561", "9e3a7cbf748d40ce99487664f55c52a4", "8c6ee7a6d7364aa3a6f81a88ad8442cb", "5c178c68c2f140918ba40ac91bbcb3c2", "4bde25e8d54a43c18c2ebc70a82f3ffa", "38c1d0bc0f014818a8a5fc8fa45beff1", "d346ef073bbb49a59c05b746dc5c1215", "25b392c68f054b9e9b1df1aa3535a22a", "acb0d219b7954e7190b5357c0d10047c", "c1810db71def4e52ad6819ac8b8331a8", "30be415f46f542089d74c3d5ffd01775", "b30f5f14e390475ebf65987dd0497ba9", "9e9ff58d574a48c3a8cc7d5e5da20a71", "80175323fc7247ef9246afc29114554c", "e1250e725c754ab786ce4d9799b21095", "7c86df5fdfd3476289dc2e835d570f3f", "30435543b5854474985ae2ad06729882", "a8dce7addfc44716abeb07fc5a03cf6c", "ec38fdc4a2184ebe9a8acacc420eda23", "efc5eac93536493c86607f220e8110bd", "69a8c96bb34e47e496eec625e8ed0426", "759cdbcbb1454e82a80c981a01ae954d", "42ae58473027478ab7b8c322be135000", "1f9591d2827746088844fc85684006ad", "fa9b5d38ac2242a4b60b979cda115c1c", "fe5b22d3d47d4fd5969091fbef0c3670", "fee6086b20f9495faac75c6ef82bdb6c", "010156d36da842e4807073d11c5fe62d", "f3a24302b6dd4c8aa37f634571d3ef9b", "cfa61f739d1443a588f3b117233ec552", "23a7d7afbbbb4942a3fe689b06179d39", "eb14fa81f00a46b8a06ce58d16a1e81a", "5d68f96f58fc4916ac1a062880d2ee94", "b54ddd4aedcc41469cd36eee80b9dad7", "5e8dee5328944b0084cb96146d4445f8", "e4631661477a45729bdc43f248b25750", "63c0b4fc8527460e8699b582e1299fea", "97762defacc340bdbd355452b584e359", "63846709529d4d90a9d810ae7785551e", "8f093d5bdf3a4e77bb7d679829b0ea96", "4c95f45ccbdd41f9b2e012e437f3ac5c", "02c34e5e8ca045679d28d10744484db1", "d3dda35fd8694a8a864b9fed18fa6052", "ff0b371093144802aa23c92f6858ea53", "76eeb17a2a91427d83bf43e9c3032746", "fa4457aeaca54435addb527f76ecf710", "1a445544baf74a3dbf8b8cd370f9678d", "fbf521e60e1c4c2596434a8eabc5272c", "a978d824c7f74666b6c935ba4eb591db", "460c9d55e446498a8eee097d945fdf12", "a9cf960e9de049c884c8b5a60100f8b9", "d10eb5600f744ca3af2765c4389210b3", "1f1ede5fb28347d687367bc96d5c52df", "8ebb79947f4b41c09180386553e4c07f", "2410bb1916a34a6f8cd8ba19eb0119d5", "888264857b4546569567bbc419b4ef71", "91507fb084db4dadb6e82c13184c59cf", "221ef5411db742469e571b30cf57bd30", "3a1bd9310e4149faad1aa709efd5e0d2", "92cedf2a54ca422a8885d60960b70da0", "345f1a1024ac4fe981aec44dd27f20ce", "885cb60c6ac149e0b289367ef730da2b", "861c1c69b44149a4bb8e782cbf53931c", "ef0bfb4d5348441389e2e09ad2749ce6", "d7e7b957676e4ff38d5f4b90bcf625a4", "1d2e1ec6b55e42ffb61698897be8a54e", "21b699d505f14a9f927ea85e2d35ca08", "a5f1cfaac57b4d50979e16944c9c788b", "19099d003d4a4ed38f80b177f310e4bb", "d6f5c33ac71c4a2585b5fb7b274c3bea", "4fec65ab19894e81bee42d5ba60461cc", "c5771696413a4a32b700f0228706ad32", "c73ce70ee1dd40e88dcea37674643367", "62a4d995d74d47bc93877b02c755572d", "c1a0d707a3db4e209910a0890eb348ca"]}, "id": "xvE7ngLsJdfG", "outputId": "539920db-3245-4887-a21f-ac5903cef1e6"}, "execution_count": 5, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "4.1: End-to-End Pipeline System\n", "============================================================\n", "\n", "1. Loading all components...\n", "   - Loading classifier...\n", "   - Loading retrieval system...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0736692735b84edcae0e054df8c48399"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5a38e272a81b4347b607785db9228f9c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["README.md: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3742aaa4114a48139c05fd23153ba410"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0e1eaa5ea0854e39a2cf9a45e40251a7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7fc8c7cac07445018c14447ac525f26d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7947335b6d5844fbb9723e05886b38c7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8303a1d2db5d443e901b7ab265da1d99"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e14d2ece481942efac66ded7a7d26550"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e2df5fc8dbe140849c5492c8a26e2298"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "515ca81da63040438850138e6fbca4cc"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5f6cc30e52314db8b3b799839a936d2b"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["   - Loading fine-tuned LLM...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dda91b883f224087b23b9a7b4df3b018"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["`torch_dtype` is deprecated! Use `dtype` instead!\n"]}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors.index.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c1810db71def4e52ad6819ac8b8331a8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "69a8c96bb34e47e496eec625e8ed0426"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "eb14fa81f00a46b8a06ce58d16a1e81a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d3dda35fd8694a8a864b9fed18fa6052"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8ebb79947f4b41c09180386553e4c07f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d7e7b957676e4ff38d5f4b90bcf625a4"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 All components loaded!\n", "\n"]}]}, {"cell_type": "markdown", "source": ["# Latency Comparisons"], "metadata": {"id": "_dVDrC1a3elY"}}, {"cell_type": "code", "source": ["print(\"\\n\" + \"=\"*60)\n", "print(\"LATENCY COMPARISONS (EXPANDED)\")\n", "print(\"=\"*60)\n", "\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# Sample by natural distribution: 39.8% det, 60.2% indet\n", "n_total = 500\n", "n_det = int(0.398 * n_total)   # 199\n", "n_indet = n_total - n_det       # 301\n", "\n", "det_sample = df_test[df_test['label'] == 0].sample(n_det, random_state=42)\n", "indet_sample = df_test[df_test['label'] == 1].sample(n_indet, random_state=42)\n", "test_sample = pd.concat([det_sample, indet_sample]).reset_index(drop=True)\n", "\n", "print(f\"Selected {len(test_sample)} test queries (natural distribution):\")\n", "print(f\"  Deterministic: {len(det_sample)} ({len(det_sample)/n_total*100:.1f}%)\")\n", "print(f\"  Indeterministic: {len(indet_sample)} ({len(indet_sample)/n_total*100:.1f}%)\")\n", "\n", "# Track latencies\n", "hybrid_latencies = []\n", "finetuned_latencies = []\n", "retrieval_only_latencies = []\n", "\n", "comparison_results = []\n", "\n", "print(\"\\n\" + \"-\"*60)\n", "print(\"TESTING QUERIES (this will take a while...)\")\n", "print(\"-\"*60)\n", "\n", "for i, (idx, row) in enumerate(test_sample.iterrows()):\n", "    query = row['instruction']\n", "    true_label = row['label']\n", "    category = row['category']\n", "\n", "    # Progress update every 50 queries\n", "    if (i + 1) % 50 == 0:\n", "        print(f\"Progress: {i + 1}/{len(test_sample)} queries completed\")\n", "\n", "    # 1. HYBRID SYSTEM\n", "    hybrid_result = chatbot.respond(query)\n", "    hybrid_latencies.append(hybrid_result['latency_ms'])\n", "\n", "    # 2. FINE-TUNED LLM (all queries through fine-tuned model)\n", "    start_time = time.time()\n", "    prompt = f\"Customer: {query}\\nAssistant:\"\n", "    inputs = chatbot.llm_tokenizer(prompt, return_tensors=\"pt\").to(chatbot.llm_model.device)\n", "\n", "    with torch.no_grad():\n", "        outputs = chatbot.llm_model.generate(\n", "            **inputs,\n", "            max_new_tokens=150,\n", "            do_sample=True,\n", "            temperature=0.7,\n", "            top_p=0.9,\n", "            pad_token_id=chatbot.llm_tokenizer.eos_token_id\n", "        )\n", "\n", "    finetuned_response = chatbot.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "    finetuned_latency = (time.time() - start_time) * 1000\n", "    finetuned_latencies.append(finetuned_latency)\n", "\n", "    # 3. RETRIEVAL-ONLY\n", "    start_time = time.time()\n", "    retrieval_response, distance = chatbot.retrieve_response(query)\n", "    retrieval_latency = (time.time() - start_time) * 1000\n", "    retrieval_only_latencies.append(retrieval_latency)\n", "\n", "    comparison_results.append({\n", "        'query': query,\n", "        'category': category,\n", "        'true_label': true_label,\n", "        'hybrid_latency_ms': hybrid_result['latency_ms'],\n", "        'hybrid_route': hybrid_result['route'],\n", "        'finetuned_latency_ms': finetuned_latency,\n", "        'retrieval_latency_ms': retrieval_latency\n", "    })\n", "\n", "print(f\"\\nCompleted all {len(test_sample)} queries\")\n", "\n", "# Statistics with confidence intervals\n", "from scipy import stats\n", "import numpy as np\n", "\n", "def compute_ci(data, confidence=0.95):\n", "    n = len(data)\n", "    mean = np.mean(data)\n", "    se = stats.sem(data)\n", "    ci = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n", "    return mean, mean - ci, mean + ci\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(f\"LATENCY COMPARISON SUMMARY (n={n_total})\")\n", "print(\"=\"*60)\n", "\n", "hybrid_mean, hybrid_lo, hybrid_hi = compute_ci(hybrid_latencies)\n", "finetuned_mean, finetuned_lo, finetuned_hi = compute_ci(finetuned_latencies)\n", "retrieval_mean, retrieval_lo, retrieval_hi = compute_ci(retrieval_only_latencies)\n", "\n", "print(f\"\\nAverage Latency (95% CI):\")\n", "print(f\"  Hybrid System:    {hybrid_mean:>8.0f} ms ({hybrid_lo:.0f}, {hybrid_hi:.0f})\")\n", "print(f\"  Fine-tuned LLM:   {finetuned_mean:>8.0f} ms ({finetuned_lo:.0f}, {finetuned_hi:.0f})\")\n", "print(f\"  Retrieval-Only:   {retrieval_mean:>8.0f} ms ({retrieval_lo:.0f}, {retrieval_hi:.0f})\")\n", "\n", "# Speedup\n", "speedup = finetuned_mean / hybrid_mean\n", "print(f\"\\nSpeedup: Hybrid is {speedup:.2f}x faster than pure LLM\")\n", "\n", "# Breakdown by route\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"HYBRID SYSTEM BREAKDOWN BY ROUTE\")\n", "print(\"=\"*60)\n", "\n", "results_df = pd.DataFrame(comparison_results)\n", "\n", "retrieval_routed = results_df[results_df['hybrid_route'] == 'RETRIEVAL']\n", "llm_routed = results_df[results_df['hybrid_route'] == 'LLM_GENERATION']\n", "\n", "print(f\"\\nRetrieval path: {len(retrieval_routed)}/{n_total} ({len(retrieval_routed)/n_total*100:.1f}%)\")\n", "if len(retrieval_routed) > 0:\n", "    ret_mean, ret_lo, ret_hi = compute_ci(retrieval_routed['hybrid_latency_ms'])\n", "    print(f\"  Avg latency: {ret_mean:.0f} ms ({ret_lo:.0f}, {ret_hi:.0f})\")\n", "\n", "print(f\"\\nLLM path: {len(llm_routed)}/{n_total} ({len(llm_routed)/n_total*100:.1f}%)\")\n", "if len(llm_routed) > 0:\n", "    llm_mean, llm_lo, llm_hi = compute_ci(llm_routed['hybrid_latency_ms'])\n", "    print(f\"  Avg latency: {llm_mean:.0f} ms ({llm_lo:.0f}, {llm_hi:.0f})\")\n", "\n", "# Save results\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "import os\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "results_df.to_csv(f'{output_path}/latency_comparison_500.csv', index=False)\n", "\n", "# Save summary\n", "summary_stats = {\n", "    'sample_size': n_total,\n", "    'distribution': {\n", "        'deterministic': n_det,\n", "        'indeterministic': n_indet\n", "    },\n", "    'latency_ms': {\n", "        'hybrid': {'mean': hybrid_mean, 'ci_low': hybrid_lo, 'ci_high': hybrid_hi},\n", "        'finetuned_llm': {'mean': finetuned_mean, 'ci_low': finetuned_lo, 'ci_high': finetuned_hi},\n", "        'retrieval_only': {'mean': retrieval_mean, 'ci_low': retrieval_lo, 'ci_high': retrieval_hi}\n", "    },\n", "    'speedup_vs_llm': speedup,\n", "    'hybrid_routing': {\n", "        'retrieval_count': len(retrieval_routed),\n", "        'llm_count': len(llm_routed),\n", "        'retrieval_avg_ms': ret_mean if len(retrieval_routed) > 0 else None,\n", "        'llm_avg_ms': llm_mean if len(llm_routed) > 0 else None\n", "    }\n", "}\n", "\n", "import json\n", "with open(f'{output_path}/latency_summary_500.json', 'w') as f:\n", "    json.dump(summary_stats, f, indent=2)\n", "\n", "print(f\"\\n\u2713 Results saved to:\")\n", "print(f\"   - {output_path}/latency_comparison_500.csv\")\n", "print(f\"   - {output_path}/latency_summary_500.json\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 LATENCY COMPARISON COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9RY0YBQ63m8A", "outputId": "ff0a1db2-3689-4cb3-e769-25cd26327011"}, "execution_count": 7, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "LATENCY COMPARISONS (EXPANDED)\n", "============================================================\n", "Selected 500 test queries (natural distribution):\n", "  Deterministic: 199 (39.8%)\n", "  Indeterministic: 301 (60.2%)\n", "\n", "------------------------------------------------------------\n", "TESTING QUERIES (this will take a while...)\n", "------------------------------------------------------------\n", "Progress: 50/500 queries completed\n", "Progress: 100/500 queries completed\n", "Progress: 150/500 queries completed\n", "Progress: 200/500 queries completed\n", "Progress: 250/500 queries completed\n", "Progress: 300/500 queries completed\n", "Progress: 350/500 queries completed\n", "Progress: 400/500 queries completed\n", "Progress: 450/500 queries completed\n", "Progress: 500/500 queries completed\n", "\n", "Completed all 500 queries\n", "\n", "============================================================\n", "LATENCY COMPARISON SUMMARY (n=500)\n", "============================================================\n", "\n", "Average Latency (95% CI):\n", "  Hybrid System:        5092 ms (4723, 5460)\n", "  Fine-tuned LLM:       8276 ms (8185, 8366)\n", "  Retrieval-Only:          8 ms (8, 8)\n", "\n", "Speedup: Hybrid is 1.63x faster than pure LLM\n", "\n", "============================================================\n", "HYBRID SYSTEM BREAKDOWN BY ROUTE\n", "============================================================\n", "\n", "Retrieval path: 199/500 (39.8%)\n", "  Avg latency: 8 ms (8, 8)\n", "\n", "LLM path: 301/500 (60.2%)\n", "  Avg latency: 8452 ms (8354, 8550)\n", "\n", "\u2713 Results saved to:\n", "   - /content/drive/MyDrive/NLP_Project/results//latency_comparison_500.csv\n", "   - /content/drive/MyDrive/NLP_Project/results//latency_summary_500.json\n", "\n", "============================================================\n", "\u2713 LATENCY COMPARISON COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "markdown", "source": ["# Baseline Response Quality Comparisons"], "metadata": {"id": "31SGsAMDKA70"}}, {"cell_type": "code", "source": ["print(\"\\n\" + \"=\"*60)\n", "print(\"BASELINE RESPONSE QUALITY COMPARISONS\")\n", "print(\"=\"*60)\n", "\n", "# Load non-fine-tuned Phi-2 model for baseline\n", "print(\"\\nLoading non-fine-tuned Phi-2 model...\")\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "import torch\n", "\n", "base_model_name = \"microsoft/phi-2\"\n", "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n", "base_model = AutoModelForCausalLM.from_pretrained(\n", "    base_model_name,\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16,\n", "    device_map=\"auto\"\n", ")\n", "base_tokenizer.pad_token = base_tokenizer.eos_token\n", "print(\"\u2713 Base Phi-2 model loaded\")\n", "\n", "# Build retrieval system from indeterministic training data\n", "print(\"\\nBuilding retrieval system from indeterministic training queries...\")\n", "print(\"Loading MASTER TRAIN dataset...\")\n", "df_train = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/train_dataset.csv')\n", "\n", "# Filter for Indeterministic (LLM) rows from TRAIN only\n", "df_indet_train = df_train[df_train['label'] == 1].reset_index(drop=True)\n", "print(f\"Indeterministic training samples: {len(df_indet_train)}\")\n", "\n", "# Build retrieval embeddings for indeterministic queries\n", "from sentence_transformers import SentenceTransformer\n", "import numpy as np\n", "from sklearn.metrics.pairwise import cosine_distances\n", "\n", "retrieval_model = SentenceTransformer('all-MiniLM-L6-v2')\n", "indet_instructions = df_indet_train['instruction'].tolist()\n", "indet_responses = df_indet_train['response'].tolist()\n", "\n", "print(\"Encoding indeterministic training queries...\")\n", "indet_embeddings = retrieval_model.encode(indet_instructions, show_progress_bar=True)\n", "print(f\"\u2713 Retrieval system built with {len(indet_embeddings)} indeterministic examples\")\n", "\n", "# Function to retrieve from indeterministic database\n", "def retrieve_from_indet(query, top_k=1):\n", "    query_embedding = retrieval_model.encode([query])\n", "    distances = cosine_distances(query_embedding, indet_embeddings)[0]\n", "    top_idx = np.argmin(distances)\n", "    return indet_responses[top_idx], distances[top_idx]\n", "\n", "# Sample diverse test queries from specific categories\n", "print(\"\\nSampling test queries for qualitative comparison...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# Select queries only from ACCOUNT, ORDER, FEEDBACK categories\n", "selected_queries = []\n", "categories_to_sample = ['ACCOUNT', 'ORDER', 'FEEDBACK']\n", "\n", "for category in categories_to_sample:\n", "    # Get 2 indeterministic queries from each category\n", "    cat_queries = df_test[(df_test['category'] == category) & (df_test['label'] == 1)].sample(2, random_state=42)\n", "    selected_queries.extend(cat_queries.to_dict('records'))\n", "\n", "print(f\"Selected {len(selected_queries)} indeterministic test queries from {categories_to_sample}\")\n", "\n", "# Store all responses\n", "comparison_results = []\n", "\n", "print(\"\\n\" + \"=\"*80)\n", "print(\"GENERATING RESPONSES FROM ALL SYSTEMS\")\n", "print(\"=\"*80)\n", "\n", "for i, query_data in enumerate(selected_queries, 1):\n", "    query = query_data['instruction']\n", "    true_label = query_data['label']\n", "    category = query_data['category']\n", "    reference_response = query_data['response']\n", "\n", "    print(f\"\\n{'='*80}\")\n", "    print(f\"QUERY {i}/{len(selected_queries)}\")\n", "    print(f\"{'='*80}\")\n", "    print(f\"\\nQuery: {query}\")\n", "    print(f\"Category: {category} | True Label: {true_label}\")\n", "\n", "    # SYSTEM 1: HYBRID (classifier \u2192 retrieval/fine-tuned LLM)\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"HYBRID SYSTEM\")\n", "    print(f\"{'-'*80}\")\n", "    try:\n", "        hybrid_result = chatbot.respond(query)\n", "        hybrid_response = hybrid_result['response']\n", "        hybrid_route = hybrid_result['route']\n", "        print(f\"Route: {hybrid_route}\")\n", "        print(f\"Response:\\n{hybrid_response}\")\n", "    except Exception as e:\n", "        hybrid_response = f\"ERROR: {str(e)}\"\n", "        hybrid_route = \"ERROR\"\n", "        print(f\"ERROR: {str(e)}\")\n", "\n", "    # SYSTEM 2: FINE-TUNED LLM (zero-shot, no routing)\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"FINE-TUNED LLM (Zero-Shot, No Routing)\")\n", "    print(f\"{'-'*80}\")\n", "    try:\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = chatbot.llm_tokenizer(prompt, return_tensors=\"pt\").to(chatbot.llm_model.device)\n", "\n", "        with torch.no_grad():\n", "            outputs = chatbot.llm_model.generate(\n", "                **inputs,\n", "                max_new_tokens=150,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=chatbot.llm_tokenizer.eos_token_id\n", "            )\n", "\n", "        finetuned_response = chatbot.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        finetuned_response = finetuned_response.split(\"Assistant:\")[-1].strip()\n", "        print(f\"Response:\\n{finetuned_response}\")\n", "    except Exception as e:\n", "        finetuned_response = f\"ERROR: {str(e)}\"\n", "        print(f\"ERROR: {str(e)}\")\n", "\n", "    # SYSTEM 3: BASE PHI-2 (non-fine-tuned)\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"BASE PHI-2 (Non-Fine-Tuned)\")\n", "    print(f\"{'-'*80}\")\n", "    try:\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = base_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n", "\n", "        with torch.no_grad():\n", "            outputs = base_model.generate(\n", "                **inputs,\n", "                max_new_tokens=150,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=base_tokenizer.eos_token_id\n", "            )\n", "\n", "        base_response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        base_response = base_response.split(\"Assistant:\")[-1].strip()\n", "        print(f\"Response:\\n{base_response}\")\n", "    except Exception as e:\n", "        base_response = f\"ERROR: {str(e)}\"\n", "        print(f\"ERROR: {str(e)}\")\n", "\n", "    # SYSTEM 4: RETRIEVAL FROM INDETERMINISTIC TRAINING DATA\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"RETRIEVAL-ONLY (Trained on Indeterministic Queries)\")\n", "    print(f\"{'-'*80}\")\n", "    try:\n", "        indet_retrieval_response, distance = retrieve_from_indet(query)\n", "        print(f\"Distance: {distance:.3f}\")\n", "        print(f\"Response:\\n{indet_retrieval_response}\")\n", "    except Exception as e:\n", "        indet_retrieval_response = f\"ERROR: {str(e)}\"\n", "        print(f\"ERROR: {str(e)}\")\n", "\n", "    # REFERENCE\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"REFERENCE (Ground Truth)\")\n", "    print(f\"{'-'*80}\")\n", "    print(f\"Response:\\n{reference_response}\")\n", "\n", "    print(f\"\\n{'='*80}\\n\")\n", "\n", "    # Store results\n", "    comparison_results.append({\n", "        'query': query,\n", "        'category': category,\n", "        'true_label': true_label,\n", "        'reference_response': reference_response,\n", "        'hybrid_response': hybrid_response,\n", "        'hybrid_route': hybrid_route,\n", "        'finetuned_response': finetuned_response,\n", "        'base_phi2_response': base_response,\n", "        'indet_retrieval_response': indet_retrieval_response\n", "    })\n", "\n", "# Save results\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"SAVING RESULTS\")\n", "print(\"=\"*60)\n", "\n", "results_df = pd.DataFrame(comparison_results)\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "import os\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "results_df.to_csv(f'{output_path}/baseline_comparison_indet.csv', index=False)\n", "print(f\"\u2713 Results saved to {output_path}/baseline_comparison_indet.csv\")\n", "\n", "# Summary statistics\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"SUMMARY STATISTICS\")\n", "print(\"=\"*60)\n", "\n", "# Count routing in hybrid\n", "hybrid_retrieval_count = sum(1 for r in comparison_results if r['hybrid_route'] == 'RETRIEVAL')\n", "hybrid_llm_count = sum(1 for r in comparison_results if r['hybrid_route'] == 'LLM_GENERATION')\n", "\n", "print(f\"\\nHybrid System Routing:\")\n", "print(f\"  Retrieval: {hybrid_retrieval_count}/{len(comparison_results)} ({hybrid_retrieval_count/len(comparison_results)*100:.1f}%)\")\n", "print(f\"  LLM Generation: {hybrid_llm_count}/{len(comparison_results)} ({hybrid_llm_count/len(comparison_results)*100:.1f}%)\")\n", "\n", "# Average response lengths\n", "avg_hybrid_len = sum(len(r['hybrid_response'].split()) for r in comparison_results) / len(comparison_results)\n", "avg_finetuned_len = sum(len(r['finetuned_response'].split()) for r in comparison_results) / len(comparison_results)\n", "avg_base_len = sum(len(r['base_phi2_response'].split()) for r in comparison_results) / len(comparison_results)\n", "avg_indet_retrieval_len = sum(len(r['indet_retrieval_response'].split()) for r in comparison_results) / len(comparison_results)\n", "avg_reference_len = sum(len(r['reference_response'].split()) for r in comparison_results) / len(comparison_results)\n", "\n", "print(f\"\\nAverage Response Length (words):\")\n", "print(f\"  Hybrid:                          {avg_hybrid_len:.1f}\")\n", "print(f\"  Fine-Tuned LLM:                  {avg_finetuned_len:.1f}\")\n", "print(f\"  Base Phi-2:                      {avg_base_len:.1f}\")\n", "print(f\"  Retrieval (Indet Training):      {avg_indet_retrieval_len:.1f}\")\n", "print(f\"  Reference:                       {avg_reference_len:.1f}\")\n", "\n", "# Save summary\n", "summary = {\n", "    \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n", "    \"num_queries_tested\": len(comparison_results),\n", "    \"categories\": categories_to_sample,\n", "    \"retrieval_source\": \"indeterministic_training_data\",\n", "    \"indet_training_samples\": len(df_indet_train),\n", "    \"hybrid_routing\": {\n", "        \"retrieval\": hybrid_retrieval_count,\n", "        \"llm\": hybrid_llm_count\n", "    },\n", "    \"avg_response_lengths\": {\n", "        \"hybrid\": float(avg_hybrid_len),\n", "        \"finetuned\": float(avg_finetuned_len),\n", "        \"base_phi2\": float(avg_base_len),\n", "        \"indet_retrieval\": float(avg_indet_retrieval_len),\n", "        \"reference\": float(avg_reference_len)\n", "    }\n", "}\n", "\n", "import json\n", "with open(f'{output_path}/baseline_summary_indet.json', 'w') as f:\n", "    json.dump(summary, f, indent=2)\n", "\n", "print(f\"\\n\u2713 Summary saved to {output_path}/baseline_summary_indet.json\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 BASELINE COMPARISON COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["07e3d2fd5f224a39afe92a0e60f2ad5f", "524732111429440a89aa637dc6747631", "fd3cec1abc214aa7900f6fdab22dadd5", "191dc68da56b418ea3399d16c2b25831", "6b5e2b72eded4a299e7c37fa22dce66b", "b4eca753fe364ff78d50586a526045f7", "cb6f347c984f408999605a1cdcee380b", "12b0a8e483044c3f9b323ea83c785ff5", "2b9ada4f5793497ebfb9399115280052", "3bcc6e2288514f3ebb49f73d54164c7e", "c493fb42c5da44cb93bfdd99dd9a8641", "64f066fb5eb7472fb2afe165526b5833", "b170ebbe13e34eea99f7b47682e587d6", "bec7140cb6fb43c99625295936dc5199", "109b515de8c149128b7fcb42585c9460", "1a70acbf5a734a23b55da38dc2a19c22", "25c73dfdb9b6458e9f8e85b69b93d864", "dc880f870bde456cb560261478148743", "32aff042c58a49d58f90866b1178e8bc", "9556eeb797ef431f9ff3c2ea2512c04c", "d80ce94354de4f7b9905d384b46a2567", "2bc74850134a4cddb08fe549248796f8"]}, "id": "7EZ-g2vHKCPt", "outputId": "03cad40d-1b12-4f52-f419-3aa7594759bd"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "BASELINE RESPONSE QUALITY COMPARISONS\n", "============================================================\n", "\n", "Loading non-fine-tuned Phi-2 model...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "07e3d2fd5f224a39afe92a0e60f2ad5f"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["\u2713 Base Phi-2 model loaded\n", "\n", "Building retrieval system from indeterministic training queries...\n", "Loading MASTER TRAIN dataset...\n", "Indeterministic training samples: 9577\n", "Encoding indeterministic training queries...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Batches:   0%|          | 0/300 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "64f066fb5eb7472fb2afe165526b5833"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["\u2713 Retrieval system built with 9577 indeterministic examples\n", "\n", "Sampling test queries for qualitative comparison...\n", "Selected 6 indeterministic test queries from ['ACCOUNT', 'ORDER', 'FEEDBACK']\n", "\n", "================================================================================\n", "GENERATING RESPONSES FROM ALL SYSTEMS\n", "================================================================================\n", "\n", "================================================================================\n", "QUERY 1/6\n", "================================================================================\n", "\n", "Query: what do i need to do to recover my user account pin code\n", "Category: ACCOUNT | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "I'll take care of it! I understand the importance of recovering your user account PIN code. Let's work together to ensure a smooth recovery process:\n", "\n", "1. Begin by visiting the \"{{Login Page URL}}\" of our platform.\n", "2. Look for the option that says \"{{Forgot PIN}}\" and click on it.\n", "3. You will be prompted to provide the email address associated with your user account.\n", "4. Once you've entered your email, keep an eye on your inbox. You should receive an email with instructions on how to reset your PIN code.\n", "5. If you don't see the email in your main inbox, please check your spam or junk folders as it may have been filtered there.\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'll take care of it! I understand that you need assistance in recovering your user account PIN code. Here's what you can do:\n", "\n", "1. Start by visiting our platform's \"{{Login Page URL}}\".\n", "2. Look for the option that says \"{{Forgot PIN}}\" and click on it.\n", "3. You will be prompted to enter the email address associated with your user account. Please provide the correct email address to proceed.\n", "4. Once you've entered the email address, an email with detailed instructions on how to recover your PIN code will be sent to you. If you don't see the email in your inbox, please check your spam or junk folder.\n", "5. Follow the instructions provided in the\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "If you have forgotten your password, you can reset it by clicking on the \"Forgot Password\" link on the login page. We'll send you an email with instructions on how to reset your password. Once you've received the email, follow the instructions to create a new password for your account.\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.002\n", "Response:\n", "I understand your concern about recovering your user account PIN code. Let me guide you through the process step by step:\n", "\n", "1. Go to the \"{{Account Recovery}}\" page on our website.\n", "2. Look for the option to recover your PIN code and click on it.\n", "3. You will be prompted to enter your registered email address or phone number associated with your user account.\n", "4. After providing the required information, a verification code will be sent to your registered email address or phone number.\n", "5. Enter the verification code in the designated field on the recovery page.\n", "6. Once the code is verified, you will be able to reset your PIN code and regain access to your user account.\n", "\n", "If you encounter any difficulties during the process or have any further questions, feel free to reach out to our customer support team, who is available {{Customer Support Hours}} at {{Customer Support Phone Number}}. We are here to assist you every step of the way to ensure a smooth and secure account recovery experience.\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "For sure! I understand that you're seeking guidance on recovering the PIN code for your user account. Allow me to assist you through the process:\n", "\n", "1. Start by visiting the \"{{Account Recovery Page URL}}\" on our platform.\n", "2. Look for the option labeled \"{{Recover PIN}}\" and select it.\n", "3. You may be prompted to provide some verification details to confirm your identity. This step is in place to ensure the security of your account.\n", "4. Once your identity is verified, you should receive an email containing instructions on how to reset or retrieve your PIN code. Please check both your primary and secondary email folders, as it may end up in either of them.\n", "5. Follow the instructions provided in the email carefully to regain access to your account and retrieve your PIN code.\n", "\n", "If you encounter any difficulties during this process or need further assistance, please don't hesitate to reach out. We're here to support you every step of the way!\"\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 2/6\n", "================================================================================\n", "\n", "Query: I want assistance to retrieve my profile PIN code\n", "Category: ACCOUNT | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "I'll take care of it! I'm here to guide you through the process of retrieving your profile PIN code. Let's get started:\n", "\n", "1. To begin, please visit our platform's \"{{Profile Recovery Page URL}}\".\n", "2. Once you're on the recovery page, you'll be prompted to enter some information. This may include your email address, username, or any other details associated with your profile.\n", "3. After providing the required information, you should receive an email with instructions on how to reset your PIN code. If you don't see it in your main inbox, please check your spam or junk folder.\n", "4. Follow the instructions in the email carefully to reset your PIN code. You may be required to\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'll take care of it! I understand your need for assistance in retrieving your profile PIN code. Your PIN code is an essential security measure that grants you access to your profile. Let's work together to resolve this:\n", "\n", "1. Begin by visiting our platform's login page.\n", "2. Look for the \"Forgot PIN\" option and click on it.\n", "3. You will be prompted to provide your email address associated with your profile.\n", "4. Once you've entered your email, keep an eye on your inbox. A comprehensive email with instructions on how to retrieve your PIN code will soon arrive. If you don't see it in your main inbox, please check your spam or junk folder.\n", "5. Follow the instructions provided\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "Sure, I can help you with that. May I have your account information, please?\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.008\n", "Response:\n", "For sure! I understand that you need assistance in retrieving your profile PIN code. Your PIN code is an essential element in maintaining the security of your profile, and I'm here to guide you through the process. Please follow these steps:\n", "\n", "1. Begin by logging into your profile using your username and password.\n", "2. Once you're logged in, navigate to the profile settings or security settings section.\n", "3. Look for an option related to \"PIN code\" or \"Profile PIN\".\n", "4. In this section, you should find an option to either view or reset your PIN code.\n", "5. If you have forgotten your PIN code, select the reset option and follow the instructions provided.\n", "6. You may be asked to verify your identity through additional security measures such as answering security questions or confirming your contact information.\n", "7. Once you have successfully reset your PIN code, make sure to choose a strong and memorable code.\n", "\n", "Remember, should you encounter any difficulties or require further assistance throughout this process, I'll be right here to support you. Your satisfaction and the security of your profile are our priorities.\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'm on it! I understand your need for assistance in retrieving your profile PIN code. Your PIN code is essential for accessing and securing your profile. Let's walk through the steps together:\n", "\n", "1. To retrieve your profile PIN code, please visit our website at {{Website URL}} and navigate to the login page.\n", "2. On the login page, look for the \"Forgot PIN\" option. This option will guide you through the process of retrieving your PIN code.\n", "3. Click on the \"Forgot PIN\" option and follow the instructions provided. You may be required to verify your identity through a secondary authentication method.\n", "4. Once you've completed the necessary steps, you will receive an email with instructions on how to reset your profile PIN code.\n", "5. Check your email inbox, including your spam or junk folder, for the email containing the instructions.\n", "6. Follow the instructions in the email to reset your PIN code and regain access to your profile.\n", "\n", "If you encounter any difficulties during this process or have any further questions, please don't hesitate to reach out to our customer support team. They are available {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat on our website at {{Website URL}}. Remember, we're here to assist you every step of the way.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 3/6\n", "================================================================================\n", "\n", "Query: I need assistance trackng purchase {{Order Number}}\n", "Category: ORDER | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "Your message means a lot! I'm aligned with the idea that you need assistance in tracking your purchase with the purchase number {{Order Number}}. I'm here to help you with that. To track your purchase, please visit the '{{Order Tracking}}' section on our website. Once there, you should be able to enter your purchase number and view the current status of your order. If you encounter any difficulties or have any further questions, feel free to ask. I'm here to provide you with the support you need to have a seamless tracking experience. Let's get started! How are you feeling today? Is there anything else I can assist you with? Your well-being is my top priority. Let's make this tracking process as smooth\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "We're here for you! I take note that you need assistance in tracking the purchase with the purchase number {{Order Number}}. To help you with that, I recommend visiting the '{{Order Tracking}}' section on our website. There, you'll be able to enter your purchase number and get real-time updates on the status of your order. If you have any other questions or need further guidance, please feel free to ask. I'm here to ensure you have a seamless experience! How are you feeling today? Let's make sure you have all the information you need to stay informed about your purchase. Your satisfaction is our top priority!\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'm sorry, but I don't understand what you're asking. Could you please rephrase your question?\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.024\n", "Response:\n", "Thank you for reaching out! I'm here to assist you with tracking the status of the purchase with purchase number {{Order Number}}. To provide you with accurate information, could you please provide me with additional details such as the email address or name associated with the purchase? This will allow me to locate the order and provide you with the most up-to-date tracking information.\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "Your message means a lot! I'm aligned with the idea that you need assistance in tracking the status of your purchase with order number {{Order Number}}. To provide you with accurate information, could you please confirm the purchase number again? Once I have the correct information, I will be able to guide you on how to track the purchase and provide you with the latest updates.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 4/6\n", "================================================================================\n", "\n", "Query: i cannotedit purchase {{Order Number}}\n", "Category: ORDER | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "I'm sorry to hear that you're having trouble editing purchase number {{Order Number}}. I understand how important it is to make changes to your order, and I'm here to assist you. Could you please provide more details about the specific edits you would like to make? This will help me guide you through the process and ensure that your purchase is exactly as you want it. Your satisfaction is our top priority, and I'm committed to resolving this issue for you. Let's work together to find a solution. Thank you for reaching out to us. How can I assist you further? Is there anything else I can help you with? Remember, we're here to support you every step of the way. Please don't hesitate to ask for help\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I've realized that you're experiencing difficulties editing purchase number {{Order Number}}. I apologize for any inconvenience this may have caused. To better assist you, could you please provide more details about the specific changes you would like to make? This will allow us to investigate the issue further and provide you with the appropriate solution. Your satisfaction is our top priority, and we are committed to resolving this matter for you. Thank you for your patience and cooperation. How can I further assist you? \n", "\n", "Please note that I am an AI language model and cannot make any changes to your purchase without your explicit permission. If you need assistance with any other aspect, feel free to let me know. I'm here to help! \n", "\n", "Best regards,\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'm sorry, but I cannot edit purchases. Is there something else I can help you with?\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.080\n", "Response:\n", "I've been informed that you're experiencing difficulties with payment for your purchase associated with the order number {{Order Number}}. I apologize for the inconvenience caused by this issue. To help resolve this, please follow these steps:\n", "\n", "1. Double-Check Payment Details: Verify that the payment information you provided is accurate, including the card number, expiration date, CVV, and billing address.\n", "\n", "2. Check Payment Method Availability: Ensure that your chosen payment method is supported by our platform. Some payment methods may have certain restrictions or limitations.\n", "\n", "3. Contact Your Bank or Payment Provider: Reach out to your bank or payment provider to confirm if there are any issues on their end that may be preventing the payment from going through. They might be able to provide further assistance and insights.\n", "\n", "4. Use an Alternative Payment Method: If all else fails, consider using a different payment method, such as a different debit or credit card, PayPal, or any other supported options.\n", "\n", "If the issue persists after trying these steps, please don't hesitate to contact our dedicated support team during {{Customer Support Hours}} at {{Customer Support Phone Number}} or via Live Chat on our website at {{Website URL}}. We're here to help you resolve this payment issue and ensure that your purchase is processed successfully.\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I've noticed that you are facing difficulties in editing purchase number {{Order Number}}. I apologize for the inconvenience caused. To assist you further, could you please specify the changes you would like to make to your purchase? With this information, I will be able to find a suitable solution for you. Your satisfaction is our top priority, and we are committed to resolving any issues you may have.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 5/6\n", "================================================================================\n", "\n", "Query: can ya help me lodge a claim against ur organization\n", "Category: FEEDBACK | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "I've grasped that you need assistance in lodging a claim against our organization. I apologize for any inconvenience you may have experienced, and I'm here to help you navigate the process. To lodge a claim, please provide me with the details of your complaint, including any relevant documentation or evidence. Rest assured, we take all claims seriously and will thoroughly investigate the matter. Your feedback is essential to us, and we appreciate your patience as we work towards a resolution. How can I assist you further in this matter? Let's work together to ensure your concerns are addressed appropriately.\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I've realized that you're seeking assistance with lodging a claim against our organization, and I apologize for any inconvenience or dissatisfaction you may have experienced. Your feedback is incredibly important to us, and we take all claims seriously. To ensure that your claim is handled appropriately, I recommend reaching out to our customer support team directly. They will guide you through the process and gather all the necessary information to address your claim promptly and effectively. You can contact them through our website or by calling our dedicated customer support hotline. We appreciate your patience and understanding as we work towards resolving your concern. How may I further assist you?  Let me know if there's anything else I can do to help.  Let me know if there's anything else I can do to\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "Customer: Hi, I'd like to lodge a claim against your organization.\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.062\n", "Response:\n", "I realized, you require assistance in lodging a claim against our organization. I apologize for the inconvenience caused and assure you that I'm here to support you through this process. To better assist you, could you please provide me with the details of your claim? This will help me gather all the necessary information and ensure that your concerns are addressed effectively.\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "I'm clearly cognizant your need for assistance in lodging a claim against our organization. I apologize for any inconvenience or dissatisfaction you may have experienced, and I'm here to help you through the process. To proceed with lodging a claim, could you please provide me with some details about the issue you encountered? Any relevant information, such as order numbers, dates, or descriptions of the problem, will greatly assist us in addressing your concern efficiently and effectively. Rest assured that your feedback is of utmost importance to us, and we are committed to resolving any issues you may have faced.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 6/6\n", "================================================================================\n", "\n", "Query: assistance to submit my feedback for urproducts\n", "Category: FEEDBACK | True Label: 1\n", "\n", "--------------------------------------------------------------------------------\n", "HYBRID SYSTEM\n", "--------------------------------------------------------------------------------\n", "Route: LLM_GENERATION\n", "Response:\n", "Thank you for your interest in providing feedback for our products! Your insights are incredibly valuable to us as we strive to improve and deliver the best possible experience. To submit your feedback, you can visit our website and navigate to the product page of the item you would like to review. On the product page, you will find a section where you can rate the product and write a detailed review. Additionally, we have a dedicated feedback form on our website where you can share your thoughts and suggestions. We appreciate your time and effort in helping us enhance our products. If you need any further assistance, please let me know. I'm here to help! How can I assist you today? \n", "\n", "Please note that this response is generated by an AI and\n", "\n", "--------------------------------------------------------------------------------\n", "FINE-TUNED LLM (Zero-Shot, No Routing)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "We appreciate your interest in sharing your feedback with us! Your insights are invaluable in helping us improve our products. I'm here to guide you through the process of submitting your feedback. Could you please let me know which specific products you would like to provide feedback on? Once I have that information, I can assist you further in submitting your feedback. Your thoughts and opinions matter to us, and we look forward to hearing from you! How are you today? Let's get started on this feedback journey together! \ud83d\ude0a\ud83d\udca1\ud83d\udcdd\n", "\n", "--------------------------------------------------------------------------------\n", "BASE PHI-2 (Non-Fine-Tuned)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "Thank you for your feedback. We're sorry to hear that you had a negative experience with our product. We would love to hear more about it so that we can improve. Could you please provide us with more details about what went wrong?\n", "\n", "--------------------------------------------------------------------------------\n", "RETRIEVAL-ONLY (Trained on Indeterministic Queries)\n", "--------------------------------------------------------------------------------\n", "Distance: 0.216\n", "Response:\n", "I'm on it! We appreciate your interest in providing feedback for our products. Your insights and opinions are extremely valuable to us as we strive to meet and exceed your expectations. To submit your feedback, you can visit our website and look for the \"Feedback\" or \"Contact Us\" section. Alternatively, you can reach out to our customer support team who will be more than happy to assist you in submitting your feedback. We look forward to hearing from you and thank you for taking the time to share your thoughts!\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth)\n", "--------------------------------------------------------------------------------\n", "Response:\n", "We truly appreciate your willingness to provide feedback on our products! Your insights are invaluable to us as they help us understand how our products are meeting your needs and expectations. To submit your feedback, you can visit our website and navigate to the product page of the specific item you would like to review. Look for the \"Leave a Review\" or \"Write a Review\" option, and click on it. You will be redirected to a feedback form where you can share your thoughts, opinions, and ratings. Rest assured, your feedback will be carefully reviewed and shared with the appropriate teams. Thank you for taking the time to share your valuable feedback with us!\n", "\n", "================================================================================\n", "\n", "\n", "============================================================\n", "SAVING RESULTS\n", "============================================================\n", "\u2713 Results saved to /content/drive/MyDrive/NLP_Project/results//baseline_comparison_indet.csv\n", "\n", "============================================================\n", "SUMMARY STATISTICS\n", "============================================================\n", "\n", "Hybrid System Routing:\n", "  Retrieval: 0/6 (0.0%)\n", "  LLM Generation: 6/6 (100.0%)\n", "\n", "Average Response Length (words):\n", "  Hybrid:                          119.0\n", "  Fine-Tuned LLM:                  114.7\n", "  Base Phi-2:                      24.8\n", "  Retrieval (Indet Training):      130.8\n", "  Reference:                       120.8\n", "\n", "\u2713 Summary saved to /content/drive/MyDrive/NLP_Project/results//baseline_summary_indet.json\n", "\n", "============================================================\n", "\u2713 BASELINE COMPARISON COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "markdown", "source": ["LIMITATIONS OF EACH APPROACH\n", "\n", "Hybrid System:\n", "- Depends on classifier accuracy - misrouting leads to poor responses\n", "- Retrieval path fails on queries not well-represented in training data\n", "- LLM path slower than pure retrieval (5-6 seconds vs <10ms)\n", "- Complexity: requires maintaining 3 separate components (classifier, retrieval index, fine-tuned LLM)\n", "- Placeholder templates ({{Order Number}}) require post-processing to fill with actual values\n", "- Classifier may struggle with edge cases that blend deterministic and indeterministic characteristics\n", "- System performance degrades if any component fails\n", "\n", "Zero-Shot LLM (Base Phi-2, No Fine-Tuning):\n", "- Generic responses lacking domain-specific templates and tone\n", "- No knowledge of company-specific policies, business hours, or procedures\n", "- Slower for all queries - no fast retrieval path for simple factual questions\n", "- May hallucinate facts about policies, return windows, or contact information\n", "- Cannot leverage template-based responses for deterministic queries\n", "- Computational cost: all queries require full LLM inference (5-6 seconds each)\n", "- Inconsistent response quality - may be off-topic for straightforward factual questions\n", "- Lacks empathetic customer service tone present in fine-tuned model\n", "\n", "Retrieval-Only (Force All Queries Through Semantic Search):\n", "- Cannot handle novel or complex queries requiring contextual reasoning\n", "- Fails on queries phrased differently from training examples\n", "- No personalization or adaptation to user's specific situation\n", "- Poor performance on edge cases not represented in training set\n", "- Cannot combine information from multiple sources\n", "- Limited to template responses - lacks conversational flexibility\n", "- May retrieve semantically similar but contextually wrong responses\n", "- No ability to handle multi-step reasoning or troubleshooting\n", "- Fails completely on indeterministic queries requiring empathy or problem-solving"], "metadata": {"id": "0EplqLn6Mls7"}}, {"cell_type": "markdown", "source": ["# Test 2"], "metadata": {"id": "ARxncCVMKhPE"}}, {"cell_type": "code", "source": [], "metadata": {"id": "UW0BxDNEKilN"}, "execution_count": null, "outputs": []}]}