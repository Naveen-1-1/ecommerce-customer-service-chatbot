{"cells": [{"cell_type": "markdown", "metadata": {"id": "Q4qrTqYjmUkW"}, "source": ["# Phase 0: Environment Setup & Verification"]}, {"cell_type": "markdown", "metadata": {"id": "OiFKn3R2mrtF"}, "source": ["## 0.2: Mount Google Drive"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7UIp6ZhWvlvD", "outputId": "646ea84c-dbcc-401e-83f6-0be3b78384fc"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Mounted at /content/drive\n"]}], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Wkgqw1THg0DS", "outputId": "d11ebc46-5ad7-47b8-871e-53f13a770f72"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n", "\u2713 Project directory created at: /content/drive/MyDrive/NLP_Project\n", "\n", "Directory structure:\n", "total 16\n", "drwx------ 2 root root 4096 Dec 10 00:42 checkpoints\n", "drwx------ 2 root root 4096 Nov 19 01:28 data\n", "drwx------ 2 root root 4096 Dec 10 00:13 models\n", "drwx------ 2 root root 4096 Nov 19 01:28 results\n"]}], "source": ["from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Create project directory in Drive\n", "import os\n", "project_path = '/content/drive/MyDrive/NLP_Project'\n", "os.makedirs(project_path, exist_ok=True)\n", "os.makedirs(f'{project_path}/data', exist_ok=True)\n", "os.makedirs(f'{project_path}/models', exist_ok=True)\n", "os.makedirs(f'{project_path}/results', exist_ok=True)\n", "os.makedirs(f'{project_path}/checkpoints', exist_ok=True)\n", "\n", "print(f\"\u2713 Project directory created at: {project_path}\")\n", "print(f\"\\nDirectory structure:\")\n", "!ls -la /content/drive/MyDrive/NLP_Project/"]}, {"cell_type": "markdown", "metadata": {"id": "VG4GckODms6r"}, "source": ["## 0.3: Install Required Libraries"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lsxHbzmThMMb", "outputId": "45acd9d6-272a-4ca5-e722-ba4c0b42ca17"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Installing required packages...\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n", "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n", "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n", "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n", "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n", "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n", "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n", "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n", "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n", "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n", "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n", "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n", "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n", "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n", "\n", "==================================================\n", "VERIFYING INSTALLATIONS\n", "==================================================\n", "\u2713 All core libraries imported successfully!\n", "\n", "Library versions:\n", "  PyTorch: 2.9.0+cu126\n", "  Transformers: 4.57.3\n", "  PEFT: 0.18.0\n", "  Sentence Transformers: 5.2.0\n"]}], "source": ["print(\"Installing required packages...\")\n", "!pip install -q peft accelerate bitsandbytes\n", "!pip install -q sentence-transformers faiss-cpu\n", "!pip install -q rouge-score bert-score\n", "!pip install -q datasets\n", "!pip install -U bitsandbytes accelerate\n", "\n", "print(\"\\n\" + \"=\"*50)\n", "print(\"VERIFYING INSTALLATIONS\")\n", "print(\"=\"*50)\n", "\n", "# Verify installations\n", "import pandas as pd\n", "import numpy as np\n", "import torch\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "import transformers\n", "import peft\n", "import sentence_transformers\n", "from sentence_transformers import SentenceTransformer\n", "from datasets import load_dataset\n", "\n", "print(\"\u2713 All core libraries imported successfully!\")\n", "print(f\"\\nLibrary versions:\")\n", "print(f\"  PyTorch: {torch.__version__}\")\n", "print(f\"  Transformers: {transformers.__version__}\")\n", "print(f\"  PEFT: {peft.__version__}\")\n", "print(f\"  Sentence Transformers: {sentence_transformers.__version__}\")"]}, {"cell_type": "markdown", "metadata": {"id": "-DuydHstwHxI"}, "source": ["## LLM Fine-Tuning"]}, {"cell_type": "markdown", "metadata": {"id": "-PA4Gipasbgs"}, "source": ["### LoRA Setup & Small-Scale Training"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JSVSjnODyYL7", "outputId": "600f6e3e-43b2-4020-cd54-86d0a86f434a"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Loading MASTER TRAIN dataset...\n", "The length of det. train dataset: 9577\n", "\n", "Loading Phi-2 model (full precision for A100)...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9129dbfaf9234acf938bd4ecf626c3f5", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "f6ec122a448e43cdb7f574964d6e2850", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a4abef788d61415c9ce5959fbac7cb8e", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c1ce62bdd77841cf9a59fad264dd9598", "version_major": 2, "version_minor": 0}, "text/plain": ["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "07fc020cdefa43d8acb07384c65d08ec", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["`torch_dtype` is deprecated! Use `dtype` instead!\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "2e3532a69a3b44e4a215693499b924b8", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "436c1d13ab3442728899bd6d09740eca", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "0c7b97470ffb43f88adc633818441f0e", "version_major": 2, "version_minor": 0}, "text/plain": ["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "2465a986c4d44813b2dedec84aab27d3", "version_major": 2, "version_minor": 0}, "text/plain": ["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "76a7fa1450224b1d84a87698a375e296", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e97328b13ada4415b44679a0a9254068", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["   \u2713 Model loaded\n", "   GPU memory: 14.48 GB\n", "\n", "Preparing model for LoRA training...\n", "trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879\n", "   \u2713 LoRA configured\n", "   GPU memory after LoRA: 14.54 GB\n", "Formatting prompts...\n", "Created 8619 training prompts and 958 validation prompts.\n"]}], "source": ["print(\"Loading MASTER TRAIN dataset...\")\n", "df_train = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/train_dataset.csv')\n", "\n", "# Filter for Indeterministic (LLM) rows from TRAIN only\n", "df_indet_train = df_train[df_train['label'] == 1].reset_index(drop=True)\n", "\n", "print(f\"The length of det. train dataset: {len(df_indet_train)}\")\n", "\n", "print(\"\\nLoading Phi-2 model (full precision for A100)...\")\n", "model_name = \"mistralai/Mistral-7B-v0.1\"\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"right\"\n", "\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    device_map=\"auto\",\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "\n", "# Enable gradients for inputs to prevent RuntimeError during backward pass\n", "model.enable_input_require_grads()\n", "\n", "print(f\"   \u2713 Model loaded\")\n", "print(f\"   GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n", "\n", "# Prepare model for LoRA training\n", "print(\"\\nPreparing model for LoRA training...\")\n", "\n", "from peft import LoraConfig, get_peft_model\n", "\n", "# Lora Config\n", "lora_config = LoraConfig(\n", "    r=16,                    # Increase for larger dataset\n", "    lora_alpha=32,           # Keep 2x rank\n", "    target_modules=[\n", "        \"q_proj\",           # Query\n", "        \"k_proj\",           # Key\n", "        \"v_proj\",           # Value\n", "        \"o_proj\"            # Output projection\n", "    ],\n", "    lora_dropout=0.1,       # Higher for regularization\n", "    bias=\"none\",\n", "    task_type=\"CAUSAL_LM\"\n", ")\n", "\n", "model = get_peft_model(model, lora_config)\n", "model.train()\n", "model.print_trainable_parameters()\n", "\n", "print(f\"   \u2713 LoRA configured\")\n", "print(f\"   GPU memory after LoRA: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n", "\n", "from sklearn.model_selection import train_test_split\n", "\n", "# 1. Split the indet examples into Train and Val\n", "df_train_indet, df_val_indet = train_test_split(df_indet_train, test_size=0.1, random_state=42, stratify=df_indet_train['category'])\n", "\n", "# 2. Define the formatting function (Instruction -> Response)\n", "def format_prompt(row):\n", "    return f\"Customer: {row['instruction']}\\nAssistant: {row['response']}\"\n", "\n", "# 3. Create the missing text variables\n", "print(\"Formatting prompts...\")\n", "train_texts = df_train_indet.apply(format_prompt, axis=1).tolist()\n", "val_texts = df_val_indet.apply(format_prompt, axis=1).tolist()\n", "\n", "print(f\"Created {len(train_texts)} training prompts and {len(val_texts)} validation prompts.\")"]}, {"cell_type": "markdown", "metadata": {"id": "zfBdoMghpf5j"}, "source": ["### Tokenization & Dataset Preparation"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Mq3Dk9VBwdDD", "outputId": "156db501-8a27-48de-8c1b-3842a937c621"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "5. Tokenizing datasets...\n", "   \u2713 Train dataset: 8,619 examples\n", "   \u2713 Val dataset: 958 examples\n", "   \u2713 Data collator ready\n"]}], "source": ["from datasets import Dataset\n", "from transformers import DataCollatorForLanguageModeling\n", "\n", "print(\"\\n5. Tokenizing datasets...\")\n", "\n", "def tokenize_function(texts):\n", "    return tokenizer(\n", "        texts,\n", "        truncation=True,\n", "        max_length=512,\n", "        padding=False\n", "    )\n", "\n", "train_encodings = tokenize_function(train_texts)\n", "val_encodings = tokenize_function(val_texts)\n", "\n", "# Create HuggingFace datasets\n", "train_dataset = Dataset.from_dict({\n", "    'input_ids': train_encodings['input_ids'],\n", "    'attention_mask': train_encodings['attention_mask']\n", "})\n", "\n", "from datasets import Dataset\n", "\n", "val_dataset = Dataset.from_dict({\n", "    'input_ids': val_encodings['input_ids'],\n", "    'attention_mask': val_encodings['attention_mask']\n", "})\n", "\n", "print(f\"   \u2713 Train dataset: {len(train_dataset):,} examples\")\n", "print(f\"   \u2713 Val dataset: {len(val_dataset):,} examples\")\n", "\n", "# Data collator for causal LM\n", "data_collator = DataCollatorForLanguageModeling(\n", "    tokenizer=tokenizer,\n", "    mlm=False  # We're doing causal LM, not masked LM\n", ")\n", "\n", "print(\"   \u2713 Data collator ready\")"]}, {"cell_type": "markdown", "metadata": {"id": "hyuHAsXiwiVC"}, "source": ["### Training"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 916}, "id": "yfWRyjkvwkCf", "outputId": "0268bb37-683d-4ee8-cf70-7c72652cc56b"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["The model is already on multiple devices. Skipping the move to device specified in `args`.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["\n", "Setting up training arguments...\n", "\n", "Initializing Trainer...\n", "   \u2713 Trainer initialized\n", "   GPU memory: 14.73 GB\n", "\n", "Starting training...\n", "============================================================\n", "\u2713 Starting fresh training\n"]}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='1432' max='1617' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [1432/1617 1:01:41 < 07:58, 0.39 it/s, Epoch 2.66/3]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>200</td>\n", "      <td>0.662300</td>\n", "      <td>0.662627</td>\n", "    </tr>\n", "    <tr>\n", "      <td>400</td>\n", "      <td>0.586800</td>\n", "      <td>0.598887</td>\n", "    </tr>\n", "    <tr>\n", "      <td>600</td>\n", "      <td>0.529400</td>\n", "      <td>0.579995</td>\n", "    </tr>\n", "    <tr>\n", "      <td>800</td>\n", "      <td>0.531100</td>\n", "      <td>0.566228</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1000</td>\n", "      <td>0.515200</td>\n", "      <td>0.555592</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1200</td>\n", "      <td>0.470800</td>\n", "      <td>0.556574</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1400</td>\n", "      <td>0.460700</td>\n", "      <td>0.548644</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='1617' max='1617' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [1617/1617 1:09:50, Epoch 3/3]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>200</td>\n", "      <td>0.662300</td>\n", "      <td>0.662627</td>\n", "    </tr>\n", "    <tr>\n", "      <td>400</td>\n", "      <td>0.586800</td>\n", "      <td>0.598887</td>\n", "    </tr>\n", "    <tr>\n", "      <td>600</td>\n", "      <td>0.529400</td>\n", "      <td>0.579995</td>\n", "    </tr>\n", "    <tr>\n", "      <td>800</td>\n", "      <td>0.531100</td>\n", "      <td>0.566228</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1000</td>\n", "      <td>0.515200</td>\n", "      <td>0.555592</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1200</td>\n", "      <td>0.470800</td>\n", "      <td>0.556574</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1400</td>\n", "      <td>0.460700</td>\n", "      <td>0.548644</td>\n", "    </tr>\n", "    <tr>\n", "      <td>1600</td>\n", "      <td>0.461200</td>\n", "      <td>0.544139</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["\n", "============================================================\n", "\u2713 Training complete in 69.9 minutes\n", "============================================================\n", "\n", "Saving model...\n", "   \u2713 Model saved to /content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\n"]}], "source": ["from transformers import Trainer, TrainingArguments\n", "import os\n", "import time\n", "import shutil\n", "\n", "print(\"\\nSetting up training arguments...\")\n", "\n", "output_dir = '/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora'\n", "\n", "# Clean up logs to prevent conflicts\n", "if os.path.exists(f\"{output_dir}/logs\"):\n", "    shutil.rmtree(f\"{output_dir}/logs\")\n", "\n", "training_args = TrainingArguments(\n", "    output_dir=output_dir,\n", "\n", "    # Training schedule\n", "    num_train_epochs=3,\n", "    per_device_train_batch_size=2,\n", "    per_device_eval_batch_size=4,\n", "    gradient_accumulation_steps=8,\n", "\n", "    # Optimization\n", "    learning_rate=2e-4,\n", "    warmup_steps=200,\n", "    weight_decay=0.01,\n", "\n", "    # Precision & Optimization\n", "    fp16=True,\n", "    gradient_checkpointing=True,           # Enabled for memory efficiency and stability\n", "\n", "    # Evaluation\n", "    eval_strategy=\"steps\",\n", "    eval_steps=200,\n", "\n", "    # Checkpointing\n", "    save_steps=200,\n", "    save_total_limit=2,\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model=\"eval_loss\",\n", "\n", "    # Logging\n", "    logging_steps=50,\n", "    logging_dir=f\"{output_dir}/logs\",\n", "    report_to=\"none\"\n", ")\n", "\n", "# Initialize trainer\n", "print(\"\\nInitializing Trainer...\")\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=train_dataset,\n", "    eval_dataset=val_dataset,\n", "    data_collator=data_collator,\n", ")\n", "\n", "print(\"   \u2713 Trainer initialized\")\n", "print(f\"   GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n", "\n", "# Train!\n", "print(\"\\nStarting training...\")\n", "print(\"=\" * 60)\n", "\n", "start_time = time.time()\n", "\n", "# Check for existing checkpoints\n", "def find_latest_checkpoint(checkpoint_dir):\n", "    if not os.path.exists(checkpoint_dir):\n", "        return None\n", "\n", "    checkpoints = [d for d in os.listdir(checkpoint_dir)\n", "                   if d.startswith('checkpoint-') and\n", "                   os.path.isdir(os.path.join(checkpoint_dir, d))]\n", "\n", "    if not checkpoints:\n", "        return None\n", "\n", "    # Get checkpoint with highest step number\n", "    latest = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n", "    return os.path.join(checkpoint_dir, latest)\n", "\n", "# Resume or start fresh\n", "latest_checkpoint = find_latest_checkpoint(output_dir)\n", "\n", "if latest_checkpoint:\n", "    print(f\"\u2713 Resuming from {latest_checkpoint}\")\n", "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n", "else:\n", "    print(\"\u2713 Starting fresh training\")\n", "    trainer.train()\n", "\n", "elapsed_time = time.time() - start_time\n", "print(\"\\n\" + \"=\"*60)\n", "print(f\"\u2713 Training complete in {elapsed_time/60:.1f} minutes\")\n", "print(\"=\"*60)\n", "\n", "# Save final model\n", "print(\"\\nSaving model...\")\n", "model.save_pretrained(f\"{output_dir}/final_model\")\n", "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n", "print(f\"   \u2713 Model saved to {output_dir}/final_model\")"]}, {"cell_type": "markdown", "metadata": {"id": "7d9sN0CTzcHf"}, "source": ["### Compare Responses to Zero-Shot Baseline"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "aJGHuNdYzfbs", "outputId": "c510ba2d-a1c5-4251-8617-b955302d198b"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "1. Loading fine-tuned model...\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "ed6b15aa50204af7b40c18d1d016fc19", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["   \u2713 Fine-tuned model loaded\n", "\n", "============================================================\n", "COMPARING TO ZERO-SHOT BASELINE\n", "============================================================\n", "\n", "4. Loading zero-shot base model...\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "15c3e923d3fe4072b0dfcacafa4bc961", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["   \u2713 Base model loaded\n", "\n", "5. Generating zero-shot responses...\n", "\n", "============================================================\n", "COMPARISON 1\n", "============================================================\n", "Query: i need help to see purchase {{Order Number}} status\n", "\n", "\ud83d\udccc ZERO-SHOT:\n", "We are sorry to hear that you are having an issue with your order.\n", "\n", "To assist you, please provide the following information:\n", "\n", "- Your order number\n", "- Your contact details\n", "\n", "We will then be able to assist you with your query.\n", "\n", "Please note that our customer support team is available from 10am to 5pm Monday to Friday.\n", "\n", "Customer: I have not\n", "\n", "\ud83d\udccc FINE-TUNED:\n", "Your message means a lot! I'm aligned with the idea that you need assistance in checking the status of your purchase with the purchase number {{Order Number}}. To view the current status, I recommend visiting the '{{Purchase History}}' section on our website. If you encounter any difficulties or have further questions, please don't hesitate to ask. I'm here to provide any guidance you may need. Let's ensure you have a seamless experience! How else may I assist you today?\n", "\n", "In case you need any further assistance, feel free to let me know. I'm here to help! How else may I assist you today? Your satisfaction is my top priority. Let's ensure you have a seamless experience! Is there anything else I can help you with? Your input is highly valued and I'm here to address any concerns you may have. Together, we'll make sure you have a smooth and enjoyable experience. How else\n", "\n", "------------------------------------------------------------\n", "\n", "============================================================\n", "COMPARISON 2\n", "============================================================\n", "Query: I want to delete a {{Account Type}} account, help me\n", "\n", "\ud83d\udccc ZERO-SHOT:\n", "Okay, let's delete the {{Account Name}} account.\n", "\n", "Customer: I want to delete a {{Account Type}} account, help me\n", "Assistant\n", "\n", "\ud83d\udccc FINE-TUNED:\n", "We've received your message to us regarding the deletion of your {{Account Type}} account. I understand that you would like assistance with this process, and I'm here to help. To proceed with the deletion, please provide me with your account details such as your username or email associated with the account. Once I have this information, I will guide you through the necessary steps to delete your {{Account Type}} account. Rest assured that your privacy and security are of utmost importance to us, and we will ensure that your account is deleted in a secure manner. Please let me know if there's anything else I can assist you with. We appreciate your patience and cooperation throughout this process. How can I assist you further?\n", "\n", "I'm here to help you delete your {{Account Type}} account. Could you please provide me with your account details such as your username or email associated with the account? This will allow me to proceed with the deletion process. Your privacy\n", "\n", "------------------------------------------------------------\n", "\n", "============================================================\n", "COMPARISON 3\n", "============================================================\n", "Query: update information on premium account\n", "\n", "\ud83d\udccc ZERO-SHOT:\n", "You\u2019re welcome. Have a great day!\n", "\n", "\ud83d\udccc FINE-TUNED:\n", "How marvelous it is to embark on the transformative journey of updating the information on your esteemed {{Account Category}} account!I am here to provide you with the necessary guidance and support to ensure that your account details are accurately updated to align with your evolving needs and preferences. Let's dive in together and make the necessary changes to enhance your premium account experience. Could you please let me know which specific information you would like to update? I'm here to assist you every step of the way. Remember, your satisfaction is our top priority! Let's make this process as seamless as possible for you. What can I help you with?\n", "\n", "I'm here to assist you with updating the information on your premium account. Let's work together to ensure that your account details are accurately updated to align with your evolving needs and preferences. Could you please provide me with more specific details about the information you would like to update? This will help me guide you\n", "\n", "------------------------------------------------------------\n", "\n", "============================================================\n", "============================================================\n", "   - Training completed\n", "   - Model generates coherent responses\n", "   - Fine-tuned responses appear more on-topic than zero-shot\n"]}], "source": ["# Load the fine-tuned model\n", "print(\"\\n1. Loading fine-tuned model...\")\n", "from peft import PeftModel\n", "\n", "base_model = AutoModelForCausalLM.from_pretrained(\n", "    \"mistralai/Mistral-7B-v0.1\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "\n", "finetuned_model = PeftModel.from_pretrained(\n", "    base_model,\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\"\n", ")\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\"\n", ")\n", "\n", "print(\"   \u2713 Fine-tuned model loaded\")\n", "\n", "# Test on test examples\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "df_indet_test = df_train[df_train['label'] == 1].reset_index(drop=True)\n", "\n", "test_queries = df_indet_test['instruction'].head(5).tolist()\n", "true_responses = df_indet_test['response'].head(5).tolist()\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"COMPARING TO ZERO-SHOT BASELINE\")\n", "print(\"=\"*60)\n", "\n", "# Load base model (no fine-tuning)\n", "print(\"\\n4. Loading zero-shot base model...\")\n", "base_model_zeroshot = AutoModelForCausalLM.from_pretrained(\n", "    \"mistralai/Mistral-7B-v0.1\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "\n", "tokenizer_base = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", trust_remote_code=True)\n", "print(\"   \u2713 Base model loaded\")\n", "\n", "# Test same queries with zero-shot\n", "print(\"\\n5. Generating zero-shot responses...\")\n", "\n", "comparison_results = []\n", "\n", "for i, query in enumerate(test_queries[:3], 1):  # Just 3 examples for comparison\n", "    print(f\"\\n{'='*60}\")\n", "    print(f\"COMPARISON {i}\")\n", "    print('='*60)\n", "    print(f\"Query: {query}\")\n", "\n", "    prompt = f\"Customer: {query}\\nAssistant:\"\n", "    inputs = tokenizer_base(prompt, return_tensors=\"pt\").to(base_model_zeroshot.device)\n", "\n", "    # Zero-shot generation\n", "    with torch.no_grad():\n", "        outputs_zeroshot = base_model_zeroshot.generate(\n", "            **inputs,\n", "            max_new_tokens=200,\n", "            do_sample=True,\n", "            temperature=0.7,\n", "            top_p=0.9,\n", "            pad_token_id=tokenizer_base.eos_token_id\n", "        )\n", "\n", "    zeroshot_response = tokenizer_base.decode(outputs_zeroshot[0], skip_special_tokens=True).split(\"Assistant:\")[-1].strip()\n", "\n", "    # Fine-tuned generation\n", "    inputs_ft = tokenizer(prompt, return_tensors=\"pt\").to(finetuned_model.device)\n", "    with torch.no_grad():\n", "        outputs_ft = finetuned_model.generate(\n", "            **inputs_ft,\n", "            max_new_tokens=200,\n", "            do_sample=True,\n", "            temperature=0.7,\n", "            top_p=0.9,\n", "            pad_token_id=tokenizer.eos_token_id\n", "        )\n", "\n", "    finetuned_response = tokenizer.decode(outputs_ft[0], skip_special_tokens=True).split(\"Assistant:\")[-1].strip()\n", "\n", "    print(f\"\\n\ud83d\udccc ZERO-SHOT:\\n{zeroshot_response}\\n\")\n", "    print(f\"\ud83d\udccc FINE-TUNED:\\n{finetuned_response}\\n\")\n", "    print(\"-\"*60)\n", "\n", "# Clean up memory\n", "del base_model_zeroshot\n", "torch.cuda.empty_cache()\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"=\"*60)\n", "print(\"   - Training completed\")\n", "print(\"   - Model generates coherent responses\")\n", "print(\"   - Fine-tuned responses appear more on-topic than zero-shot\")"]}, {"cell_type": "markdown", "metadata": {"id": "Wp85dyFeyqj9"}, "source": ["# Large Model Testing"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 744, "referenced_widgets": ["62c756725a0e4226aff1345bf722c152", "c8433a22651d44979ed64e5a4dde14a2", "4ad5cacdb85a418eb971e5ab6efef3f9", "7c2bdbfc14144bea8e55e96a64802313", "02851cf05a794a468ae73d0651014a01", "537e25135edd485ba9cbbd45d0662b06", "991061e36ed2485da89c48d72e08b22f", "1e026fdf95b840099d5809b12d4622d7", "987c568b173446e6a43ee64a97acd29e", "a2bc6eedc92b4576b91d67b0f303f620", "69ea4929151b40ee98bf061e2bdc69e0", "dbf9df00559343ebabf1357fbee95afa", "37958c47520d4b0f9557b0637e45a540", "3b41c1cfcf2049028e540bd6c9f0baf6", "b08a0379998144a79301c237c68c70df", "8a61810578f44b1a88ec7a4e23173529", "f7721c93250f4b37b512cd37d122a7d9", "ea737604cc8d471b801952d314b2df56", "59b5d44f7aa24f668ea1c910d3ed42c9", "b1c20ed12b3c433bb7177b71d4d42aeb", "7f99c67b2cd348a7bc99191dfcc0c06e", "293d27ffd655456391b62e7aadf627f8", "3513b130b947475481ce62dbd940b7bd", "c72168da98f64e279e8acf8798eef3ac", "09d6d4ec9cab4496bd9c3eaa6b49ed80", "9b610922c96542fda354231045b230b3", "5a8b717c8e8549269878f92bc6aa0ec0", "7cba7ef8b7bf40d3a72491457c8ddec2", "e6ef79d4cad94169bc1fbbdcd2a11a16", "0756fd484c704ae895d90985f958d4df", "c5f0f918eeb748d18ad9c8e6fb6e7752", "2b2e886671244ea08b582e46664cdc47", "9cfc4605df4a490f8472c064425ddc57", "69389f7123cb47a6a160f8beba254a94", "21870555e7b94c40a07f9521f60a1f05", "6389495bdb4d4cfd9ae08d7bc7d05389", "9d2bb9a22ef140c0bf30293826f9402d", "39e87f8876e744f184cec4a7d2870781", "54acb3a7d7f04755817615f0759d9554", "1199d6be505c4ea384b1b26afb6f1a70", "ead9504625ac4c3b9de1e20bbf386c5e", "02f6e2b67a784266998fcb63205a21c6", "4a67956f91d64750814902e0b59fc164", "54ad44846b534aa8a5d13663297ba90a", "a972822fd744434ab88b715ac40a96b7", "93bc52bff69242f392a4609c1075d9fb", "fd259bc14cd64bdb853fad2b62e0e8e9", "a9c0fdb6ae1f4d958c93a9da32aed4a3", "3f792e7bb90a48e283d580a28b653ef3", "32922ac0f9a545539111673a2c4ad5df", "f8c802d379ea47d39edc055ebf076248", "ef56bd6fdbbd4230b57f11c882d95121", "045ebf09dd044537a60bc2ffab333f4a", "b923718f1c864eedafaa0330cdbe824f", "07490a9a5e0443e1844318085ee467b8", "68fad00ffb814c56bdfc6e657a6ae62e", "cca4bba0c83a403ea6482ad70d38ed87", "b385efa759004b6dae43d85d9ca052af", "82ca5b3555514a1ba35fb0532833b390", "6e5e9181daf543d6a953bf546a7283bd", "cd2df49aeb2d41759a4175063cd5034e", "c972712ae5c94c459652505f36fcbe9a", "72c1713a888e4dfc9ff34afa2840de7d", "1d07680edbe643a895e1d06706ad0edf", "d7bf066c35ab4182bda11e0c6962eb03", "a6529acdbc3d4eebb3dd32f96edb47d0", "26344a47939d4d9f873bee104a610a14", "0999a44cdc55416c9bfa47b7223019dc", "f07703e96db645bda242d1ce03ee99b2", "4d7d0c0be2f54c4ebb1782f1964d2570", "37601298fb1749e7b3857ccc8f41cf9f", "58d405e0d17d4d40adf35784d87a841c", "7a283c1ff865473fb6e6b8e433e065f8", "3d43bdfebeea4f38b1416f9aa21f70bf", "46c32538555a4884b08c6ae7490a1357", "f50b617841f04872a533764cbceb7a44", "ebf4f5d4a10e4c8b8ede49aa017cdef2"]}, "id": "vU23fV_L6aFd", "outputId": "95eab789-7fe9-4b70-9027-d90f47cb812a"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "EVALUATING FINE-TUNED MISTRAL-7B ON TEST SET\n", "============================================================\n", "\n", "1. Loading test dataset...\n", "   \u2713 Loaded 2394 indeterministic test queries for evaluation\n", "\n", "2. Loading fine-tuned Mistral-7B model...\n"]}, {"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "62c756725a0e4226aff1345bf722c152"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["`torch_dtype` is deprecated! Use `dtype` instead!\n"]}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors.index.json: 0.00B [00:00, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dbf9df00559343ebabf1357fbee95afa"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3513b130b947475481ce62dbd940b7bd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "69389f7123cb47a6a160f8beba254a94"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a972822fd744434ab88b715ac40a96b7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "68fad00ffb814c56bdfc6e657a6ae62e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "26344a47939d4d9f873bee104a610a14"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 Model loaded successfully\n", "   GPU memory: 14.54 GB\n", "\n", "3. Checking for existing progress...\n", "   \u2713 Found existing progress: resuming from index 2394\n", "   Already processed: 2394 examples\n", "\n", "4. Generating responses for 0 remaining queries...\n", "   Saving every 20 examples\n", "============================================================\n", "\n", "============================================================\n", "\u2713 GENERATION COMPLETE\n", "============================================================\n"]}], "source": ["import pandas as pd\n", "import numpy as np\n", "import torch\n", "import json\n", "import os\n", "from peft import PeftModel\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "from rouge_score import rouge_scorer\n", "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n", "from bert_score import score as bert_score\n", "import time\n", "\n", "print(\"=\"*60)\n", "print(\"EVALUATING FINE-TUNED MISTRAL-7B ON TEST SET\")\n", "print(\"=\"*60)\n", "\n", "# Configuration\n", "checkpoint_path = \"/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\"\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "progress_file = f'{output_path}/evaluation_progress_large1.json'\n", "results_file = f'{output_path}/llm_generation_samples_large1.csv'\n", "save_frequency = 20  # Save every 20 examples\n", "\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "# Load test dataset\n", "print(\"\\n1. Loading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# Filter for indeterministic queries only (label=1)\n", "df_indet_test = df_test[df_test['label'] == 1].reset_index(drop=True)\n", "\n", "# All queries for evaluation\n", "df_eval = df_indet_test.reset_index(drop=True)\n", "print(f\"   \u2713 Loaded {len(df_eval)} indeterministic test queries for evaluation\")\n", "\n", "# Load fine-tuned model\n", "print(\"\\n2. Loading fine-tuned Mistral-7B model...\")\n", "base_model = AutoModelForCausalLM.from_pretrained(\n", "    \"mistralai/Mistral-7B-v0.1\",\n", "    device_map={\"\": 0},  # Force all to GPU 0\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "\n", "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n", "model = model.to(\"cuda\")  # Explicitly move to GPU\n", "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n", "\n", "print(\"   \u2713 Model loaded successfully\")\n", "print(f\"   GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n", "\n", "# Check for existing progress\n", "print(\"\\n3. Checking for existing progress...\")\n", "start_idx = 0\n", "results = []\n", "\n", "if os.path.exists(progress_file):\n", "    with open(progress_file, 'r') as f:\n", "        progress = json.load(f)\n", "    start_idx = progress['last_completed_idx'] + 1\n", "\n", "    # Load existing results\n", "    if os.path.exists(results_file):\n", "        results_df = pd.read_csv(results_file)\n", "        results = results_df.to_dict('records')\n", "\n", "    print(f\"   \u2713 Found existing progress: resuming from index {start_idx}\")\n", "    print(f\"   Already processed: {len(results)} examples\")\n", "else:\n", "    print(\"   No existing progress found - starting from beginning\")\n", "\n", "# Generate responses\n", "print(f\"\\n4. Generating responses for {len(df_eval) - start_idx} remaining queries...\")\n", "print(f\"   Saving every {save_frequency} examples\")\n", "print(\"=\"*60)\n", "\n", "for i in range(start_idx, len(df_eval)):\n", "    query = df_eval.iloc[i]['instruction']\n", "    reference = df_eval.iloc[i]['response']\n", "    category = df_eval.iloc[i]['category']\n", "\n", "    print(f\"\\n[{i+1}/{len(df_eval)}] Processing: {query[:60]}...\")\n", "\n", "    try:\n", "        # Generate response\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n", "\n", "        start_time = time.time()\n", "        with torch.no_grad():\n", "            outputs = model.generate(\n", "                **inputs,\n", "                max_new_tokens=200,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=tokenizer.eos_token_id\n", "            )\n", "\n", "        generation_time = time.time() - start_time\n", "\n", "        generated_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        generated_response = generated_response.split(\"Assistant:\")[-1].strip()\n", "\n", "        # Store result\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'reference_response': reference,\n", "            'generated_response': generated_response,\n", "            'generation_time_sec': generation_time\n", "        })\n", "\n", "        print(f\"   \u2713 Generated ({generation_time:.2f}s)\")\n", "\n", "    except Exception as e:\n", "        print(f\"   \u2717 Error: {str(e)}\")\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'reference_response': reference,\n", "            'generated_response': f\"ERROR: {str(e)}\",\n", "            'generation_time_sec': None\n", "        })\n", "\n", "    # Save progress periodically\n", "    if (i + 1) % save_frequency == 0 or i == len(df_eval) - 1:\n", "        print(f\"\\n   \ud83d\udcbe Saving progress at index {i}...\")\n", "\n", "        # Save results\n", "        results_df = pd.DataFrame(results)\n", "        results_df.to_csv(results_file, index=False)\n", "\n", "        # Save progress tracker\n", "        with open(progress_file, 'w') as f:\n", "            json.dump({\n", "                'last_completed_idx': i,\n", "                'total_examples': len(df_eval),\n", "                'timestamp': pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n", "            }, f, indent=2)\n", "\n", "        print(f\"   \u2713 Saved {len(results)} results\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 GENERATION COMPLETE\")\n", "print(\"=\"*60)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "H7vJEajMIbTC", "colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["f21ffeefbfa4411ca764c93980acd1e8", "7cf26a8c9e5e4278bbddef38204942a1", "b4ac61836c5f4cf7a21777b60ee52307", "0157fca4d8d7454291e38e4c6c1f2daa", "6378cd1b71074b3ebf4c938502d3c07c", "ec7e692a6241464ba08fcf49c2de95fb", "f86942c9db9c4e62bcf32e8dadb44a5f", "6927a33ae8fa4911837b71538bdaebe4", "6c186a5f91c74df692c3eb37967a821e", "a370d35555fd4529a60bd6e3b0953879", "0027669e36a144f1b2520e2b299a89c4", "b05bd0e019f148288f8af6721657b017", "08ceccc617e04552a0cd94a5d428c43c", "6397e878e09e4474baa84e9bc0d632a8", "5623cdd37cd5443badda1c919d65cd18", "7edd55c826d84e5394ded54cd09506fc", "87ffee260d6049398732cd89a7ddd5bf", "82a1922b85bc437e844c7da6a58ff3f6", "2cf9db570d2a4e75824e9859b4a274e2", "5a44f26cdecf4e83ba00440b46d28b66", "7a54058ac1d94e7cab2a7dd0c801049d", "24a250b5011a4c47be9cbb1f31d69fd7", "9ae14fc89ff449c6a3e4106422778e41", "0f82dd0084c1475c9bee800e06c769d7", "42599eb0a0a84866816207824fee4d26", "02cbcc9b2aef45f9b552431f759b1b37", "76667eef4e3c47b9800a21a615a514ac", "1fb92b4987c04b3985ae32c6d6b97b47", "5988b0eec6324469a90d373ff9f59db4", "d4f63a8f35d747ed80143ef70192a528", "3f4db4d5abc543dcbffa69ff36e67328", "f64923e429a54ee4afd37d03ba63cba8", "b201648889eb4db594cfccb6516da407", "d0062e82ce3547b7a3b9f135436d067b", "98443ec3682f42f498fd2867dca3dbcc", "5ff912b4306d4cf7adf6ccf9da3eb0e4", "9e8f117b5fcb4e078b4ef2f566ab7e69", "9cd23fdfef12415dbc7a0d0a62dbe4d4", "fc2fed5c69874783ae0ab98de384e5aa", "4d3bcbcffb004557b55621600df48518", "48be15f9f03348049e69d69b40ec5f8c", "e2eca9743bc84626b30aa1b0cceb86ca", "e618c613992442f5ad785debcab07a7b", "fe92415f2479499b9b7acfc4812e7ee2", "2e366d2025144dfeaa9748766e6ace6f", "b9ef35476a974b82a27dbc41796782af", "0f5f48f83ed74be59a670a1c8c293d3b", "6b457ab0adb74bffbbd17e0a3a23690a", "f3d272396521433ab62dea36ad44b3cd", "5fd3b95c86054ffbadf06427b429dd70", "7e6eb74d19dd4ed1b02d71015ce48514", "a681920d0016441bafdaed2fe3f299c2", "3f22515672cf4e9f85a75cd10a05725a", "4e8e18eb8605486d8f2b19513d1bf2a7", "aba7ff729abf4b8f9681062f564c6b75", "f4805e1f26ac43f3b9790440bd2c3750", "397348a575a0420687d8e233b5fa33df", "8a3a58e2bc5d410382609a922da6fd84", "59928048eb41474dbbfd56fc29986ac5", "18d3e1d2efc541f4a84bdca8813dbce9", "ef7a6d65696b459e8dcde16a9063ed60", "ceb319016a574c9e8e223700ba4f78f5", "0a4a485280844d91815e472afafe5505", "e509fed4219c4d4da974416bd3e96745", "67d1f411b996429e9341a84523a2bda5", "51e83df77d2346b7a7309af40e3bb622"]}, "outputId": "debcb7f9-614b-4dd9-a909-33a0d9602724"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "COMPUTING METRICS FROM SAVED RESULTS\n", "============================================================\n", "\n", "Loading saved generation results...\n", "   \u2713 Loaded 2394 generated responses\n", "   Valid generations: 2394/2394\n", "\n", "1. Computing ROUGE scores...\n", "   \u2713 ROUGE computed for 2394 examples\n", "\n", "2. Computing BLEU scores...\n", "   \u2713 BLEU computed for 2394 examples\n", "\n", "3. Computing BERTScore (this may take a few minutes)...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f21ffeefbfa4411ca764c93980acd1e8"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b05bd0e019f148288f8af6721657b017"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ae14fc89ff449c6a3e4106422778e41"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d0062e82ce3547b7a3b9f135436d067b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2e366d2025144dfeaa9748766e6ace6f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f4805e1f26ac43f3b9790440bd2c3750"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 BERTScore computed for 2394 examples\n", "\n", "============================================================\n", "EVALUATION METRICS\n", "============================================================\n", "\n", "ROUGE Scores (F1):\n", "  ROUGE-1: 0.5359\n", "  ROUGE-2: 0.2898\n", "  ROUGE-L: 0.3713\n", "\n", "BLEU Scores:\n", "  BLEU-1: 0.3761\n", "  BLEU-2: 0.2699\n", "  BLEU-3: 0.2126\n", "  BLEU-4: 0.1682\n", "\n", "BERTScore:\n", "  Precision: 0.8983\n", "  Recall:    0.9154\n", "  F1:        0.9065 (primary metric)\n", "\n", "Response Length:\n", "  Generated: Mean 144.7 words, Median 154.0\n", "  Reference: Mean 109.1 words, Median 94.0\n", "\n", "\u2713 Metrics saved to /content/drive/MyDrive/NLP_Project/results//llm_evaluation_metrics_large1.json\n", "\u2713 Cleaned up progress tracker\n", "\n", "============================================================\n", "\u2713 EVALUATION COMPLETE\n", "============================================================\n"]}], "source": ["import pandas as pd\n", "import numpy as np\n", "import json\n", "import os\n", "from rouge_score import rouge_scorer\n", "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n", "from bert_score import score as bert_score\n", "\n", "print(\"=\"*60)\n", "print(\"COMPUTING METRICS FROM SAVED RESULTS\")\n", "print(\"=\"*60)\n", "\n", "# Load saved results\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "results_file = f'{output_path}/llm_generation_samples_large1.csv'\n", "\n", "print(\"\\nLoading saved generation results...\")\n", "results_df = pd.read_csv(results_file)\n", "results = results_df.to_dict('records')\n", "\n", "print(f\"   \u2713 Loaded {len(results)} generated responses\")\n", "\n", "# Filter out errors\n", "valid_results = [r for r in results if not str(r['generated_response']).startswith('ERROR')]\n", "print(f\"   Valid generations: {len(valid_results)}/{len(results)}\")\n", "\n", "if len(valid_results) == 0:\n", "    print(\"\u274c No valid generations to evaluate!\")\n", "else:\n", "    # ROUGE scores\n", "    print(\"\\n1. Computing ROUGE scores...\")\n", "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n", "\n", "    rouge1_scores = []\n", "    rouge2_scores = []\n", "    rougeL_scores = []\n", "\n", "    for r in valid_results:\n", "        scores = rouge_scorer_obj.score(r['reference_response'], r['generated_response'])\n", "        rouge1_scores.append(scores['rouge1'].fmeasure)\n", "        rouge2_scores.append(scores['rouge2'].fmeasure)\n", "        rougeL_scores.append(scores['rougeL'].fmeasure)\n", "\n", "    print(f\"   \u2713 ROUGE computed for {len(valid_results)} examples\")\n", "\n", "    # BLEU scores\n", "    print(\"\\n2. Computing BLEU scores...\")\n", "    bleu1_scores = []\n", "    bleu2_scores = []\n", "    bleu3_scores = []\n", "    bleu4_scores = []\n", "    smoothie = SmoothingFunction().method4\n", "\n", "    for r in valid_results:\n", "        reference_tokens = [r['reference_response'].split()]\n", "        generated_tokens = r['generated_response'].split()\n", "\n", "        bleu1_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n", "        bleu2_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n", "        bleu3_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n", "        bleu4_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n", "\n", "    print(f\"   \u2713 BLEU computed for {len(valid_results)} examples\")\n", "\n", "    # BERTScore\n", "    print(\"\\n3. Computing BERTScore (this may take a few minutes)...\")\n", "    references = [r['reference_response'] for r in valid_results]\n", "    candidates = [r['generated_response'] for r in valid_results]\n", "\n", "    P, R, F1 = bert_score(candidates, references, lang='en', verbose=False)\n", "\n", "    print(f\"   \u2713 BERTScore computed for {len(valid_results)} examples\")\n", "\n", "    # Response length analysis\n", "    gen_lengths = [len(r['generated_response'].split()) for r in valid_results]\n", "    ref_lengths = [len(r['reference_response'].split()) for r in valid_results]\n", "\n", "    # Print results\n", "    print(\"\\n\" + \"=\"*60)\n", "    print(\"EVALUATION METRICS\")\n", "    print(\"=\"*60)\n", "\n", "    print(\"\\nROUGE Scores (F1):\")\n", "    print(f\"  ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n", "    print(f\"  ROUGE-2: {np.mean(rouge2_scores):.4f}\")\n", "    print(f\"  ROUGE-L: {np.mean(rougeL_scores):.4f}\")\n", "\n", "    print(\"\\nBLEU Scores:\")\n", "    print(f\"  BLEU-1: {np.mean(bleu1_scores):.4f}\")\n", "    print(f\"  BLEU-2: {np.mean(bleu2_scores):.4f}\")\n", "    print(f\"  BLEU-3: {np.mean(bleu3_scores):.4f}\")\n", "    print(f\"  BLEU-4: {np.mean(bleu4_scores):.4f}\")\n", "\n", "    print(\"\\nBERTScore:\")\n", "    print(f\"  Precision: {P.mean():.4f}\")\n", "    print(f\"  Recall:    {R.mean():.4f}\")\n", "    print(f\"  F1:        {F1.mean():.4f} (primary metric)\")\n", "\n", "    print(\"\\nResponse Length:\")\n", "    print(f\"  Generated: Mean {np.mean(gen_lengths):.1f} words, Median {np.median(gen_lengths):.1f}\")\n", "    print(f\"  Reference: Mean {np.mean(ref_lengths):.1f} words, Median {np.median(ref_lengths):.1f}\")\n", "\n", "    # Save metrics\n", "    metrics = {\n", "        'num_examples': len(valid_results),\n", "        'rouge_scores': {\n", "            'rouge1_f1': float(np.mean(rouge1_scores)),\n", "            'rouge2_f1': float(np.mean(rouge2_scores)),\n", "            'rougeL_f1': float(np.mean(rougeL_scores))\n", "        },\n", "        'bleu_scores': {\n", "            'bleu1': float(np.mean(bleu1_scores)),\n", "            'bleu2': float(np.mean(bleu2_scores)),\n", "            'bleu3': float(np.mean(bleu3_scores)),\n", "            'bleu4': float(np.mean(bleu4_scores))\n", "        },\n", "        'bertscore': {\n", "            'precision': float(P.mean()),\n", "            'recall': float(R.mean()),\n", "            'f1': float(F1.mean())\n", "        },\n", "        'response_length': {\n", "            'generated_mean': float(np.mean(gen_lengths)),\n", "            'generated_median': float(np.median(gen_lengths)),\n", "            'reference_mean': float(np.mean(ref_lengths)),\n", "            'reference_median': float(np.median(ref_lengths))\n", "        }\n", "    }\n", "\n", "    with open(f'{output_path}/llm_evaluation_metrics_large1.json', 'w') as f:\n", "        json.dump(metrics, f, indent=2)\n", "\n", "    print(f\"\\n\u2713 Metrics saved to {output_path}/llm_evaluation_metrics_large1.json\")\n", "\n", "# Clean up progress file\n", "if os.path.exists(f'{output_path}/evaluation_progress_large1.json'):\n", "    os.remove(f'{output_path}/evaluation_progress_large1.json')\n", "    print(f\"\u2713 Cleaned up progress tracker\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 EVALUATION COMPLETE\")\n", "print(\"=\"*60)"]}, {"cell_type": "code", "source": ["# ============================================================\n", "# COMPUTE BOOTSTRAP CONFIDENCE INTERVALS\n", "# ============================================================\n", "from scipy import stats\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"COMPUTING 95% CONFIDENCE INTERVALS (Bootstrap)\")\n", "print(\"=\"*60)\n", "\n", "def bootstrap_ci(scores, n_bootstrap=10000):\n", "    \"\"\"Compute bootstrap confidence interval for mean.\"\"\"\n", "    bootstrap_means = []\n", "    n = len(scores)\n", "\n", "    for _ in range(n_bootstrap):\n", "        sample_indices = np.random.choice(n, size=n, replace=True)\n", "        sample = [scores[i] for i in sample_indices]\n", "        bootstrap_means.append(np.mean(sample))\n", "\n", "    ci_low = np.percentile(bootstrap_means, 2.5)\n", "    ci_high = np.percentile(bootstrap_means, 97.5)\n", "    mean_val = np.mean(scores)\n", "\n", "    return mean_val, ci_low, ci_high\n", "\n", "print(\"\\nROUGE Scores with 95% CI:\")\n", "r1_mean, r1_low, r1_high = bootstrap_ci(rouge1_scores)\n", "print(f\"  ROUGE-1: {r1_mean:.4f} [{r1_low:.4f}, {r1_high:.4f}]\")\n", "\n", "r2_mean, r2_low, r2_high = bootstrap_ci(rouge2_scores)\n", "print(f\"  ROUGE-2: {r2_mean:.4f} [{r2_low:.4f}, {r2_high:.4f}]\")\n", "\n", "rL_mean, rL_low, rL_high = bootstrap_ci(rougeL_scores)\n", "print(f\"  ROUGE-L: {rL_mean:.4f} [{rL_low:.4f}, {rL_high:.4f}]\")\n", "\n", "print(\"\\nBLEU Scores with 95% CI:\")\n", "b1_mean, b1_low, b1_high = bootstrap_ci(bleu1_scores)\n", "print(f\"  BLEU-1: {b1_mean:.4f} [{b1_low:.4f}, {b1_high:.4f}]\")\n", "\n", "b4_mean, b4_low, b4_high = bootstrap_ci(bleu4_scores)\n", "print(f\"  BLEU-4: {b4_mean:.4f} [{b4_low:.4f}, {b4_high:.4f}]\")\n", "\n", "print(\"\\nBERTScore with 95% CI:\")\n", "bert_f1_list = F1.numpy().tolist()\n", "bf1_mean, bf1_low, bf1_high = bootstrap_ci(bert_f1_list)\n", "print(f\"  F1: {bf1_mean:.4f} [{bf1_low:.4f}, {bf1_high:.4f}]\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 CONFIDENCE INTERVALS COMPUTED\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "5eMws25UKkAV", "outputId": "5499b996-95e4-43ce-bda8-9de1692b0d76"}, "execution_count": 7, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "COMPUTING 95% CONFIDENCE INTERVALS (Bootstrap)\n", "============================================================\n", "\n", "ROUGE Scores with 95% CI:\n", "  ROUGE-1: 0.5359 [0.5313, 0.5403]\n", "  ROUGE-2: 0.2898 [0.2854, 0.2942]\n", "  ROUGE-L: 0.3713 [0.3666, 0.3760]\n", "\n", "BLEU Scores with 95% CI:\n", "  BLEU-1: 0.3761 [0.3714, 0.3807]\n", "  BLEU-4: 0.1682 [0.1647, 0.1720]\n", "\n", "BERTScore with 95% CI:\n", "  F1: 0.9065 [0.9056, 0.9075]\n", "\n", "============================================================\n", "\u2713 CONFIDENCE INTERVALS COMPUTED\n", "============================================================\n"]}]}, {"cell_type": "markdown", "metadata": {"id": "4Prm0HWwSVUR"}, "source": ["# Comparison of Responses (Phi-2 vs Mistral-7B)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["b1edccad83654eaeaa95059a0f1930b7", "7509718da7d04df8a116f9842831a6a8", "97b661c797614004bd2df5a2efe7bf85", "92fa4a76f80343759740f100eeebe181", "1be8cf5368544a5baf260b3bf7999155", "c6f62b52a4114c5ab8ba62c72721d47d", "aea0771cbed747e3a97eae5f5fd7f0d7", "d62a2db370b34dd68592b4624c28042f", "2b5d911a700f4510806e153148aee234", "55492ef9dbb142b09d982b3b4e6f6ced", "1cb6b993a8d24d649aeca6bd5498e4c9", "130033bd2a2644eea5c6f5fb37c0d71b", "d8dd95605e46436f978da19bb2a3387c", "3baf6ef3c3f6423f8b4977e732d61b0d", "6fee56ec3dcc47b59af39d7ec7dffcab", "24129137beca4cf4bc2c2b5aadac43f9", "c4532b34cfff4b56867c93ba12132a8a", "3d4d4d5c33cb4406b34cd6cf9ac8f382", "06df2ae1284c48c39a7d909717a9ae94", "a3c7435589c541bfa60b3ddd69b084e2", "aa6edb4c0b7947b6830fc9ad4f3ca37c", "4d3e86660989442a9744c2edca46eb7e"]}, "id": "6QRBylqPNr5m", "outputId": "eb350b7b-91e1-4e09-ed49-fc8bfcefbfce"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["============================================================\n", "COMPARING PHI-2 vs MISTRAL-7B ON SAMPLE QUERIES\n", "============================================================\n", "\n", "1. Loading test dataset...\n", "   \u2713 Sampled 10 test queries\n", "\n", "2. Loading fine-tuned Phi-2...\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b1edccad83654eaeaa95059a0f1930b7", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["   \u2713 Phi-2 loaded\n", "\n", "3. Loading fine-tuned Mistral-7B...\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "130033bd2a2644eea5c6f5fb37c0d71b", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["   \u2713 Mistral-7B loaded\n", "\n", "4. Generating responses from both models...\n", "============================================================\n", "\n", "[1/10] Query: need assistance changing to the {{Account Category}} account...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[2/10] Query: using {{Account Type}} accouny...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[3/10] Query: I have forgotten my user profile pwd, where to reset it?...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[4/10] Query: I need help closing a platinum account...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[5/10] Query: could uhelp me to close my freemium account...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[6/10] Query: what do i need to do to earn several of ur item...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[7/10] Query: can ya help me to notify of an error with  signup...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[8/10] Query: what do I need to do to cancel purchase {{Order Number}}?...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[9/10] Query: remove freemium account...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "[10/10] Query: purchase {{Order Number}} current status...\n", "   Generating: Phi-2...\n", "   \u2713 Phi-2 done\n", "   Generating: Mistral-7B...\n", "   \u2713 Mistral-7B done\n", "\n", "5. Saving comparison results...\n", "   \u2713 Saved to /content/drive/MyDrive/NLP_Project/results//phi2_vs_mistral_comparison.csv\n", "\n", "================================================================================\n", "SIDE-BY-SIDE COMPARISON\n", "================================================================================\n", "\n", "================================================================================\n", "QUERY 1/10\n", "================================================================================\n", "\n", "Query: need assistance changing to the {{Account Category}} account\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I'm on it! I'm here to guide you through the process of upgrading to our prestigious {{Account Category}} account:\n", "\n", "1. **Sign In:** Start by logging into our platform using your credentials.\n", "2. **Profile Navigation:** Once you're signed in, navigate to your account settings or profile section.\n", "3. **Upgrade Option:** Look for an option that says \"Upgrade\" or \"Switch Account Type\".\n", "4. **Choose {{Account Category}}:** Select the \"{{Account Category}} Account\" option from the available choices.\n", "5. **Confirmation and Final Steps:** Follow the on-screen instructions to confirm your selection and complete the upgrade process.\n", "\n", "If you encounter any difficulties along the way or have any additional questions, please don't hesitate to reach out. We appreciate your trust in us and look forward to providing you with all the benefits that come with our {{Account Category}} account. Enjoy the enhanced features and privileges!\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I'll take care of it! I'm thrilled to provide you with the necessary guidance to switch to our prestigious {{Account Category}} account:\n", "\n", "1. **Access Your Account:** Begin by logging into our platform.\n", "2. **Account Settings:** Once inside, navigate to your '{{Settings}}' or '{{Profile}}' section.\n", "3. **Upgrade Option:** Look for an option labeled '{{Upgrade Account}}' or '{{Account Change}}'.\n", "4. **Select {{Account Category}}:** Choose the '{{Account Type}}' option from the available choices.\n", "5. **Finalize the Switch:** Follow the prompts on the screen to complete the transition.\n", "\n", "If you encounter any challenges during the process or have any additional questions, please don't hesitate to reach out. Your decision to upgrade to the {{Account Category}} account is truly commendable, and I'm here to ensure a seamless experience\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "I'll do my best! I'm thrilled to assist you in upgrading to our {{Account Category}} account and unlocking a world of exclusive benefits:\n", "\n", "1. Login: Start by logging in to your account.\n", "2. Account Settings: Once you're in, navigate to your '{{Settings}}' or '{{Profile}}' section.\n", "3. {{Account Category}} Upgrade: Look for an option called '{{{{Account Category}} Upgrade}}' or '{{Account Change}}'.\n", "4. Selection: Choose the '{{Account Type}}' option from the available choices.\n", "5. Confirmation: Follow the on-screen prompts to confirm and finalize the account upgrade.\n", "\n", "If you encounter any difficulties during the process or have any additional questions, please don't hesitate to reach out. Your upgrade is important to us, and I'll be here to ensure a smooth transition. Get ready to experience an enhanced level of service and a host of exciting features with your new {{Account Category}} account!\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 2/10\n", "================================================================================\n", "\n", "Query: using {{Account Type}} accouny\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Absolutely! I'm thrilled to assist you in utilizing our {{Account Type}} account and unlocking its incredible features:\n", "\n", "1. **Accessing Your Account:** Start by logging into our platform.\n", "2. **Profile Management:** Once you're inside, navigate to your '{{Settings}}' or '{{Profile}}' section.\n", "3. **Account Type Selection:** Look for an option that allows you to switch or downgrade your account type.\n", "4. **{{Account Type}} Account:** Choose the '{{Account Type}}' option from the available list.\n", "5. **Confirmation and Final Steps:** Follow the on-screen prompts to confirm your selection and complete the process.\n", "\n", "Should you encounter any difficulties or have further inquiries, please don't hesitate to reach out. Your decision to switch to the {{Account Type}} account is highly appreciated, and I'm here to ensure a seamless transition. Enjoy the exclusive benefits that come with your {{Account Type\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I'm on it! I'm thrilled to assist you in utilizing the remarkable features of our {{Account Type}} account:\n", "\n", "1. Access Your Account: Begin by logging into our platform.\n", "2. Navigate to Account Settings: Once logged in, locate the '{{Settings}}' or '{{Profile}}' section.\n", "3. Upgrade Option: Within this section, search for an option labeled '{{Upgrade Account}}' or '{{Account Change}}'.\n", "4. Select {{Account Type}} Account: Choose the '{{Account Type}}' option from the available choices.\n", "5. Finalize the Process: Follow the on-screen prompts to complete the upgrade.\n", "\n", "Should you encounter any challenges or have further questions, please don't hesitate to reach out. I'm here to ensure a seamless transition and help you make the most of our {{Account Type}} account's exceptional benefits. Enjoy the enhanced experience!\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "I absolutely appreciate your intention to make the most out of our Free account! Let's make sure you have all the necessary information to navigate and utilize it to its fullest potential:\n", "\n", "1. Discover Features: Take a moment to explore the various features and benefits available in our Free account. This will allow you to understand and leverage its capabilities effectively.\n", "\n", "2. Utilize Resources: Make use of our comprehensive resources, such as tutorials, guides, and support materials, which are specifically designed to assist you in getting the most out of your Free account.\n", "\n", "3. Engage with Community: Join our vibrant community of users who share their experiences, ideas, and best practices in utilizing the Free account. This community can provide valuable insights and assistance throughout your journey.\n", "\n", "4. Stay Updated: Keep an eye out for any updates or new features that we release for the Free account. This will ensure that you're always up-to-date with the latest enhancements and improvements.\n", "\n", "Remember, if you have any questions or encounter any issues while using the Free account, our dedicated support team is here to provide you with the assistance you need. Enjoy exploring all the possibilities that come with our Free account!\"\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 3/10\n", "================================================================================\n", "\n", "Query: I have forgotten my user profile pwd, where to reset it?\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I'm happy to help! I understand that you've forgotten your user profile password and need assistance with resetting it. Don't worry, we'll guide you through the process.\n", "\n", "To reset your user profile password, follow these steps:\n", "\n", "1. Go to our platform's login page at \"{{Login Page URL}}\".\n", "2. Look for the \"Forgot Password\" option and click on it.\n", "3. Enter the email address associated with your user profile when prompted.\n", "4. You should receive an email with instructions on how to reset your password. If you don't see it in your main inbox, please check your spam or junk folders as well.\n", "5. Follow the instructions in the email carefully to create a new password for your user profile.\n", "\n", "If you encounter any difficulties during the process or if you have any further questions, please feel free to reach out to me. I'm here to assist you every step of the way!\n", "\n", "Remember\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Oh no! I understand how frustrating it can be to forget your user profile password. Don't worry, I'm here to help you reset it and get you back into your account. Here's what you need to do:\n", "\n", "1. Head over to the \"{{Login Page URL}}\" of our platform.\n", "2. Look for the \"{{Forgot Password}}\" option and click on it.\n", "3. You'll be prompted to enter the email address associated with your user profile.\n", "4. Once you've entered your email, keep an eye on your inbox. You should receive an email with instructions on how to reset your password. If you don't see it in your main inbox, please check your spam or junk folders as well.\n", "5. Follow the instructions in the email to create a new, secure password for your user profile.\n", "\n", "If you encounter any issues or if the email doesn\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "I've taken note that you have forgotten your user profile password and you're looking for the right place to reset it. Don't worry, I'm here to guide you through the process. To reset your password, please follow these steps:\n", "\n", "1. Visit our platform's login page by going to \"{{Login Page URL}}\".\n", "2. Look for the option that says \"{{Forgot Password}}\" and click on it.\n", "3. You will be prompted to enter the email address associated with your user profile. Please make sure to enter the correct email address.\n", "4. After submitting your email address, we will send you an email with detailed instructions on how to reset your password. It may take a few minutes for the email to arrive, so please be patient.\n", "5. Once you receive the email, open it and carefully follow the instructions provided to create a new password for your user profile.\n", "\n", "Remember to choose a strong password that includes a combination of letters, numbers, and special characters to ensure the security of your account. If you encounter any difficulties or have any further questions, feel free to reach out to our customer support team for assistance. We're always here to help you regain access to your user profile.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 4/10\n", "================================================================================\n", "\n", "Query: I need help closing a platinum account\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We've received your message to us for assistance with closing your {{Account Category}} account. I understand that you would like to know the steps involved in the account closure process. To ensure we provide you with accurate information, could you please provide me with your account details such as your account number or email address associated with the {{Account Category}} account? This will help us locate your account and guide you through the necessary steps to close it successfully. Rest assured, we are here to support you throughout this process and address any concerns you may have. How may I assist you further? Let's work together to close your {{Account Category}} account smoothly. Your satisfaction is our top priority. Please provide me with the required information, and I will guide you through the account closure process. Thank you for choosing us for your {{Account Category}} account. We appreciate your trust and value your time with us. Let's get started on closing your account. \n", "\n", "Please provide your account number or email address\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We've received your message to us for assistance with closing your {{Account Category}} account. I understand that you're seeking guidance on the account closure process, and I'm here to provide the necessary support. To initiate the closure, please follow these steps: \n", "1. Log in to your account on our website.\n", "2. Navigate to the account settings or profile section.\n", "3. Look for the option to manage or close your account.\n", "4. Follow the prompts or instructions provided to complete the closure process.\n", "If you encounter any difficulties or have further questions along the way, please don't hesitate to reach out to our customer support team. They are available {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat on our website at {{Website URL}}. We want to ensure a smooth experience for you as you close your {{Account Category}} account. Thank you for your trust in our services. How\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "Thank you for your message to us about closing your {{Account Category}} account. We understand that account closures can be a sensitive and important process, and we want to ensure that everything goes smoothly for you. To proceed with closing your {{Account Category}} account, please provide us with the following details: your account number or any other identification information associated with your account. Once we have this information, our team will assist you with the necessary steps to close your account. We value your trust and want to ensure that your experience with us remains positive, even when closing an account.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 5/10\n", "================================================================================\n", "\n", "Query: could uhelp me to close my freemium account\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We're here for you about closing your freemium account. I understand that you may have reasons for wanting to close your account, and I'm here to assist you through the process. To proceed with closing your freemium account, I kindly request you to provide me with your account details such as your username or email address associated with the account. This will help me locate your account and guide you through the necessary steps to close it. Rest assured, I'm here to support you every step of the way. How can I assist you further? Is there anything else you would like to know? Let me know, and I'll be more than happy to help you. Remember, your satisfaction is our top priority. Let's work together to ensure a smooth account closure process for you. What do you need assistance with next?\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Thank you for trusting to us about closing your {{Account Category}} account. I understand that you would like assistance with this process, and I'm here to help. To close your account, please provide me with some additional information such as your account username or email address associated with the account. With this information, I will be able to guide you through the necessary steps to close your account successfully. Rest assured, I am committed to ensuring a smooth experience for you. Let me know how I can assist you further. Your satisfaction is our top priority. How can I make your experience even better today? Is there anything else I can help you with? I'm here to listen and provide the support you need. Together, we will ensure your account is closed in a timely and efficient manner. Let's work together to make this process as seamless as possible for you. How can I make your experience even better today? Is there anything else I can help you with? Please\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "We appreciate your inquiry about closing your {{Account Category}} account. We understand that you no longer wish to use this account and we're here to help you through the process. To get started with the account closure, you can follow the instructions provided on our website or contact our customer support team directly. They will guide you through the necessary steps to close your {{Account Category}} account completely. We appreciate your understanding and cooperation throughout this process.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 6/10\n", "================================================================================\n", "\n", "Query: what do i need to do to earn several of ur item\n", "Category: ORDER\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Thank you for expressing your interest in earning several of our items! We have a few options available to help you achieve that. One way to earn our items is by participating in our loyalty program. By signing up and making purchases, you can accumulate points that can be redeemed for discounts or even exclusive items. Another way is by referring our products to your friends and family. If they make a purchase from your referral link, you can earn a commission. Lastly, you can also consider joining our affiliate program, where you can promote our products and earn a percentage of the sales you generate. We hope these options provide you with the opportunity to earn several of our items! Let me know if you have any further questions or need assistance with any specific details.\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Thank you for your interest in earning several of our items! To earn multiple items, you can follow these steps:\n", "\n", "1. Browse our website/catalog to find the items you are interested in.\n", "2. Add the items to your shopping cart or wishlist.\n", "3. Review your selection and ensure you have all the necessary items.\n", "4. Proceed to checkout and complete the purchase process.\n", "5. If there are any special requirements or conditions for earning the items, our customer support team will be happy to assist you.\n", "\n", "Feel free to reach out to us if you have any further questions or need additional guidance. We're here to make sure your experience earning our items is seamless and enjoyable! Happy shopping!\n", "\n", "Please let me know if there's anything else I can assist you with. I'm here to ensure your shopping journey is a success! Feel free to ask any questions or share any concerns you may have. Together\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "Thank you for your interest in earning several of our items! To proceed with the acquisition, you can follow these steps:\n", "\n", "1. Browse through our collection: Take some time to explore our wide range of products. You can visit our website or browse our catalog to find the items that catch your eye.\n", "\n", "2. Add items to your cart: Once you've identified the items you want to earn, simply click on the \"Add to Cart\" button next to each product. This will add the items to your virtual shopping cart.\n", "\n", "3. Review your cart: After you've added all the items you desire, click on the cart icon to review the contents of your cart. Here, you'll be able to see a summary of your selected items, quantities, and total cost.\n", "\n", "4. Proceed to checkout: If everything looks good, click on the \"Proceed to Checkout\" button to initiate the checkout process. You'll be prompted to provide your shipping address, select a payment method, and review the final order details.\n", "\n", "5. Complete the transaction: Follow the instructions on the checkout page to complete your transaction. Once your order is confirmed, you'll receive a confirmation email with the details of your purchase.\n", "\n", "If you encounter any difficulties or have any questions along the way, feel free to reach out to our customer support team. We're here to assist you throughout the entire process. Happy earning!\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 7/10\n", "================================================================================\n", "\n", "Query: can ya help me to notify of an error with  signup\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We're here to help to us to notify us about the error you encountered during the signup process. We understand how frustrating it can be when you face difficulties while trying to create an account. Rest assured, we are here to assist you in resolving this issue. Could you please provide more details about the error you encountered? This information will help us investigate and find the best solution for you. We appreciate your patience and cooperation in this matter. Together, we will work towards a seamless signup experience for you. How can we make this process smoother for you? Let us know! \n", "\n", "Please remember that your feedback is crucial in helping us improve our services and provide a better experience for all our users. We value your trust and are committed to ensuring a smooth signup process for you. Thank you for bringing this to our attention. We are here to assist you every step of the way. Let's get to the bottom of this issue and find a solution together.\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We appreciate your inquiry to us regarding the error you encountered during the signup process. We understand that encountering errors can be frustrating, and we're here to assist you. To better understand the issue, could you please provide us with more details about the error you encountered? This will allow us to investigate and resolve the problem promptly. We value your feedback and want to ensure a seamless signup experience for you. Thank you for bringing this to our attention, and we'll do our best to assist you. Your satisfaction is our top priority. How can I further assist you? Is there anything else you need help with? We're here to support you throughout the signup process. Let us know how we can help. Together, we'll resolve this issue and get you signed up successfully. Your proactive approach is appreciated, and we're committed to providing you with the best possible solution. Please feel free to share any additional information or questions you may\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "We're here for you to us and seeking assistance in notifying us of an error with the signup process. We appreciate your proactive approach in resolving the issue. To report the signup error, you can reach out to our customer support team either by phone or through our website's live chat feature. Our dedicated representatives will guide you through the process and ensure that your concern is addressed promptly. Your feedback is essential in helping us improve our services, and we are committed to resolving any issues you may encounter.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 8/10\n", "================================================================================\n", "\n", "Query: what do I need to do to cancel purchase {{Order Number}}?\n", "Category: ORDER\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I'm with you, your need to cancel purchase {{Order Number}}. To cancel your purchase, please follow these steps:\n", "\n", "1. Log into your {{Online Company Portal Info}} using your credentials.\n", "2. Navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section.\n", "3. Locate the purchase with the order number {{Order Number}} and click on it.\n", "4. Look for the '{{Online Order Interaction}}' option associated with your purchase and select it.\n", "5. Follow any additional instructions or prompts to complete the cancellation process.\n", "\n", "If you encounter any difficulties or have further questions, our dedicated support team is available during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website at {{Website URL}}. We are here to assist you and ensure your satisfaction. Thank you for choosing us, and we appreciate your understanding.\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "I've understood that you're looking for guidance on canceling purchase {{Order Number}}. To initiate the cancellation process, please follow these steps:\n", "\n", "1. Log into your {{Online Company Portal Info}} using your credentials.\n", "2. Navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section.\n", "3. Locate the purchase with the order number {{Order Number}} and click on it to access the details.\n", "4. Look for the option labeled '{{Online Order Interaction}}' associated with your purchase and select it.\n", "5. If prompted, provide any necessary information or feedback to complete the cancellation.\n", "\n", "If you encounter any difficulties or have further questions, our dedicated team is available to assist you during {{Customer Support Hours}}. You can reach us at {{Customer Support Phone Number}} or through the Live Chat feature on our {{Website URL}}. We value\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "I'm on your side that you need assistance with canceling your purchase with the purchase number {{Order Number}}. To initiate the cancellation process, please follow these steps:\n", "\n", "1. Log into your {{Online Company Portal Info}} using your credentials.\n", "2. Navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section, where you can view your previous purchases.\n", "3. Search for the specific purchase with the purchase number {{Order Number}}.\n", "4. Once you locate the purchase, you should see an option to '{{Online Order Interaction}}' associated with it. Please click on this option.\n", "5. You might be asked to provide feedback or confirm the cancellation. Please complete these additional steps as necessary.\n", "\n", "If you encounter any difficulties during the process or have further questions, our dedicated support team is available to assist you. You can reach out to us during {{Customer Support Hours}} at {{Customer Support Phone Number}}, or you can utilize the Live Chat feature on our {{Website URL}}. Rest assured, your satisfaction is our top priority, and we'll ensure a smooth cancellation experience for you.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 9/10\n", "================================================================================\n", "\n", "Query: remove freemium account\n", "Category: ACCOUNT\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "We appreciate your inquiry to us regarding the removal of your {{Account Category}} account. We understand that you no longer require this account and would like to assist you with its deletion. To proceed with the account removal, please provide us with your account details, such as your username or email address associated with the account. Once we have this information, we will initiate the necessary steps to remove your {{Account Category}} account. We value your time and are committed to ensuring a smooth process for you. How can we further assist you in this matter? Is there anything else you would like to discuss? Let us know so that we can provide you with the best possible assistance. Thank you for choosing our services.\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Thank you for reaching out about removing your freemium account. I understand that you no longer wish to continue with this account type, and I'm here to assist you with the process. To proceed with the account removal, please provide me with some additional information such as your account username or email address associated with the freemium account. Once I have the necessary details, I will be able to guide you through the account removal process step by step. Rest assured, we will make sure to address your request promptly and ensure a smooth experience for you. How can I assist you further in this matter? Remember, your satisfaction is our top priority. Let's work together to resolve this matter efficiently. Is there anything else I can help you with? Feel free to share your concerns or questions. I'm here to support you throughout the process. Together, we will make sure to meet your needs and requirements. How can I assist you further in this matter? Remember, your satisfaction\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "Thank you for reaching out about removing your freemium account. We understand that you may have your reasons for wanting to cancel, and we're here to assist you. To ensure a smooth cancellation process, please provide us with the necessary details, such as your account information or any specific instructions you have regarding the removal. Our dedicated team is ready to guide you and address any concerns you may have along the way. Let's work together to make this experience as seamless as possible for you.\n", "\n", "================================================================================\n", "\n", "\n", "================================================================================\n", "QUERY 10/10\n", "================================================================================\n", "\n", "Query: purchase {{Order Number}} current status\n", "Category: ORDER\n", "\n", "--------------------------------------------------------------------------------\n", "PHI-2 RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Thank you for reaching out! I completely understand that you would like to know the current status of your purchase with the order number {{Order Number}}. To provide you with accurate information, could you please confirm if you have made the purchase on our website or through a different platform? Once I have this information, I will be able to assist you further in tracking the status of your purchase. Your satisfaction is our priority, and I'm here to ensure that you have a seamless experience. Let me know if you need any additional details or if there's anything else I can assist you with. How are you doing today? I'm excited to hear from you! \ud83d\ude0a\ud83e\udd17\n", "\n", "--------------------------------------------------------------------------------\n", "MISTRAL-7B RESPONSE:\n", "--------------------------------------------------------------------------------\n", "Great! I'm glad you made the purchase on our website. To track the current status of your purchase with the purchase number {{Order Number}}, you can visit the '{{Order Tracking}}' section on our website. There, you will find all the details and updates regarding your purchase. If you encounter any difficulties or need further assistance, please don't hesitate to let me know. I'm\n", "\n", "--------------------------------------------------------------------------------\n", "REFERENCE (Ground Truth):\n", "--------------------------------------------------------------------------------\n", "I see what you mean! I'm on the same page that you would like to know the current status of your purchase with the purchase number {{Order Number}}. To provide you with accurate information regarding your purchase status, I kindly request you to provide me with your email address or phone number associated with the purchase, so I can quickly retrieve the details and assist you further.\n", "\n", "================================================================================\n", "\n", "\n", "\u2713 COMPARISON COMPLETE\n"]}], "source": ["import pandas as pd\n", "import torch\n", "from peft import PeftModel\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "\n", "print(\"=\"*60)\n", "print(\"COMPARING PHI-2 vs MISTRAL-7B ON SAMPLE QUERIES\")\n", "print(\"=\"*60)\n", "\n", "# Load test dataset\n", "print(\"\\n1. Loading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "df_indet_test = df_test[df_test['label'] == 1].reset_index(drop=True)\n", "\n", "# Sample 10 diverse queries\n", "df_sample = df_indet_test.sample(n=10, random_state=42).reset_index(drop=True)\n", "print(f\"   \u2713 Sampled {len(df_sample)} test queries\")\n", "\n", "# Load Phi-2 model\n", "print(\"\\n2. Loading fine-tuned Phi-2...\")\n", "base_phi2 = AutoModelForCausalLM.from_pretrained(\n", "    \"microsoft/phi-2\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "phi2_model = PeftModel.from_pretrained(\n", "    base_phi2,\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", ")\n", "phi2_tokenizer = AutoTokenizer.from_pretrained(\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", ")\n", "print(\"   \u2713 Phi-2 loaded\")\n", "\n", "# Load Mistral-7B model\n", "print(\"\\n3. Loading fine-tuned Mistral-7B...\")\n", "base_mistral = AutoModelForCausalLM.from_pretrained(\n", "    \"mistralai/Mistral-7B-v0.1\",\n", "    device_map=\"auto\",\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "mistral_model = PeftModel.from_pretrained(\n", "    base_mistral,\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\"\n", ")\n", "mistral_tokenizer = AutoTokenizer.from_pretrained(\n", "    \"/content/drive/MyDrive/NLP_Project/checkpoints/mistral7b_lora/final_model\"\n", ")\n", "print(\"   \u2713 Mistral-7B loaded\")\n", "\n", "# Generate responses\n", "print(\"\\n4. Generating responses from both models...\")\n", "print(\"=\"*60)\n", "\n", "comparison_results = []\n", "\n", "for i, row in df_sample.iterrows():\n", "    query = row['instruction']\n", "    category = row['category']\n", "    reference = row['response']\n", "\n", "    print(f\"\\n[{i+1}/10] Query: {query[:60]}...\")\n", "\n", "    # Phi-2 generation\n", "    print(\"   Generating: Phi-2...\")\n", "    prompt = f\"Customer: {query}\\nAssistant:\"\n", "\n", "    inputs_phi2 = phi2_tokenizer(prompt, return_tensors=\"pt\").to(phi2_model.device)\n", "    with torch.no_grad():\n", "        outputs_phi2 = phi2_model.generate(\n", "            **inputs_phi2,\n", "            max_new_tokens=200,\n", "            do_sample=True,\n", "            temperature=0.7,\n", "            top_p=0.9,\n", "            pad_token_id=phi2_tokenizer.eos_token_id\n", "        )\n", "    phi2_response = phi2_tokenizer.decode(outputs_phi2[0], skip_special_tokens=True)\n", "    phi2_response = phi2_response.split(\"Assistant:\")[-1].strip()\n", "    print(\"   \u2713 Phi-2 done\")\n", "\n", "    # Mistral-7B generation\n", "    print(\"   Generating: Mistral-7B...\")\n", "    inputs_mistral = mistral_tokenizer(prompt, return_tensors=\"pt\").to(mistral_model.device)\n", "    with torch.no_grad():\n", "        outputs_mistral = mistral_model.generate(\n", "            **inputs_mistral,\n", "            max_new_tokens=200,\n", "            do_sample=True,\n", "            temperature=0.7,\n", "            top_p=0.9,\n", "            pad_token_id=mistral_tokenizer.eos_token_id\n", "        )\n", "    mistral_response = mistral_tokenizer.decode(outputs_mistral[0], skip_special_tokens=True)\n", "    mistral_response = mistral_response.split(\"Assistant:\")[-1].strip()\n", "    print(\"   \u2713 Mistral-7B done\")\n", "\n", "    # Store results\n", "    comparison_results.append({\n", "        'query': query,\n", "        'category': category,\n", "        'phi2_response': phi2_response,\n", "        'mistral7b_response': mistral_response,\n", "        'reference_response': reference\n", "    })\n", "\n", "# Save results\n", "print(\"\\n5. Saving comparison results...\")\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "results_df = pd.DataFrame(comparison_results)\n", "results_df.to_csv(f'{output_path}/phi2_vs_mistral_comparison.csv', index=False)\n", "print(f\"   \u2713 Saved to {output_path}/phi2_vs_mistral_comparison.csv\")\n", "\n", "# Display side-by-side comparison\n", "print(\"\\n\" + \"=\"*80)\n", "print(\"SIDE-BY-SIDE COMPARISON\")\n", "print(\"=\"*80)\n", "\n", "for i, result in enumerate(comparison_results, 1):\n", "    print(f\"\\n{'='*80}\")\n", "    print(f\"QUERY {i}/10\")\n", "    print(f\"{'='*80}\")\n", "\n", "    print(f\"\\nQuery: {result['query']}\")\n", "    print(f\"Category: {result['category']}\")\n", "\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"PHI-2 RESPONSE:\")\n", "    print(f\"{'-'*80}\")\n", "    print(result['phi2_response'])\n", "\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"MISTRAL-7B RESPONSE:\")\n", "    print(f\"{'-'*80}\")\n", "    print(result['mistral7b_response'])\n", "\n", "    print(f\"\\n{'-'*80}\")\n", "    print(\"REFERENCE (Ground Truth):\")\n", "    print(f\"{'-'*80}\")\n", "    print(result['reference_response'])\n", "\n", "    print(f\"\\n{'='*80}\\n\")\n", "\n", "print(\"\\n\u2713 COMPARISON COMPLETE\")"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": ["OiFKn3R2mrtF", "VG4GckODms6r", "-PA4Gipasbgs", "zfBdoMghpf5j", "7d9sN0CTzcHf"], "gpuType": "L4", "machine_shape": "hm", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}}}}, "nbformat": 4, "nbformat_minor": 0}