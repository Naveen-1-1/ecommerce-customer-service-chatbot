{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": [], "collapsed_sections": ["OiFKn3R2mrtF", "VG4GckODms6r", "_dVDrC1a3elY", "9__2nJW4AmvM", "MkfOhuj2ArSW"], "machine_shape": "hm", "gpuType": "T4"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"state": {}}}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "source": ["# Phase 0: Environment Setup & Verification"], "metadata": {"id": "Q4qrTqYjmUkW"}}, {"cell_type": "markdown", "source": ["## 0.2: Mount Google Drive"], "metadata": {"id": "OiFKn3R2mrtF"}}, {"cell_type": "code", "source": ["from google.colab import drive\n", "drive.mount('/content/drive')"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "7UIp6ZhWvlvD", "outputId": "f06df346-8976-4d86-9323-08d3018042d6"}, "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Mounted at /content/drive\n"]}]}, {"cell_type": "code", "source": ["from google.colab import drive\n", "drive.mount('/content/drive')\n", "\n", "# Create project directory in Drive\n", "import os\n", "project_path = '/content/drive/MyDrive/NLP_Project'\n", "os.makedirs(project_path, exist_ok=True)\n", "os.makedirs(f'{project_path}/data', exist_ok=True)\n", "os.makedirs(f'{project_path}/models', exist_ok=True)\n", "os.makedirs(f'{project_path}/results', exist_ok=True)\n", "os.makedirs(f'{project_path}/checkpoints', exist_ok=True)\n", "\n", "print(f\"\u2713 Project directory created at: {project_path}\")\n", "print(f\"\\nDirectory structure:\")\n", "!ls -la /content/drive/MyDrive/NLP_Project/"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Wkgqw1THg0DS", "outputId": "ac3e92ed-0e47-400f-8a84-0ec89dbdafd2"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n", "\u2713 Project directory created at: /content/drive/MyDrive/NLP_Project\n", "\n", "Directory structure:\n", "total 16\n", "drwx------ 2 root root 4096 Dec 10 00:42 checkpoints\n", "drwx------ 2 root root 4096 Nov 19 01:28 data\n", "drwx------ 2 root root 4096 Dec 10 00:13 models\n", "drwx------ 2 root root 4096 Nov 19 01:28 results\n"]}]}, {"cell_type": "markdown", "source": ["## 0.3: Install Required Libraries"], "metadata": {"id": "VG4GckODms6r"}}, {"cell_type": "code", "source": ["print(\"Installing required packages...\")\n", "!pip install -q peft accelerate bitsandbytes\n", "!pip install -q sentence-transformers faiss-cpu\n", "!pip install -q rouge-score bert-score\n", "!pip install -q datasets\n", "!pip install -U bitsandbytes accelerate\n", "\n", "print(\"\\n\" + \"=\"*50)\n", "print(\"VERIFYING INSTALLATIONS\")\n", "print(\"=\"*50)\n", "\n", "# Verify installations\n", "import pandas as pd\n", "import numpy as np\n", "import torch\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "import transformers\n", "import peft\n", "import sentence_transformers\n", "from sentence_transformers import SentenceTransformer\n", "from datasets import load_dataset\n", "\n", "print(\"\u2713 All core libraries imported successfully!\")\n", "print(f\"\\nLibrary versions:\")\n", "print(f\"  PyTorch: {torch.__version__}\")\n", "print(f\"  Transformers: {transformers.__version__}\")\n", "print(f\"  PEFT: {peft.__version__}\")\n", "print(f\"  Sentence Transformers: {sentence_transformers.__version__}\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "lsxHbzmThMMb", "outputId": "4b70b36f-d309-4c38-adea-5749a3229fbc"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Installing required packages...\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n", "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n", "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n", "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n", "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n", "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n", "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n", "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n", "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n", "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n", "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n", "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n", "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n", "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n", "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n", "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n", "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n", "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n", "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n", "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n", "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n", "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n", "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n", "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n", "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n", "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n", "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n", "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n", "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n", "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n", "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n", "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n", "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n", "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n", "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n", "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n", "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n", "\n", "==================================================\n", "VERIFYING INSTALLATIONS\n", "==================================================\n", "\u2713 All core libraries imported successfully!\n", "\n", "Library versions:\n", "  PyTorch: 2.9.0+cu126\n", "  Transformers: 4.57.3\n", "  PEFT: 0.18.0\n", "  Sentence Transformers: 5.2.0\n"]}]}, {"cell_type": "markdown", "source": ["# Phase 4: End-to-End Pipeline Integration"], "metadata": {"id": "Qj4gomad3DMR"}}, {"cell_type": "markdown", "source": ["## 4.1: End-to-End Pipeline System"], "metadata": {"id": "LWMoVJ-bnqEj"}}, {"cell_type": "code", "source": ["import torch\n", "import pandas as pd\n", "import time\n", "import pickle\n", "import faiss\n", "from sentence_transformers import SentenceTransformer\n", "from peft import PeftModel\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "\n", "print(\"=\"*60)\n", "print(\"4.1: End-to-End Pipeline System\")\n", "print(\"=\"*60)\n", "\n", "# Build complete pipeline class\n", "class HybridChatbot:\n", "    def __init__(self):\n", "        print(\"\\n1. Loading all components...\")\n", "\n", "        # Classifier\n", "        print(\"   - Loading classifier...\")\n", "        with open('/content/drive/MyDrive/NLP_Project/models/classifier/logistic_regression.pkl', 'rb') as f:\n", "            self.classifier = pickle.load(f)\n", "        with open('/content/drive/MyDrive/NLP_Project/models/classifier/tfidf_vectorizer.pkl', 'rb') as f:\n", "            self.tfidf = pickle.load(f)\n", "\n", "        # Retrieval system\n", "        print(\"   - Loading retrieval system...\")\n", "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n", "        self.retrieval_index = faiss.read_index('/content/drive/MyDrive/NLP_Project/models/retrieval/faiss_index.bin')\n", "        self.retrieval_data = pd.read_csv('/content/drive/MyDrive/NLP_Project/models/retrieval/deterministic_qa_pairs.csv')\n", "\n", "        # LLM\n", "        print(\"   - Loading fine-tuned LLM...\")\n", "        base_model = AutoModelForCausalLM.from_pretrained(\n", "            \"microsoft/phi-2\",\n", "            device_map=\"auto\",\n", "            trust_remote_code=True,\n", "            torch_dtype=torch.float16\n", "        )\n", "        self.llm_model = PeftModel.from_pretrained(\n", "            base_model,\n", "            \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", "        )\n", "        self.llm_tokenizer = AutoTokenizer.from_pretrained(\n", "            \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", "        )\n", "\n", "        print(\"   \u2713 All components loaded!\\n\")\n", "\n", "    def classify_query(self, query):\n", "        \"\"\"Returns 0 for deterministic, 1 for indeterministic\"\"\"\n", "        query_tfidf = self.tfidf.transform([query])\n", "        return self.classifier.predict(query_tfidf)[0]\n", "\n", "    def retrieve_response(self, query, k=1):\n", "        \"\"\"Semantic search for deterministic queries\"\"\"\n", "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n", "        distances, indices = self.retrieval_index.search(query_embedding.astype('float32'), k)\n", "        return self.retrieval_data.iloc[indices[0][0]]['response'], distances[0][0]\n", "\n", "    def generate_response(self, query, max_tokens=150):\n", "        \"\"\"LLM generation for indeterministic queries\"\"\"\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\").to(self.llm_model.device)\n", "\n", "        with torch.no_grad():\n", "            outputs = self.llm_model.generate(\n", "                **inputs,\n", "                max_new_tokens=max_tokens,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=self.llm_tokenizer.eos_token_id\n", "            )\n", "\n", "        response = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        return response.split(\"Assistant:\")[-1].strip()\n", "\n", "    def respond(self, query):\n", "        \"\"\"Main pipeline: classify \u2192 route \u2192 respond\"\"\"\n", "        start_time = time.time()\n", "\n", "        # Step 1: Classify\n", "        prediction = self.classify_query(query)\n", "        route = \"RETRIEVAL\" if prediction == 0 else \"LLM_GENERATION\"\n", "\n", "        # Step 2: Get response\n", "        if prediction == 0:  # Deterministic\n", "            response, distance = self.retrieve_response(query)\n", "            confidence = 1.0 / (1.0 + distance)  # Convert distance to confidence\n", "        else:  # Indeterministic\n", "            response = self.generate_response(query)\n", "            confidence = None\n", "\n", "        latency = (time.time() - start_time) * 1000\n", "\n", "        return {\n", "            'query': query,\n", "            'route': route,\n", "            'response': response,\n", "            'latency_ms': latency,\n", "            'confidence': confidence\n", "        }\n", "\n", "# Initialize chatbot\n", "chatbot = HybridChatbot()\n", "\n", "# Test it\n", "print(chatbot.respond(\"I'm unhappy with the order I received\"))"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "referenced_widgets": ["4b7806b781ef45408a7c561bada03c31", "e33aa1572f0a48dcb14a1fbafe3e2591", "527e3c5ed15842d8a73b5a4f306f2951", "f4cd42282fa64df7a6257aff86282e03", "d9de8fba67374fb6ac6de8aaf7b34b35", "8da8cf3ee6234259a8b63eea2f3f085a", "654f208bee5d446e9798690f976771c9", "88481ff56a5e417aa6fcf287767d74ea", "50e4c228ef1e4fdaa98c4a74867cc838", "78258b352aeb4a2fb35e2e058b47bbfc", "53d69c10b0654dcb9d367090838851d3"], "height": 260}, "id": "xxtUwaBi3Lee", "outputId": "c4070795-faa1-4816-fb63-c5d42347714d"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "4.1: End-to-End Pipeline System\n", "============================================================\n", "\n", "1. Loading all components...\n", "   - Loading classifier...\n", "   - Loading retrieval system...\n", "   - Loading fine-tuned LLM...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4b7806b781ef45408a7c561bada03c31"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 All components loaded!\n", "\n", "{'query': \"I'm unhappy with the order I received\", 'route': 'LLM_GENERATION', 'response': \"I'm sorry to hear that you're unhappy with the order you received. I understand how frustrating this can be, and I apologize for any inconvenience caused. To better assist you, could you please provide me with some specific details about the issues you encountered with the order? This will help me investigate the matter thoroughly and find a suitable solution for you. Your feedback is highly valuable to us, and we appreciate your patience as we work towards resolving this matter. How can I assist you further? Is there anything else I can do to help? Remember, your satisfaction is our top priority, and we're committed to ensuring your order meets your expectations. Thank you for bringing this to our attention, and we'll do our best to make things right.\", 'latency_ms': 13497.104406356812, 'confidence': None}\n"]}]}, {"cell_type": "markdown", "source": ["## 4.2: End-to-End Pipeline Testing\n", "\n"], "metadata": {"id": "aYmqY27LnwKP"}}, {"cell_type": "code", "source": ["print(\"\\n\" + \"=\"*60)\n", "print(\"INTEGRATION TESTING\")\n", "print(\"=\"*60)\n", "\n", "# Load test dataset\n", "print(\"\\nLoading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "print(f\"Loaded {len(df_test)} test examples\")\n", "\n", "# Sample diverse queries from test set\n", "print(\"\\nSampling diverse queries for integration testing...\")\n", "\n", "# Sample strategy: Get examples from each category + label combination\n", "sample_queries = []\n", "\n", "# Get unique categories\n", "categories = df_test['category'].unique()\n", "\n", "# For each category, try to get both deterministic and indeterministic examples\n", "for category in sorted(categories):\n", "    cat_df = df_test[df_test['category'] == category]\n", "\n", "    # Try to get 1 deterministic (label=0)\n", "    det_samples = cat_df[cat_df['label'] == 0]\n", "    if len(det_samples) > 0:\n", "        sample = det_samples.sample(1, random_state=42).iloc[0]\n", "        sample_queries.append(sample.to_dict())\n", "\n", "    # Try to get 1 indeterministic (label=1)\n", "    indet_samples = cat_df[cat_df['label'] == 1]\n", "    if len(indet_samples) > 0:\n", "        sample = indet_samples.sample(1, random_state=42).iloc[0]\n", "        sample_queries.append(sample.to_dict())\n", "\n", "# Convert to DataFrame\n", "df_test_queries = pd.DataFrame(sample_queries)\n", "\n", "print(f\"Selected {len(df_test_queries)} test queries spanning {df_test_queries['category'].nunique()} categories\")\n", "print(f\"  Deterministic (label=0): {len(df_test_queries[df_test_queries['label'] == 0])}\")\n", "print(f\"  Indeterministic (label=1): {len(df_test_queries[df_test_queries['label'] == 1])}\")\n", "\n", "# Track results\n", "results = []\n", "routing_errors = 0\n", "crashes = 0\n", "retrieval_latencies = []\n", "llm_latencies = []\n", "\n", "print(\"\\n\" + \"-\"*60)\n", "print(\"INDIVIDUAL TEST RESULTS\")\n", "print(\"-\"*60)\n", "\n", "for i, row in df_test_queries.iterrows():\n", "    query = row['instruction']\n", "    true_label = row['label']\n", "    expected_route = \"RETRIEVAL\" if true_label == 0 else \"LLM_GENERATION\"\n", "    category = row['category']\n", "\n", "    print(f\"\\nTest {i+1}/{len(df_test_queries)}:\")\n", "    print(f\"  Query: {query}\")\n", "    print(f\"  True Label: {true_label} ({expected_route})\")\n", "    print(f\"  Category: {category}\")\n", "\n", "    try:\n", "        # Run query through pipeline\n", "        result = chatbot.respond(query)\n", "\n", "        # Check routing correctness\n", "        actual_route = result['route']\n", "        routing_correct = (actual_route == expected_route)\n", "\n", "        if not routing_correct:\n", "            routing_errors += 1\n", "            print(f\"  \u26a0\ufe0f  ROUTING ERROR: Expected {expected_route}, got {actual_route}\")\n", "        else:\n", "            print(f\"  \u2713 Correct routing: {actual_route}\")\n", "\n", "        # Track latency by route\n", "        if actual_route == \"RETRIEVAL\":\n", "            retrieval_latencies.append(result['latency_ms'])\n", "        else:\n", "            llm_latencies.append(result['latency_ms'])\n", "\n", "        # Display results\n", "        print(f\"  Latency: {result['latency_ms']:.0f} ms\")\n", "        if result['confidence'] is not None:\n", "            print(f\"  Confidence: {result['confidence']:.3f}\")\n", "        print(f\"  Response preview: {result['response'][:100]}...\")\n", "\n", "        # Store result\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'true_label': true_label,\n", "            'expected_route': expected_route,\n", "            'actual_route': actual_route,\n", "            'routing_correct': routing_correct,\n", "            'latency_ms': result['latency_ms'],\n", "            'confidence': result['confidence'],\n", "            'response': result['response'],\n", "            'crashed': False\n", "        })\n", "\n", "    except Exception as e:\n", "        crashes += 1\n", "        print(f\"  \u274c CRASH: {str(e)}\")\n", "        import traceback\n", "        traceback.print_exc()\n", "\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'true_label': true_label,\n", "            'expected_route': expected_route,\n", "            'actual_route': None,\n", "            'routing_correct': False,\n", "            'latency_ms': None,\n", "            'confidence': None,\n", "            'response': None,\n", "            'crashed': True,\n", "            'error': str(e)\n", "        })\n", "\n", "# Summary statistics\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"INTEGRATION TEST SUMMARY\")\n", "print(\"=\"*60)\n", "\n", "total_tests = len(df_test_queries)\n", "successful_tests = total_tests - crashes\n", "routing_accuracy = ((total_tests - routing_errors - crashes) / total_tests) * 100\n", "\n", "print(f\"\\nTest Coverage:\")\n", "print(f\"  Total queries tested: {total_tests}\")\n", "print(f\"  Successful completions: {successful_tests} ({successful_tests/total_tests*100:.1f}%)\")\n", "print(f\"  Crashes: {crashes} ({crashes/total_tests*100:.1f}%)\")\n", "\n", "print(f\"\\nRouting Accuracy:\")\n", "print(f\"  Correct routing: {total_tests - routing_errors - crashes}/{total_tests} ({routing_accuracy:.1f}%)\")\n", "print(f\"  Routing errors: {routing_errors}\")\n", "\n", "print(f\"\\nLatency Analysis:\")\n", "if retrieval_latencies:\n", "    print(f\"  Retrieval path:\")\n", "    print(f\"    Count: {len(retrieval_latencies)}\")\n", "    print(f\"    Average: {sum(retrieval_latencies)/len(retrieval_latencies):.0f} ms\")\n", "    print(f\"    Min: {min(retrieval_latencies):.0f} ms\")\n", "    print(f\"    Max: {max(retrieval_latencies):.0f} ms\")\n", "\n", "if llm_latencies:\n", "    print(f\"  LLM generation path:\")\n", "    print(f\"    Count: {len(llm_latencies)}\")\n", "    print(f\"    Average: {sum(llm_latencies)/len(llm_latencies):.0f} ms\")\n", "    print(f\"    Min: {min(llm_latencies):.0f} ms\")\n", "    print(f\"    Max: {max(llm_latencies):.0f} ms\")\n", "\n", "if retrieval_latencies and llm_latencies:\n", "    avg_overall = (sum(retrieval_latencies) + sum(llm_latencies)) / (len(retrieval_latencies) + len(llm_latencies))\n", "    print(f\"  Overall average: {avg_overall:.0f} ms\")\n", "\n", "# Category coverage\n", "print(f\"\\nCategory Coverage:\")\n", "categories_tested = set([r['category'] for r in results])\n", "print(f\"  Unique categories tested: {len(categories_tested)}\")\n", "print(f\"  Categories: {', '.join(sorted(categories_tested))}\")\n", "\n", "# Routing breakdown\n", "retrieval_count = sum(1 for r in results if r['actual_route'] == 'RETRIEVAL' and not r['crashed'])\n", "llm_count = sum(1 for r in results if r['actual_route'] == 'LLM_GENERATION' and not r['crashed'])\n", "print(f\"\\nRouting Distribution:\")\n", "print(f\"  Routed to Retrieval: {retrieval_count} ({retrieval_count/successful_tests*100:.1f}%)\")\n", "print(f\"  Routed to LLM: {llm_count} ({llm_count/successful_tests*100:.1f}%)\")\n", "\n", "# Pass/Fail determination\n", "print(\"\\n\" + \"=\"*60)\n", "if crashes == 0 and routing_accuracy >= 90:\n", "    print(\"\u2705 INTEGRATION TEST PASSED\")\n", "    print(f\"   - No crashes detected\")\n", "    print(f\"   - Routing accuracy: {routing_accuracy:.1f}%\")\n", "    print(f\"   - All query types processed successfully\")\n", "elif crashes == 0:\n", "    print(\"\u26a0\ufe0f  INTEGRATION TEST PARTIAL PASS\")\n", "    print(f\"   - No crashes detected\")\n", "    print(f\"   - Routing accuracy below 90%: {routing_accuracy:.1f}%\")\n", "else:\n", "    print(\"\u274c INTEGRATION TEST FAILED\")\n", "    print(f\"   - {crashes} crashes detected\")\n", "    print(f\"   - Routing accuracy: {routing_accuracy:.1f}%\")\n", "print(\"=\"*60)\n", "\n", "# Save results\n", "results_df = pd.DataFrame(results)\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "import os\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "results_df.to_csv(f'{output_path}/integration_test_results.csv', index=False)\n", "print(f\"\\n\u2713 Results saved to {output_path}/integration_test_results.csv\")\n", "\n", "# Display sample results\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"SAMPLE RESPONSES\")\n", "print(\"=\"*60)\n", "\n", "# Show 2 retrieval examples\n", "print(\"\\n--- Retrieval Path Examples ---\")\n", "retrieval_results = [r for r in results if r['actual_route'] == 'RETRIEVAL' and not r['crashed']]\n", "for r in retrieval_results[:2]:\n", "    print(f\"\\nQuery: {r['query']}\")\n", "    print(f\"Category: {r['category']} | Route: {r['actual_route']} | Latency: {r['latency_ms']:.0f} ms\")\n", "    if r['confidence']:\n", "        print(f\"Confidence: {r['confidence']:.3f}\")\n", "    print(f\"Response: {r['response'][:200]}...\")\n", "\n", "# Show 2 LLM examples\n", "print(\"\\n--- LLM Generation Path Examples ---\")\n", "llm_results = [r for r in results if r['actual_route'] == 'LLM_GENERATION' and not r['crashed']]\n", "for r in llm_results[:2]:\n", "    print(f\"\\nQuery: {r['query']}\")\n", "    print(f\"Category: {r['category']} | Route: {r['actual_route']} | Latency: {r['latency_ms']:.0f} ms\")\n", "    print(f\"Response: {r['response'][:200]}...\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 INTEGRATION TESTING COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-NHqKFl7-4oJ", "outputId": "3d332f0f-d1d4-438b-cc50-ad25294b3e05"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "INTEGRATION TESTING\n", "============================================================\n", "\n", "Loading test dataset...\n", "Loaded 3978 test examples\n", "\n", "Sampling diverse queries for integration testing...\n", "Selected 8 test queries spanning 8 categories\n", "  Deterministic (label=0): 5\n", "  Indeterministic (label=1): 3\n", "\n", "------------------------------------------------------------\n", "INDIVIDUAL TEST RESULTS\n", "------------------------------------------------------------\n", "\n", "Test 1/8:\n", "  Query: what do i need to do to recover my user account pin code\n", "  True Label: 1 (LLM_GENERATION)\n", "  Category: ACCOUNT\n", "  \u2713 Correct routing: LLM_GENERATION\n", "  Latency: 9615 ms\n", "  Response preview: I'm happy to help! I understand that you need assistance in recovering your user account pin code. D...\n", "\n", "Test 2/8:\n", "  Query: where could I see the early termination penalties?\n", "  True Label: 0 (RETRIEVAL)\n", "  Category: CANCEL\n", "  \u2713 Correct routing: RETRIEVAL\n", "  Latency: 11 ms\n", "  Confidence: 0.944\n", "  Response preview: Assuredly! If you want to find information about the early termination penalty, the best place to lo...\n", "\n", "Test 3/8:\n", "  Query: customer assistance mail address\n", "  True Label: 0 (RETRIEVAL)\n", "  Category: CONTACT\n", "  \u2713 Correct routing: RETRIEVAL\n", "  Latency: 8 ms\n", "  Confidence: 0.812\n", "  Response preview: Thank you for reaching out! I understand your need to know our customer assistance email. To ensure ...\n", "\n", "Test 4/8:\n", "  Query: can ya help me lodge a claim against ur organization\n", "  True Label: 1 (LLM_GENERATION)\n", "  Category: FEEDBACK\n", "  \u2713 Correct routing: LLM_GENERATION\n", "  Latency: 9330 ms\n", "  Response preview: I'm sorry to hear that you're facing an issue and need to lodge a claim against our organization. Yo...\n", "\n", "Test 5/8:\n", "  Query: I don't know what I have to do to download my bill #37777\n", "  True Label: 0 (RETRIEVAL)\n", "  Category: INVOICE\n", "  \u2713 Correct routing: RETRIEVAL\n", "  Latency: 10 ms\n", "  Confidence: 0.987\n", "  Response preview: I see what you mean your confusion about how to download your bill with the number #{{Invoice Number...\n", "\n", "Test 6/8:\n", "  Query: I need assistance trackng purchase {{Order Number}}\n", "  True Label: 1 (LLM_GENERATION)\n", "  Category: ORDER\n", "  \u2713 Correct routing: LLM_GENERATION\n", "  Latency: 6885 ms\n", "  Response preview: I'm on it! I'm here to assist you in tracking the purchase with the purchase number {{Order Number}}...\n", "\n", "Test 7/8:\n", "  Query: the delivery addrses isnt valid can i get some help\n", "  True Label: 0 (RETRIEVAL)\n", "  Category: SHIPPING\n", "  \u2713 Correct routing: RETRIEVAL\n", "  Latency: 42 ms\n", "  Confidence: 0.594\n", "  Response preview: No problem, I'm here to help! To resolve the issue with updating your address, please follow these s...\n", "\n", "Test 8/8:\n", "  Query: I need to receive the newsletter, how do I do it?\n", "  True Label: 0 (RETRIEVAL)\n", "  Category: SUBSCRIPTION\n", "  \u2713 Correct routing: RETRIEVAL\n", "  Latency: 9 ms\n", "  Confidence: 0.972\n", "  Response preview: It's wonderful to hear that you're eager to receive our newsletter! We appreciate your interest in s...\n", "\n", "============================================================\n", "INTEGRATION TEST SUMMARY\n", "============================================================\n", "\n", "Test Coverage:\n", "  Total queries tested: 8\n", "  Successful completions: 8 (100.0%)\n", "  Crashes: 0 (0.0%)\n", "\n", "Routing Accuracy:\n", "  Correct routing: 8/8 (100.0%)\n", "  Routing errors: 0\n", "\n", "Latency Analysis:\n", "  Retrieval path:\n", "    Count: 5\n", "    Average: 16 ms\n", "    Min: 8 ms\n", "    Max: 42 ms\n", "  LLM generation path:\n", "    Count: 3\n", "    Average: 8610 ms\n", "    Min: 6885 ms\n", "    Max: 9615 ms\n", "  Overall average: 3239 ms\n", "\n", "Category Coverage:\n", "  Unique categories tested: 8\n", "  Categories: ACCOUNT, CANCEL, CONTACT, FEEDBACK, INVOICE, ORDER, SHIPPING, SUBSCRIPTION\n", "\n", "Routing Distribution:\n", "  Routed to Retrieval: 5 (62.5%)\n", "  Routed to LLM: 3 (37.5%)\n", "\n", "============================================================\n", "\u2705 INTEGRATION TEST PASSED\n", "   - No crashes detected\n", "   - Routing accuracy: 100.0%\n", "   - All query types processed successfully\n", "============================================================\n", "\n", "\u2713 Results saved to /content/drive/MyDrive/NLP_Project/results//integration_test_results.csv\n", "\n", "============================================================\n", "SAMPLE RESPONSES\n", "============================================================\n", "\n", "--- Retrieval Path Examples ---\n", "\n", "Query: where could I see the early termination penalties?\n", "Category: CANCEL | Route: RETRIEVAL | Latency: 11 ms\n", "Confidence: 0.944\n", "Response: Assuredly! If you want to find information about the early termination penalty, the best place to look is your contract agreement. It should clearly outline the details and fees associated with termin...\n", "\n", "Query: customer assistance mail address\n", "Category: CONTACT | Route: RETRIEVAL | Latency: 8 ms\n", "Confidence: 0.812\n", "Response: Thank you for reaching out! I understand your need to know our customer assistance email. To ensure the privacy and security of our customers, we encourage you to reach out to our customer service tea...\n", "\n", "--- LLM Generation Path Examples ---\n", "\n", "Query: what do i need to do to recover my user account pin code\n", "Category: ACCOUNT | Route: LLM_GENERATION | Latency: 9615 ms\n", "Response: I'm happy to help! I understand that you need assistance in recovering your user account pin code. Don't worry, I'm here to guide you through the process.\n", "\n", "To recover your pin code, please follow thes...\n", "\n", "Query: can ya help me lodge a claim against ur organization\n", "Category: FEEDBACK | Route: LLM_GENERATION | Latency: 9330 ms\n", "Response: I'm sorry to hear that you're facing an issue and need to lodge a claim against our organization. Your feedback is important to us, and we take all complaints seriously. To assist you with the claim p...\n", "\n", "============================================================\n", "\u2713 INTEGRATION TESTING COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "markdown", "source": ["# Phase 5: Metrics Evaluation"], "metadata": {"id": "XPy9YA4avv85"}}, {"cell_type": "markdown", "source": ["## 5.1: Retrieval System"], "metadata": {"id": "oUx4q-vmv1ql"}}, {"cell_type": "code", "source": ["import os\n", "import numpy as np\n", "import pandas as pd\n", "from sentence_transformers import SentenceTransformer\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"RETRIEVAL EVALUATION METRICS\")\n", "print(\"=\"*60)\n", "\n", "# 1. Load test dataset\n", "print(\"\\nLoading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# 2. Filter for deterministic test queries only\n", "df_det_test = df_test[df_test['label'] == 0].reset_index(drop=True)\n", "print(f\"Loaded {len(df_det_test)} deterministic test queries\")\n", "\n", "# 3. Load sentence transformer model (same as used for training index)\n", "print(\"\\nLoading sentence transformer model...\")\n", "model = SentenceTransformer('all-MiniLM-L6-v2')\n", "\n", "# 4. Encode test queries\n", "print(\"\\nEncoding test queries...\")\n", "test_embeddings = model.encode(\n", "    df_det_test['instruction'].tolist(),\n", "    show_progress_bar=True,\n", "    convert_to_numpy=True\n", ")\n", "\n", "# 5. Load FAISS index and training data\n", "import faiss\n", "retrieval_path = '/content/drive/MyDrive/NLP_Project/models/retrieval/'\n", "\n", "print(\"\\nLoading FAISS index...\")\n", "index = faiss.read_index(f'{retrieval_path}/faiss_index.bin')\n", "\n", "print(\"Loading training retrieval data...\")\n", "df_det_train = pd.read_csv(f'{retrieval_path}/deterministic_qa_pairs.csv')\n", "\n", "print(f\"   \u2713 Index loaded with {index.ntotal:,} vectors\")\n", "print(f\"   \u2713 Training data: {len(df_det_train):,} examples\")\n", "\n", "# 6. Evaluate at multiple k values\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"RETRIEVAL ACCURACY AT MULTIPLE K VALUES\")\n", "print(\"=\"*60)\n", "\n", "k_values = [1, 3, 5, 10]\n", "results = {}\n", "\n", "for k in k_values:\n", "    print(f\"\\nEvaluating Top-{k} retrieval...\")\n", "\n", "    # Search for top-k matches\n", "    distances, indices = index.search(test_embeddings.astype('float32'), k)\n", "\n", "    # Calculate metrics\n", "    category_matches = 0\n", "    exact_matches = 0\n", "\n", "    for i, test_row in df_det_test.iterrows():\n", "        test_category = test_row['category']\n", "        test_query = test_row['instruction'].lower().strip()\n", "\n", "        # Get top-k retrieved examples\n", "        retrieved_indices = indices[i][:k]\n", "        retrieved_categories = df_det_train.iloc[retrieved_indices]['category'].values\n", "        retrieved_queries = df_det_train.iloc[retrieved_indices]['instruction'].values\n", "\n", "        # Check category match (if correct category in top-k)\n", "        if test_category in retrieved_categories:\n", "            category_matches += 1\n", "\n", "        # Check exact match (if exact query text in top-k)\n", "        for retrieved_query in retrieved_queries:\n", "            if test_query == retrieved_query.lower().strip():\n", "                exact_matches += 1\n", "                break\n", "\n", "    # Calculate percentages\n", "    category_accuracy = (category_matches / len(df_det_test)) * 100\n", "    exact_match_rate = (exact_matches / len(df_det_test)) * 100\n", "\n", "    results[k] = {\n", "        'category_accuracy': category_accuracy,\n", "        'exact_match': exact_match_rate\n", "    }\n", "\n", "    print(f\"   Top-{k} Results:\")\n", "    print(f\"      Category Accuracy:  {category_accuracy:.2f}%\")\n", "    print(f\"      Exact Match:        {exact_match_rate:.2f}%\")\n", "\n", "# 7. Print summary table\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"RETRIEVAL ACCURACY SUMMARY\")\n", "print(\"=\"*60)\n", "print(f\"{'K':<5} {'Category Accuracy':<20} {'Exact Match':<15}\")\n", "print(\"-\" * 60)\n", "for k in k_values:\n", "    print(f\"{k:<5}  {results[k]['category_accuracy']:>17.2f}%  {results[k]['exact_match']:>12.2f}%\")\n", "\n", "# 8. COMPUTE DISTANCE STATISTICS (Top-1)\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"DISTANCE STATISTICS (TOP-1 MATCHES)\")\n", "print(\"=\"*60)\n", "\n", "# Get Top-1 distances\n", "distances_top1, indices_top1 = index.search(test_embeddings.astype('float32'), 1)\n", "top1_distances = distances_top1[:, 0]\n", "\n", "# Compute statistics\n", "distance_stats = {\n", "    'mean': float(np.mean(top1_distances)),\n", "    'median': float(np.median(top1_distances)),\n", "    'min': float(np.min(top1_distances)),\n", "    'max': float(np.max(top1_distances)),\n", "    'std': float(np.std(top1_distances))\n", "}\n", "\n", "print(f\"\\nDistance Distribution:\")\n", "print(f\"   Mean:     {distance_stats['mean']:.4f}\")\n", "print(f\"   Median:   {distance_stats['median']:.4f}\")\n", "print(f\"   Min:      {distance_stats['min']:.4f}\")\n", "print(f\"   Max:      {distance_stats['max']:.4f}\")\n", "print(f\"   Std Dev:  {distance_stats['std']:.4f}\")\n", "\n", "# Distance distribution buckets\n", "print(f\"\\nDistance Distribution Breakdown:\")\n", "low = np.sum(top1_distances < 0.1)\n", "medium = np.sum((top1_distances >= 0.1) & (top1_distances < 0.5))\n", "high = np.sum(top1_distances >= 0.5)\n", "\n", "print(f\"   Distances < 0.1:     {low:>4} ({low/len(top1_distances)*100:>5.1f}%) - Excellent match\")\n", "print(f\"   Distances 0.1-0.5:   {medium:>4} ({medium/len(top1_distances)*100:>5.1f}%) - Good match\")\n", "print(f\"   Distances > 0.5:     {high:>4} ({high/len(top1_distances)*100:>5.1f}%) - Weak match\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 RETRIEVAL EVALUATION COMPLETE\")\n", "print(\"=\"*60)\n", "\n", "# Save results\n", "results_summary = {\n", "    'k_values': k_values,\n", "    'accuracies': results,\n", "    'distance_stats': distance_stats\n", "}\n", "\n", "import json\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "with open(f'{output_path}/retrieval_evaluation.json', 'w') as f:\n", "    json.dump(results_summary, f, indent=2)\n", "\n", "print(f\"\\n\u2713 Results saved to {output_path}/retrieval_evaluation.json\")"], "metadata": {"id": "k42UNzgsv0Fr", "colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["cedc160d3d884150a3690d62d95b15c7", "17b40adee0a441cba43ca5c53198c926", "014231e7b6564b658aae4d27ec6af8c6", "eab3caa6ea3e4b1b9134e3418820a417", "bfb40bc5f25f488c9bd665d6136667cb", "25596bed8f2343da9bd7113c424c9daf", "0c4fb730eee6435f8d5f0243ff5c9f9c", "0f8e26d3e30a456ebc33ce77fe458ca8", "77f22237ba94451d84a20f685c562db5", "4cbc8c9044974daebb695ff8549d85c5", "83b288453e5f4de09e76ec01c81f0904"]}, "outputId": "d4c82da6-278d-4149-8937-760597a1452e"}, "execution_count": 8, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "RETRIEVAL EVALUATION METRICS\n", "============================================================\n", "\n", "Loading test dataset...\n", "Loaded 1584 deterministic test queries\n", "\n", "Loading sentence transformer model...\n", "\n", "Encoding test queries...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Batches:   0%|          | 0/50 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cedc160d3d884150a3690d62d95b15c7"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["\n", "Loading FAISS index...\n", "Loading training retrieval data...\n", "   \u2713 Index loaded with 6,333 vectors\n", "   \u2713 Training data: 6,333 examples\n", "\n", "============================================================\n", "RETRIEVAL ACCURACY AT MULTIPLE K VALUES\n", "============================================================\n", "\n", "Evaluating Top-1 retrieval...\n", "   Top-1 Results:\n", "      Category Accuracy:  99.94%\n", "      Exact Match:        4.73%\n", "\n", "Evaluating Top-3 retrieval...\n", "   Top-3 Results:\n", "      Category Accuracy:  99.94%\n", "      Exact Match:        4.73%\n", "\n", "Evaluating Top-5 retrieval...\n", "   Top-5 Results:\n", "      Category Accuracy:  99.94%\n", "      Exact Match:        4.73%\n", "\n", "Evaluating Top-10 retrieval...\n", "   Top-10 Results:\n", "      Category Accuracy:  99.94%\n", "      Exact Match:        4.73%\n", "\n", "============================================================\n", "RETRIEVAL ACCURACY SUMMARY\n", "============================================================\n", "K     Category Accuracy    Exact Match    \n", "------------------------------------------------------------\n", "1                  99.94%          4.73%\n", "3                  99.94%          4.73%\n", "5                  99.94%          4.73%\n", "10                 99.94%          4.73%\n", "\n", "============================================================\n", "DISTANCE STATISTICS (TOP-1 MATCHES)\n", "============================================================\n", "\n", "Distance Distribution:\n", "   Mean:     0.1045\n", "   Median:   0.0641\n", "   Min:      0.0000\n", "   Max:      1.0209\n", "   Std Dev:  0.1307\n", "\n", "Distance Distribution Breakdown:\n", "   Distances < 0.1:     1084 ( 68.4%) - Excellent match\n", "   Distances 0.1-0.5:    456 ( 28.8%) - Good match\n", "   Distances > 0.5:       44 (  2.8%) - Weak match\n", "\n", "============================================================\n", "\u2713 RETRIEVAL EVALUATION COMPLETE\n", "============================================================\n", "\n", "\u2713 Results saved to /content/drive/MyDrive/NLP_Project/results//retrieval_evaluation.json\n"]}]}, {"cell_type": "code", "source": ["# ============================================================\n", "# CONFIDENCE INTERVALS FOR RETRIEVAL METRICS\n", "# ============================================================\n", "from statsmodels.stats.proportion import proportion_confint\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"CONFIDENCE INTERVALS FOR RETRIEVAL METRICS\")\n", "print(\"=\"*60)\n", "\n", "def compute_ci(n_correct, n_total):\n", "    \"\"\"Compute 95% Wilson score confidence interval.\"\"\"\n", "    accuracy = n_correct / n_total\n", "    ci_low, ci_high = proportion_confint(\n", "        n_correct,\n", "        n_total,\n", "        alpha=0.05,\n", "        method='wilson'\n", "    )\n", "    return accuracy, ci_low, ci_high\n", "\n", "n_test = len(df_det_test)\n", "\n", "# 1. Category Accuracy CI (from your k=1 results)\n", "print(\"\\n1. CATEGORY ACCURACY:\")\n", "n_category_correct = category_matches  # From your evaluation loop\n", "acc, ci_low, ci_high = compute_ci(n_category_correct, n_test)\n", "print(f\"   n_correct: {n_category_correct}/{n_test}\")\n", "print(f\"   {acc*100:.2f}% [{ci_low*100:.2f}%, {ci_high*100:.2f}%]\")\n", "\n", "# 2. Exact Match Rate CI\n", "print(\"\\n2. EXACT MATCH RATE:\")\n", "n_exact_matches = exact_matches  # From your evaluation loop\n", "exact_rate, ci_low, ci_high = compute_ci(n_exact_matches, n_test)\n", "print(f\"   n_exact: {n_exact_matches}/{n_test}\")\n", "print(f\"   {exact_rate*100:.2f}% [{ci_low*100:.2f}%, {ci_high*100:.2f}%]\")\n", "\n", "# 3. Distance Distribution CIs\n", "print(\"\\n3. DISTANCE DISTRIBUTION:\")\n", "\n", "# Excellent (< 0.1)\n", "n_excellent = low  # From your distance distribution code\n", "acc, ci_low, ci_high = compute_ci(n_excellent, n_test)\n", "print(f\"   Excellent (<0.1):  {n_excellent}/{n_test}\")\n", "print(f\"                      {acc*100:.1f}% [{ci_low*100:.1f}%, {ci_high*100:.1f}%]\")\n", "\n", "# Good (0.1-0.5)\n", "n_good = medium  # From your distance distribution code\n", "acc, ci_low, ci_high = compute_ci(n_good, n_test)\n", "print(f\"   Good (0.1-0.5):    {n_good}/{n_test}\")\n", "print(f\"                      {acc*100:.1f}% [{ci_low*100:.1f}%, {ci_high*100:.1f}%]\")\n", "\n", "# Weak (> 0.5)\n", "n_weak = high  # From your distance distribution code\n", "acc, ci_low, ci_high = compute_ci(n_weak, n_test)\n", "print(f\"   Weak (>0.5):       {n_weak}/{n_test}\")\n", "print(f\"                      {acc*100:.1f}% [{ci_low*100:.1f}%, {ci_high*100:.1f}%]\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 CONFIDENCE INTERVALS COMPUTED FROM ACTUAL DATA\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "2H51viLiKgON", "outputId": "6b6e656d-63e1-4db2-8ca0-5b363f65b5e0"}, "execution_count": 10, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "CONFIDENCE INTERVALS FOR RETRIEVAL METRICS\n", "============================================================\n", "\n", "1. CATEGORY ACCURACY:\n", "   n_correct: 1583/1584\n", "   99.94% [99.64%, 99.99%]\n", "\n", "2. EXACT MATCH RATE:\n", "   n_exact: 75/1584\n", "   4.73% [3.79%, 5.89%]\n", "\n", "3. DISTANCE DISTRIBUTION:\n", "   Excellent (<0.1):  1084/1584\n", "                      68.4% [66.1%, 70.7%]\n", "   Good (0.1-0.5):    456/1584\n", "                      28.8% [26.6%, 31.1%]\n", "   Weak (>0.5):       44/1584\n", "                      2.8% [2.1%, 3.7%]\n", "\n", "============================================================\n", "\u2713 CONFIDENCE INTERVALS COMPUTED FROM ACTUAL DATA\n", "============================================================\n"]}]}, {"cell_type": "markdown", "source": ["## 5.2: Binary Classifier"], "metadata": {"id": "jmDU1doQv4kq"}}, {"cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import confusion_matrix\n", "import pickle\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"PER-CATEGORY CLASSIFIER ANALYSIS\")\n", "print(\"=\"*60)\n", "\n", "# 1. Load test dataset\n", "print(\"\\nLoading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "print(f\"Loaded {len(df_test)} test examples\")\n", "\n", "# 2. Load trained classifier and vectorizer\n", "classifier_path = '/content/drive/MyDrive/NLP_Project/models/classifier'\n", "\n", "print(\"\\nLoading trained classifier...\")\n", "with open(f'{classifier_path}/logistic_regression.pkl', 'rb') as f:\n", "    classifier = pickle.load(f)\n", "\n", "with open(f'{classifier_path}/tfidf_vectorizer.pkl', 'rb') as f:\n", "    vectorizer = pickle.load(f)\n", "\n", "print(\"   \u2713 Classifier and vectorizer loaded\")\n", "\n", "# 3. Make predictions with confidence scores\n", "print(\"\\nMaking predictions on test set...\")\n", "X_test = vectorizer.transform(df_test['instruction'])\n", "y_test = df_test['label']\n", "\n", "# Predictions\n", "y_pred = classifier.predict(X_test)\n", "\n", "# Confidence scores (probabilities)\n", "y_proba = classifier.predict_proba(X_test)\n", "\n", "# Get confidence for predicted class\n", "confidence_scores = np.max(y_proba, axis=1)\n", "\n", "# Add predictions and confidence to dataframe\n", "df_test['predicted_label'] = y_pred\n", "df_test['confidence'] = confidence_scores\n", "df_test['correct'] = (y_pred == y_test).astype(int)\n", "\n", "print(\"   \u2713 Predictions complete\")\n", "\n", "# 4. PART 1: Per-Category Accuracy Breakdown\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"CLASSIFIER ACCURACY BY CATEGORY\")\n", "print(\"=\"*60)\n", "\n", "categories = df_test['category'].unique()\n", "category_results = []\n", "\n", "for category in sorted(categories):\n", "    # Filter for this category\n", "    cat_df = df_test[df_test['category'] == category]\n", "\n", "    # Calculate metrics\n", "    total = len(cat_df)\n", "    correct = cat_df['correct'].sum()\n", "    accuracy = (correct / total) * 100\n", "\n", "    category_results.append({\n", "        'category': category,\n", "        'total': total,\n", "        'correct': correct,\n", "        'accuracy': accuracy\n", "    })\n", "\n", "    print(f\"\\n{category}:\")\n", "    print(f\"   Total examples: {total}\")\n", "    print(f\"   Correct: {correct}\")\n", "    print(f\"   Accuracy: {accuracy:.2f}%\")\n", "\n", "# Create summary dataframe\n", "df_category_results = pd.DataFrame(category_results)\n", "df_category_results = df_category_results.sort_values('accuracy', ascending=False)\n", "\n", "print(\"\\n\" + \"-\"*60)\n", "print(\"SUMMARY TABLE\")\n", "print(\"-\"*60)\n", "print(f\"{'Category':<20} {'Total':<10} {'Correct':<10} {'Accuracy':<10}\")\n", "print(\"-\"*60)\n", "for _, row in df_category_results.iterrows():\n", "    print(f\"{row['category']:<20} {row['total']:<10} {row['correct']:<10} {row['accuracy']:>8.2f}%\")\n", "\n", "# 5. PART 2: Confusion Matrix\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"CONFUSION MATRIX\")\n", "print(\"=\"*60)\n", "\n", "cm = confusion_matrix(y_test, y_pred)\n", "print(f\"\\n{'':20} {'Predicted Det':>15} {'Predicted Indet':>15}\")\n", "print(f\"{'True Det':20} {cm[0][0]:>15} {cm[0][1]:>15}\")\n", "print(f\"{'True Indet':20} {cm[1][0]:>15} {cm[1][1]:>15}\")\n", "\n", "# 6. PART 3: Confidence Distribution Analysis\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"PREDICTION CONFIDENCE DISTRIBUTION\")\n", "print(\"=\"*60)\n", "\n", "# Overall confidence statistics\n", "print(\"\\nOverall Confidence Statistics:\")\n", "print(f\"   Mean:     {df_test['confidence'].mean():.4f}\")\n", "print(f\"   Median:   {df_test['confidence'].median():.4f}\")\n", "print(f\"   Min:      {df_test['confidence'].min():.4f}\")\n", "print(f\"   Max:      {df_test['confidence'].max():.4f}\")\n", "print(f\"   Std Dev:  {df_test['confidence'].std():.4f}\")\n", "\n", "# Confidence by correctness\n", "print(\"\\nConfidence by Prediction Correctness:\")\n", "correct_conf = df_test[df_test['correct'] == 1]['confidence']\n", "incorrect_conf = df_test[df_test['correct'] == 0]['confidence']\n", "\n", "print(f\"   Correct predictions - Mean confidence:   {correct_conf.mean():.4f}\")\n", "if len(incorrect_conf) > 0:\n", "    print(f\"   Incorrect predictions - Mean confidence: {incorrect_conf.mean():.4f}\")\n", "else:\n", "    print(f\"   Incorrect predictions - Mean confidence: N/A (no errors)\")\n", "\n", "# Confidence distribution buckets\n", "print(\"\\nConfidence Distribution:\")\n", "very_high = np.sum(df_test['confidence'] >= 0.9)\n", "high = np.sum((df_test['confidence'] >= 0.7) & (df_test['confidence'] < 0.9))\n", "medium = np.sum((df_test['confidence'] >= 0.5) & (df_test['confidence'] < 0.7))\n", "low = np.sum(df_test['confidence'] < 0.5)\n", "\n", "total = len(df_test)\n", "print(f\"   Very High (\u22650.9): {very_high:>5} ({very_high/total*100:>5.1f}%)\")\n", "print(f\"   High (0.7-0.9):   {high:>5} ({high/total*100:>5.1f}%)\")\n", "print(f\"   Medium (0.5-0.7): {medium:>5} ({medium/total*100:>5.1f}%)\")\n", "print(f\"   Low (<0.5):       {low:>5} ({low/total*100:>5.1f}%)\")\n", "\n", "# 7. PART 4: High-Confidence vs Low-Confidence Accuracy\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"HIGH-CONFIDENCE VS LOW-CONFIDENCE ACCURACY\")\n", "print(\"=\"*60)\n", "\n", "# Define thresholds\n", "high_confidence_threshold = 0.9\n", "low_confidence_threshold = 0.7\n", "\n", "# Split data\n", "high_conf_df = df_test[df_test['confidence'] > high_confidence_threshold]\n", "medium_conf_df = df_test[(df_test['confidence'] >= low_confidence_threshold) &\n", "                          (df_test['confidence'] <= high_confidence_threshold)]\n", "low_conf_df = df_test[df_test['confidence'] < low_confidence_threshold]\n", "\n", "# Calculate accuracies\n", "high_conf_acc = high_conf_df['correct'].mean() * 100 if len(high_conf_df) > 0 else 0\n", "medium_conf_acc = medium_conf_df['correct'].mean() * 100 if len(medium_conf_df) > 0 else 0\n", "low_conf_acc = low_conf_df['correct'].mean() * 100 if len(low_conf_df) > 0 else 0\n", "\n", "print(f\"\\nHigh Confidence (>{high_confidence_threshold}):\")\n", "print(f\"   Examples: {len(high_conf_df)}\")\n", "print(f\"   Accuracy: {high_conf_acc:.2f}%\")\n", "print(f\"   Errors: {len(high_conf_df) - high_conf_df['correct'].sum()}\")\n", "\n", "print(f\"\\nMedium Confidence ({low_confidence_threshold}-{high_confidence_threshold}):\")\n", "print(f\"   Examples: {len(medium_conf_df)}\")\n", "print(f\"   Accuracy: {medium_conf_acc:.2f}%\")\n", "print(f\"   Errors: {len(medium_conf_df) - medium_conf_df['correct'].sum()}\")\n", "\n", "print(f\"\\nLow Confidence (<{low_confidence_threshold}):\")\n", "print(f\"   Examples: {len(low_conf_df)}\")\n", "print(f\"   Accuracy: {low_conf_acc:.2f}%\")\n", "print(f\"   Errors: {len(low_conf_df) - low_conf_df['correct'].sum()}\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 CLASSIFIER ANALYSIS COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0hAl-euT4Syv", "outputId": "4b9e83a5-aafb-40cc-c615-a625a253209a"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "PER-CATEGORY CLASSIFIER ANALYSIS\n", "============================================================\n", "\n", "Loading test dataset...\n", "Loaded 3978 test examples\n", "\n", "Loading trained classifier...\n", "   \u2713 Classifier and vectorizer loaded\n", "\n", "Making predictions on test set...\n", "   \u2713 Predictions complete\n", "\n", "============================================================\n", "CLASSIFIER ACCURACY BY CATEGORY\n", "============================================================\n", "\n", "ACCOUNT:\n", "   Total examples: 1197\n", "   Correct: 1197\n", "   Accuracy: 100.00%\n", "\n", "CANCEL:\n", "   Total examples: 190\n", "   Correct: 190\n", "   Accuracy: 100.00%\n", "\n", "CONTACT:\n", "   Total examples: 400\n", "   Correct: 400\n", "   Accuracy: 100.00%\n", "\n", "FEEDBACK:\n", "   Total examples: 399\n", "   Correct: 399\n", "   Accuracy: 100.00%\n", "\n", "INVOICE:\n", "   Total examples: 400\n", "   Correct: 400\n", "   Accuracy: 100.00%\n", "\n", "ORDER:\n", "   Total examples: 798\n", "   Correct: 797\n", "   Accuracy: 99.87%\n", "\n", "SHIPPING:\n", "   Total examples: 394\n", "   Correct: 393\n", "   Accuracy: 99.75%\n", "\n", "SUBSCRIPTION:\n", "   Total examples: 200\n", "   Correct: 200\n", "   Accuracy: 100.00%\n", "\n", "------------------------------------------------------------\n", "SUMMARY TABLE\n", "------------------------------------------------------------\n", "Category             Total      Correct    Accuracy  \n", "------------------------------------------------------------\n", "ACCOUNT              1197       1197         100.00%\n", "CANCEL               190        190          100.00%\n", "CONTACT              400        400          100.00%\n", "FEEDBACK             399        399          100.00%\n", "INVOICE              400        400          100.00%\n", "SUBSCRIPTION         200        200          100.00%\n", "ORDER                798        797           99.87%\n", "SHIPPING             394        393           99.75%\n", "\n", "============================================================\n", "CONFUSION MATRIX\n", "============================================================\n", "\n", "                       Predicted Det Predicted Indet\n", "True Det                        1583               1\n", "True Indet                         1            2393\n", "\n", "============================================================\n", "PREDICTION CONFIDENCE DISTRIBUTION\n", "============================================================\n", "\n", "Overall Confidence Statistics:\n", "   Mean:     0.9682\n", "   Median:   0.9801\n", "   Min:      0.5052\n", "   Max:      0.9998\n", "   Std Dev:  0.0419\n", "\n", "Confidence by Prediction Correctness:\n", "   Correct predictions - Mean confidence:   0.9683\n", "   Incorrect predictions - Mean confidence: 0.6485\n", "\n", "Confidence Distribution:\n", "   Very High (\u22650.9):  3786 ( 95.2%)\n", "   High (0.7-0.9):     174 (  4.4%)\n", "   Medium (0.5-0.7):    18 (  0.5%)\n", "   Low (<0.5):           0 (  0.0%)\n", "\n", "============================================================\n", "HIGH-CONFIDENCE VS LOW-CONFIDENCE ACCURACY\n", "============================================================\n", "\n", "High Confidence (>0.9):\n", "   Examples: 3786\n", "   Accuracy: 100.00%\n", "   Errors: 0\n", "\n", "Medium Confidence (0.7-0.9):\n", "   Examples: 174\n", "   Accuracy: 100.00%\n", "   Errors: 0\n", "\n", "Low Confidence (<0.7):\n", "   Examples: 18\n", "   Accuracy: 88.89%\n", "   Errors: 2\n", "\n", "============================================================\n", "\u2713 CLASSIFIER ANALYSIS COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "code", "source": ["# ============================================================\n", "# CONFIDENCE INTERVALS\n", "# ============================================================\n", "from statsmodels.stats.proportion import proportion_confint\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"CONFIDENCE INTERVALS (95%)\")\n", "print(\"=\"*60)\n", "\n", "def compute_ci(n_correct, n_total):\n", "    \"\"\"Compute 95% Wilson score confidence interval.\"\"\"\n", "    accuracy = n_correct / n_total\n", "    ci_low, ci_high = proportion_confint(\n", "        n_correct,\n", "        n_total,\n", "        alpha=0.05,\n", "        method='wilson'\n", "    )\n", "    return accuracy, ci_low, ci_high\n", "\n", "print(f\"\\n{'Category':<20} {'Accuracy':>10} {'95% CI':>28}\")\n", "print(\"-\"*60)\n", "\n", "for _, row in df_category_results.iterrows():\n", "    category = row['category']\n", "    n_correct = int(row['correct'])\n", "    n_total = int(row['total'])\n", "\n", "    acc, ci_low, ci_high = compute_ci(n_correct, n_total)\n", "\n", "    print(f\"{category:<20} {acc*100:>9.2f}% [{ci_low*100:>6.2f}%, {ci_high*100:>6.2f}%]\")\n", "\n", "# Overall\n", "overall_correct = df_test['correct'].sum()\n", "overall_total = len(df_test)\n", "acc, ci_low, ci_high = compute_ci(overall_correct, overall_total)\n", "\n", "print(\"-\"*60)\n", "print(f\"{'OVERALL':<20} {acc*100:>9.2f}% [{ci_low*100:>6.2f}%, {ci_high*100:>6.2f}%]\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "guLzntYBGcaV", "outputId": "5f350e93-5c47-46da-85c0-673a6c438518"}, "execution_count": 4, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "CONFIDENCE INTERVALS (95%)\n", "============================================================\n", "\n", "Category               Accuracy                       95% CI\n", "------------------------------------------------------------\n", "ACCOUNT                 100.00% [ 99.68%, 100.00%]\n", "CANCEL                  100.00% [ 98.02%, 100.00%]\n", "CONTACT                 100.00% [ 99.05%, 100.00%]\n", "FEEDBACK                100.00% [ 99.05%, 100.00%]\n", "INVOICE                 100.00% [ 99.05%, 100.00%]\n", "SUBSCRIPTION            100.00% [ 98.12%, 100.00%]\n", "ORDER                    99.87% [ 99.29%,  99.98%]\n", "SHIPPING                 99.75% [ 98.58%,  99.96%]\n", "------------------------------------------------------------\n", "OVERALL                  99.95% [ 99.82%,  99.99%]\n"]}]}, {"cell_type": "markdown", "source": ["## 5.3: LLM"], "metadata": {"id": "e627IX-kv5Bm"}}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import torch\n", "import json\n", "import os\n", "from peft import PeftModel\n", "from transformers import AutoTokenizer, AutoModelForCausalLM\n", "from rouge_score import rouge_scorer\n", "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n", "from bert_score import score as bert_score\n", "import time\n", "\n", "print(\"=\"*60)\n", "print(\"EVALUATING FINE-TUNED PHI-2 ON TEST SET\")\n", "print(\"=\"*60)\n", "\n", "# Configuration\n", "checkpoint_path = \"/content/drive/MyDrive/NLP_Project/checkpoints/phi2_lora/final_model\"\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "progress_file = f'{output_path}/evaluation_progress1.json'\n", "results_file = f'{output_path}/llm_generation_samples1.csv'\n", "save_frequency = 20  # Save every 20 examples\n", "\n", "os.makedirs(output_path, exist_ok=True)\n", "\n", "# Load test dataset\n", "print(\"\\n1. Loading test dataset...\")\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# Filter for indeterministic queries only (label=1)\n", "df_indet_test = df_test[df_test['label'] == 1].reset_index(drop=True)\n", "\n", "# All queries for evaluation\n", "df_eval = df_indet_test.reset_index(drop=True)\n", "print(f\"   \u2713 Loaded {len(df_eval)} indeterministic test queries for evaluation\")\n", "\n", "# Load fine-tuned model\n", "print(\"\\n2. Loading fine-tuned Phi-2 model...\")\n", "base_model = AutoModelForCausalLM.from_pretrained(\n", "    \"microsoft/phi-2\",\n", "    device_map={\"\": 0},  # Force all to GPU 0\n", "    trust_remote_code=True,\n", "    torch_dtype=torch.float16\n", ")\n", "\n", "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n", "model = model.to(\"cuda\")  # Explicitly move to GPU\n", "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n", "\n", "print(\"   \u2713 Model loaded successfully\")\n", "print(f\"   GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n", "\n", "# Check for existing progress\n", "print(\"\\n3. Checking for existing progress...\")\n", "start_idx = 0\n", "results = []\n", "\n", "if os.path.exists(progress_file):\n", "    with open(progress_file, 'r') as f:\n", "        progress = json.load(f)\n", "    start_idx = progress['last_completed_idx'] + 1\n", "\n", "    # Load existing results\n", "    if os.path.exists(results_file):\n", "        results_df = pd.read_csv(results_file)\n", "        results = results_df.to_dict('records')\n", "\n", "    print(f\"   \u2713 Found existing progress: resuming from index {start_idx}\")\n", "    print(f\"   Already processed: {len(results)} examples\")\n", "else:\n", "    print(\"   No existing progress found - starting from beginning\")\n", "\n", "# Generate responses\n", "print(f\"\\n4. Generating responses for {len(df_eval) - start_idx} remaining queries...\")\n", "print(f\"   Saving every {save_frequency} examples\")\n", "print(\"=\"*60)\n", "\n", "for i in range(start_idx, len(df_eval)):\n", "    query = df_eval.iloc[i]['instruction']\n", "    reference = df_eval.iloc[i]['response']\n", "    category = df_eval.iloc[i]['category']\n", "\n", "    print(f\"\\n[{i+1}/{len(df_eval)}] Processing: {query[:60]}...\")\n", "\n", "    try:\n", "        # Generate response\n", "        prompt = f\"Customer: {query}\\nAssistant:\"\n", "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n", "\n", "        start_time = time.time()\n", "        with torch.no_grad():\n", "            outputs = model.generate(\n", "                **inputs,\n", "                max_new_tokens=200,\n", "                do_sample=True,\n", "                temperature=0.7,\n", "                top_p=0.9,\n", "                pad_token_id=tokenizer.eos_token_id\n", "            )\n", "\n", "        generation_time = time.time() - start_time\n", "\n", "        generated_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "        generated_response = generated_response.split(\"Assistant:\")[-1].strip()\n", "\n", "        # Store result\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'reference_response': reference,\n", "            'generated_response': generated_response,\n", "            'generation_time_sec': generation_time\n", "        })\n", "\n", "        print(f\"   \u2713 Generated ({generation_time:.2f}s)\")\n", "\n", "    except Exception as e:\n", "        print(f\"   \u2717 Error: {str(e)}\")\n", "        results.append({\n", "            'query': query,\n", "            'category': category,\n", "            'reference_response': reference,\n", "            'generated_response': f\"ERROR: {str(e)}\",\n", "            'generation_time_sec': None\n", "        })\n", "\n", "    # Save progress periodically\n", "    if (i + 1) % save_frequency == 0 or i == len(df_eval) - 1:\n", "        print(f\"\\n   \ud83d\udcbe Saving progress at index {i}...\")\n", "\n", "        # Save results\n", "        results_df = pd.DataFrame(results)\n", "        results_df.to_csv(results_file, index=False)\n", "\n", "        # Save progress tracker\n", "        with open(progress_file, 'w') as f:\n", "            json.dump({\n", "                'last_completed_idx': i,\n", "                'total_examples': len(df_eval),\n", "                'timestamp': pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n", "            }, f, indent=2)\n", "\n", "        print(f\"   \u2713 Saved {len(results)} results\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 GENERATION COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 431, "referenced_widgets": ["c038bf5b5e884656b667338e2f4b4682", "0dc73dcc72494548a3e3775c65e48282", "e2e6215fb1b0442d8740714dae1ef018", "f2138651dabf4caabf6078d497061878", "43ae34b74b4b4f35b0ab10d2bc8946e7", "65a70cc43bb7405fbc0edb09a7b0f774", "188b547f0a9b4f7cb859f58836fd1a67", "e3508b9a1c4b42398adc210023d38f26", "798bc47d246e48d3ab2be4881e015641", "6028ea78b85e48ceacf445bcb8946523", "fd247ab8d5d5497486934d4e2abf0f7d"]}, "id": "W0hNR1OZ9c_8", "outputId": "9f4ed7d3-fa75-47d3-cc8a-e1cb3ae4b197"}, "execution_count": 2, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "EVALUATING FINE-TUNED PHI-2 ON TEST SET\n", "============================================================\n", "\n", "1. Loading test dataset...\n", "   \u2713 Loaded 2394 indeterministic test queries for evaluation\n", "\n", "2. Loading fine-tuned Phi-2 model...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c038bf5b5e884656b667338e2f4b4682"}}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 Model loaded successfully\n", "   GPU memory: 5.61 GB\n", "\n", "3. Checking for existing progress...\n", "   \u2713 Found existing progress: resuming from index 2394\n", "   Already processed: 2394 examples\n", "\n", "4. Generating responses for 0 remaining queries...\n", "   Saving every 20 examples\n", "============================================================\n", "\n", "============================================================\n", "\u2713 GENERATION COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import json\n", "import os\n", "from rouge_score import rouge_scorer\n", "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n", "from bert_score import score as bert_score\n", "\n", "print(\"=\"*60)\n", "print(\"COMPUTING METRICS FROM SAVED RESULTS\")\n", "print(\"=\"*60)\n", "\n", "# Load saved results\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "results_file = f'{output_path}/llm_generation_samples1.csv'\n", "\n", "print(\"\\nLoading saved generation results...\")\n", "results_df = pd.read_csv(results_file)\n", "results = results_df.to_dict('records')\n", "\n", "print(f\"   \u2713 Loaded {len(results)} generated responses\")\n", "\n", "# Filter out errors\n", "valid_results = [r for r in results if not str(r['generated_response']).startswith('ERROR')]\n", "print(f\"   Valid generations: {len(valid_results)}/{len(results)}\")\n", "\n", "if len(valid_results) == 0:\n", "    print(\"\u274c No valid generations to evaluate!\")\n", "else:\n", "    # ROUGE scores\n", "    print(\"\\n1. Computing ROUGE scores...\")\n", "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n", "\n", "    rouge1_scores = []\n", "    rouge2_scores = []\n", "    rougeL_scores = []\n", "\n", "    for r in valid_results:\n", "        scores = rouge_scorer_obj.score(r['reference_response'], r['generated_response'])\n", "        rouge1_scores.append(scores['rouge1'].fmeasure)\n", "        rouge2_scores.append(scores['rouge2'].fmeasure)\n", "        rougeL_scores.append(scores['rougeL'].fmeasure)\n", "\n", "    print(f\"   \u2713 ROUGE computed for {len(valid_results)} examples\")\n", "\n", "    # BLEU scores\n", "    print(\"\\n2. Computing BLEU scores...\")\n", "    bleu1_scores = []\n", "    bleu2_scores = []\n", "    bleu3_scores = []\n", "    bleu4_scores = []\n", "    smoothie = SmoothingFunction().method4\n", "\n", "    for r in valid_results:\n", "        reference_tokens = [r['reference_response'].split()]\n", "        generated_tokens = r['generated_response'].split()\n", "\n", "        bleu1_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n", "        bleu2_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n", "        bleu3_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n", "        bleu4_scores.append(sentence_bleu(reference_tokens, generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n", "\n", "    print(f\"   \u2713 BLEU computed for {len(valid_results)} examples\")\n", "\n", "    # BERTScore\n", "    print(\"\\n3. Computing BERTScore (this may take a few minutes)...\")\n", "    references = [r['reference_response'] for r in valid_results]\n", "    candidates = [r['generated_response'] for r in valid_results]\n", "\n", "    P, R, F1 = bert_score(candidates, references, lang='en', verbose=False)\n", "\n", "    print(f\"   \u2713 BERTScore computed for {len(valid_results)} examples\")\n", "\n", "    # Response length analysis\n", "    gen_lengths = [len(r['generated_response'].split()) for r in valid_results]\n", "    ref_lengths = [len(r['reference_response'].split()) for r in valid_results]\n", "\n", "    # Print results\n", "    print(\"\\n\" + \"=\"*60)\n", "    print(\"EVALUATION METRICS\")\n", "    print(\"=\"*60)\n", "\n", "    print(\"\\nROUGE Scores (F1):\")\n", "    print(f\"  ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n", "    print(f\"  ROUGE-2: {np.mean(rouge2_scores):.4f}\")\n", "    print(f\"  ROUGE-L: {np.mean(rougeL_scores):.4f}\")\n", "\n", "    print(\"\\nBLEU Scores:\")\n", "    print(f\"  BLEU-1: {np.mean(bleu1_scores):.4f}\")\n", "    print(f\"  BLEU-2: {np.mean(bleu2_scores):.4f}\")\n", "    print(f\"  BLEU-3: {np.mean(bleu3_scores):.4f}\")\n", "    print(f\"  BLEU-4: {np.mean(bleu4_scores):.4f}\")\n", "\n", "    print(\"\\nBERTScore:\")\n", "    print(f\"  Precision: {P.mean():.4f}\")\n", "    print(f\"  Recall:    {R.mean():.4f}\")\n", "    print(f\"  F1:        {F1.mean():.4f} (primary metric)\")\n", "\n", "    print(\"\\nResponse Length:\")\n", "    print(f\"  Generated: Mean {np.mean(gen_lengths):.1f} words, Median {np.median(gen_lengths):.1f}\")\n", "    print(f\"  Reference: Mean {np.mean(ref_lengths):.1f} words, Median {np.median(ref_lengths):.1f}\")\n", "\n", "    # Save metrics\n", "    metrics = {\n", "        'num_examples': len(valid_results),\n", "        'rouge_scores': {\n", "            'rouge1_f1': float(np.mean(rouge1_scores)),\n", "            'rouge2_f1': float(np.mean(rouge2_scores)),\n", "            'rougeL_f1': float(np.mean(rougeL_scores))\n", "        },\n", "        'bleu_scores': {\n", "            'bleu1': float(np.mean(bleu1_scores)),\n", "            'bleu2': float(np.mean(bleu2_scores)),\n", "            'bleu3': float(np.mean(bleu3_scores)),\n", "            'bleu4': float(np.mean(bleu4_scores))\n", "        },\n", "        'bertscore': {\n", "            'precision': float(P.mean()),\n", "            'recall': float(R.mean()),\n", "            'f1': float(F1.mean())\n", "        },\n", "        'response_length': {\n", "            'generated_mean': float(np.mean(gen_lengths)),\n", "            'generated_median': float(np.median(gen_lengths)),\n", "            'reference_mean': float(np.mean(ref_lengths)),\n", "            'reference_median': float(np.median(ref_lengths))\n", "        }\n", "    }\n", "\n", "    with open(f'{output_path}/llm_evaluation_metrics1.json', 'w') as f:\n", "        json.dump(metrics, f, indent=2)\n", "\n", "    print(f\"\\n\u2713 Metrics saved to {output_path}/llm_evaluation_metrics1.json\")\n", "\n", "# Clean up progress file\n", "if os.path.exists(f'{output_path}/evaluation_progress1.json'):\n", "    os.remove(f'{output_path}/evaluation_progress1.json')\n", "    print(f\"\u2713 Cleaned up progress tracker\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 EVALUATION COMPLETE\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["2ec72aed0c0844c48604f4bade009930", "f3a90f7976f440aabad529727632e465", "6d5f4c59f165410893bc70d6df760511", "f4ac031940434091a18fc74cba8091c3", "92db8a0bd40a4bb48dbee903b709ea8c", "efa3e723275b4fd1bb61f476a4cd7687", "0987d6e9e17d42c49648da3a21e34b8a", "27195496774d4c44bbeb0c1deaa10529", "5f815b69def14f69abecf5db256ae6a7", "53d41447227843b98ff93bfadddb09c2", "08d79f1b6762433c8f5569ed6e0f1a4a", "8209e2d67869484aac8199da2fc468c4", "ca6fa704de0e42eb861702914d6bd2ae", "483c2c5b488f460abb67f7102219819d", "d46721c2b8a3479a96d76173198d8629", "9e362f3d47a940e884fc5422773f59df", "f24584678a1746ec94338596dbc19b6c", "0f3d1f97ebce40109ca25c968e8b2d46", "55bb79788bcb440d94d1e90e7f3c3443", "ffc13beb4a284f508ae1dfea10d84cd7", "0533db9bfdc740d7a0a6da4380cfebb9", "f4116c8121f34134a7eb45cd0cc6ae83", "1baaca289eb34f219ac990ce72f28890", "8d9175d700eb4ab694a5c54519900d2f", "d53a3adc91ac43b3a3528c52bfe1ec28", "5500621dac284380ba6dcab694b12ca1", "62a3d7cd9dfc484fbf5a80472a514861", "00d91c2decb746b3bb8213211a622627", "0898aec61c674e3785a4f1b070b1abc0", "c72c206cd916432c885a54723aa764c8", "89b33b585d604dc382a4c194dcacb344", "ce3f8f8b9efb4415a3823c8f24364805", "1010a7c9fcde4bd4a3bceaffc2959005", "15a7dd5d4d374be7b30c527630f72b84", "f1cd33bfdf3a4ba3a1c76e7a0a6c6503", "f9bccd482cdc4115a9b145ae5ba03c4b", "cbc65c9a86294679aa255b89a8fe07b0", "fa0b722e0bed42b2964f8dc1fe930c57", "1f11f9948a2c4749aeb7d86bd550f5e5", "6f478ab2bc37473b8fbb61f7eb5a338d", "12ae8ab275b848418e5f66baed4bee65", "1c92e8a54a694158965415578562547c", "76f573c851a348d3a7743d5c0e163593", "1bb5f782388246a6b7f14106579c618f", "b1291179ec854798976bb1aa13b26541", "59cba9569da64cee8f620b6b92fdda32", "cb04680b64f34f2db294803d1ecdc3dd", "dfed8da4cc0a4134b1d9fa9b954c7d75", "449d5866c54d47338cc8e9519f9dac59", "2c35a93a421b4ef79d9a4e5d94fee4cb", "17fe25ca06a14d91bd7d73c3f1756c0a", "988cc2dced254901a003405d27789063", "6e0515799206405681a637dd022d5d8e", "1c66b49ebd6a4930b9b501d265287447", "3ba536f0c3274342928b1adb91d33b7c", "4e19649d03a741e3b387bd3a72dc9123", "7bccd97f39aa47438363431934a5a4a6", "c325d72ac59d4df58185979b6bb9fef6", "ce3d679853f2423a8ee4ecf9f0f0c51c", "a938270caa8945aaa4ac24db50edb654", "0173fe13b52544eaab76ad4ca3616542", "2ee4b3e9acdd49ca92fcf255575c7e1f", "45c1f23844b54274a2b81eb2e754a341", "553619e3285543ce80aa6ba058934d66", "18c87f3a1552456c89d6fddc3c631ab5", "147eabb907754c27a58c7a6bce432eb7"]}, "id": "9DP9ow-2AGwk", "outputId": "6e07cecd-269a-47ed-fffd-94d8b3795ee9"}, "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "COMPUTING METRICS FROM SAVED RESULTS\n", "============================================================\n", "\n", "Loading saved generation results...\n", "   \u2713 Loaded 2394 generated responses\n", "   Valid generations: 2394/2394\n", "\n", "1. Computing ROUGE scores...\n", "   \u2713 ROUGE computed for 2394 examples\n", "\n", "2. Computing BLEU scores...\n", "   \u2713 BLEU computed for 2394 examples\n", "\n", "3. Computing BERTScore (this may take a few minutes)...\n"]}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2ec72aed0c0844c48604f4bade009930"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8209e2d67869484aac8199da2fc468c4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1baaca289eb34f219ac990ce72f28890"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "15a7dd5d4d374be7b30c527630f72b84"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b1291179ec854798976bb1aa13b26541"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4e19649d03a741e3b387bd3a72dc9123"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}, {"output_type": "stream", "name": "stdout", "text": ["   \u2713 BERTScore computed for 2394 examples\n", "\n", "============================================================\n", "EVALUATION METRICS\n", "============================================================\n", "\n", "ROUGE Scores (F1):\n", "  ROUGE-1: 0.5691\n", "  ROUGE-2: 0.3090\n", "  ROUGE-L: 0.3971\n", "\n", "BLEU Scores:\n", "  BLEU-1: 0.4219\n", "  BLEU-2: 0.3041\n", "  BLEU-3: 0.2402\n", "  BLEU-4: 0.1906\n", "\n", "BERTScore:\n", "  Precision: 0.9036\n", "  Recall:    0.9199\n", "  F1:        0.9115 (primary metric)\n", "\n", "Response Length:\n", "  Generated: Mean 133.8 words, Median 137.0\n", "  Reference: Mean 109.1 words, Median 94.0\n", "\n", "\u2713 Metrics saved to /content/drive/MyDrive/NLP_Project/results//llm_evaluation_metrics1.json\n", "\u2713 Cleaned up progress tracker\n", "\n", "============================================================\n", "\u2713 EVALUATION COMPLETE\n", "============================================================\n"]}]}, {"cell_type": "code", "source": ["# ============================================================\n", "# COMPUTE BOOTSTRAP CONFIDENCE INTERVALS\n", "# ============================================================\n", "from scipy import stats\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"COMPUTING 95% CONFIDENCE INTERVALS (Bootstrap)\")\n", "print(\"=\"*60)\n", "\n", "def bootstrap_ci(scores, n_bootstrap=10000):\n", "    \"\"\"Compute bootstrap confidence interval for mean.\"\"\"\n", "    bootstrap_means = []\n", "    n = len(scores)\n", "\n", "    for _ in range(n_bootstrap):\n", "        sample_indices = np.random.choice(n, size=n, replace=True)\n", "        sample = [scores[i] for i in sample_indices]\n", "        bootstrap_means.append(np.mean(sample))\n", "\n", "    ci_low = np.percentile(bootstrap_means, 2.5)\n", "    ci_high = np.percentile(bootstrap_means, 97.5)\n", "    mean_val = np.mean(scores)\n", "\n", "    return mean_val, ci_low, ci_high\n", "\n", "print(\"\\nROUGE Scores with 95% CI:\")\n", "r1_mean, r1_low, r1_high = bootstrap_ci(rouge1_scores)\n", "print(f\"  ROUGE-1: {r1_mean:.4f} [{r1_low:.4f}, {r1_high:.4f}]\")\n", "\n", "r2_mean, r2_low, r2_high = bootstrap_ci(rouge2_scores)\n", "print(f\"  ROUGE-2: {r2_mean:.4f} [{r2_low:.4f}, {r2_high:.4f}]\")\n", "\n", "rL_mean, rL_low, rL_high = bootstrap_ci(rougeL_scores)\n", "print(f\"  ROUGE-L: {rL_mean:.4f} [{rL_low:.4f}, {rL_high:.4f}]\")\n", "\n", "print(\"\\nBLEU Scores with 95% CI:\")\n", "b1_mean, b1_low, b1_high = bootstrap_ci(bleu1_scores)\n", "print(f\"  BLEU-1: {b1_mean:.4f} [{b1_low:.4f}, {b1_high:.4f}]\")\n", "\n", "b4_mean, b4_low, b4_high = bootstrap_ci(bleu4_scores)\n", "print(f\"  BLEU-4: {b4_mean:.4f} [{b4_low:.4f}, {b4_high:.4f}]\")\n", "\n", "print(\"\\nBERTScore with 95% CI:\")\n", "bert_f1_list = F1.numpy().tolist()\n", "bf1_mean, bf1_low, bf1_high = bootstrap_ci(bert_f1_list)\n", "print(f\"  F1: {bf1_mean:.4f} [{bf1_low:.4f}, {bf1_high:.4f}]\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"\u2713 CONFIDENCE INTERVALS COMPUTED\")\n", "print(\"=\"*60)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4a-9cBHLyvWk", "outputId": "9d70e489-85b1-4b9f-a3ec-1c882edcda1e"}, "execution_count": 4, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\n", "============================================================\n", "COMPUTING 95% CONFIDENCE INTERVALS (Bootstrap)\n", "============================================================\n", "\n", "ROUGE Scores with 95% CI:\n", "  ROUGE-1: 0.5691 [0.5649, 0.5733]\n", "  ROUGE-2: 0.3090 [0.3045, 0.3135]\n", "  ROUGE-L: 0.3971 [0.3924, 0.4019]\n", "\n", "BLEU Scores with 95% CI:\n", "  BLEU-1: 0.4219 [0.4174, 0.4264]\n", "  BLEU-4: 0.1906 [0.1868, 0.1946]\n", "\n", "BERTScore with 95% CI:\n", "  F1: 0.9115 [0.9106, 0.9124]\n", "\n", "============================================================\n", "\u2713 CONFIDENCE INTERVALS COMPUTED\n", "============================================================\n"]}]}, {"cell_type": "markdown", "source": ["## 5.4: Hybrid System"], "metadata": {"id": "sZcf_CI-v-xU"}}, {"cell_type": "code", "source": ["import pandas as pd\n", "\n", "print(\"=\"*60)\n", "print(\"COMPUTING HYBRID SYSTEM SCORE\")\n", "print(\"=\"*60)\n", "\n", "# Load test dataset to get proportions\n", "df_test = pd.read_csv('/content/drive/MyDrive/NLP_Project/data/test_dataset.csv')\n", "\n", "# Calculate label distribution\n", "total_test = len(df_test)\n", "det_count = len(df_test[df_test['label'] == 0])\n", "indet_count = len(df_test[df_test['label'] == 1])\n", "\n", "det_proportion = det_count / total_test\n", "indet_proportion = indet_count / total_test\n", "\n", "print(f\"\\nTest Set Distribution:\")\n", "print(f\"  Total examples: {total_test}\")\n", "print(f\"  Deterministic (label=0): {det_count} ({det_proportion*100:.1f}%)\")\n", "print(f\"  Indeterministic (label=1): {indet_count} ({indet_proportion*100:.1f}%)\")\n", "\n", "# Component scores\n", "classification_accuracy = 0.9994  # Binary classifier accuracy\n", "retrieval_accuracy = 0.9994       # Top-1 category accuracy on retrieval\n", "bertscore_f1 = 0.9126             # BERTScore F1 on LLM generations\n", "\n", "print(f\"\\nComponent Scores:\")\n", "print(f\"  Classification Accuracy: {classification_accuracy:.4f} (99.94%)\")\n", "print(f\"  Retrieval Accuracy (Top-1): {retrieval_accuracy:.4f} (99.94%)\")\n", "print(f\"  LLM Quality (BERTScore F1): {bertscore_f1:.4f} (91.26%)\")\n", "\n", "# Method 1: Simple weighted average (routes are assumed correct)\n", "hybrid_score_simple = (det_proportion * retrieval_accuracy) + (indet_proportion * bertscore_f1)\n", "\n", "print(f\"\\n{'-'*60}\")\n", "print(\"METHOD 1: Simple Weighted Score\")\n", "print(f\"{'-'*60}\")\n", "print(f\"  Formula: (P_det \u00d7 Retrieval) + (P_indet \u00d7 BERTScore)\")\n", "print(f\"  Calculation:\")\n", "print(f\"    Deterministic contribution: {det_proportion:.3f} \u00d7 {retrieval_accuracy:.4f} = {det_proportion * retrieval_accuracy:.4f}\")\n", "print(f\"    Indeterministic contribution: {indet_proportion:.3f} \u00d7 {bertscore_f1:.4f} = {indet_proportion * bertscore_f1:.4f}\")\n", "print(f\"\\n  Hybrid Score: {hybrid_score_simple:.4f} ({hybrid_score_simple*100:.2f}%)\")\n", "\n", "# Method 2: Factor in classification accuracy (more conservative)\n", "hybrid_score_with_classifier = classification_accuracy * hybrid_score_simple\n", "\n", "print(f\"\\n{'-'*60}\")\n", "print(\"METHOD 2: Including Classification Accuracy\")\n", "print(f\"{'-'*60}\")\n", "print(f\"  Formula: Classification \u00d7 [(P_det \u00d7 Retrieval) + (P_indet \u00d7 BERTScore)]\")\n", "print(f\"  Calculation:\")\n", "print(f\"    Base score: {hybrid_score_simple:.4f}\")\n", "print(f\"    With classification: {classification_accuracy:.4f} \u00d7 {hybrid_score_simple:.4f} = {hybrid_score_with_classifier:.4f}\")\n", "print(f\"\\n  Hybrid Score: {hybrid_score_with_classifier:.4f} ({hybrid_score_with_classifier*100:.2f}%)\")\n", "\n", "# Summary\n", "print(f\"\\n{'='*60}\")\n", "print(\"HYBRID SYSTEM SCORE SUMMARY\")\n", "print(f\"{'='*60}\")\n", "print(f\"\\nRecommended Score (Method 1): {hybrid_score_simple:.4f}\")\n", "print(f\"  Interpretation:\")\n", "print(f\"    \u2022 Deterministic path ({det_proportion*100:.1f}% of queries): {retrieval_accuracy*100:.2f}% accuracy\")\n", "print(f\"    \u2022 Indeterministic path ({indet_proportion*100:.1f}% of queries): {bertscore_f1*100:.2f}% semantic similarity\")\n", "print(f\"    \u2022 Overall weighted performance: {hybrid_score_simple*100:.2f}%\")\n", "\n", "print(f\"\\nConservative Score (Method 2): {hybrid_score_with_classifier:.4f}\")\n", "print(f\"  Factors in potential classification errors: {hybrid_score_with_classifier*100:.2f}%\")\n", "\n", "# Save scores\n", "scores = {\n", "    'test_set_distribution': {\n", "        'total': total_test,\n", "        'deterministic_count': det_count,\n", "        'indeterministic_count': indet_count,\n", "        'deterministic_proportion': float(det_proportion),\n", "        'indeterministic_proportion': float(indet_proportion)\n", "    },\n", "    'component_scores': {\n", "        'classification_accuracy': float(classification_accuracy),\n", "        'retrieval_accuracy': float(retrieval_accuracy),\n", "        'llm_bertscore_f1': float(bertscore_f1)\n", "    },\n", "    'hybrid_scores': {\n", "        'simple_weighted': float(hybrid_score_simple),\n", "        'with_classifier': float(hybrid_score_with_classifier)\n", "    }\n", "}\n", "\n", "import json\n", "output_path = '/content/drive/MyDrive/NLP_Project/results/'\n", "with open(f'{output_path}/hybrid_system_score.json', 'w') as f:\n", "    json.dump(scores, f, indent=2)\n", "\n", "print(f\"\\n\u2713 Scores saved to {output_path}/hybrid_system_score.json\")"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "GxIWebx6vk8q", "outputId": "7e930c7b-3bc9-47ec-906d-f4b7d1eed941"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["============================================================\n", "COMPUTING HYBRID SYSTEM SCORE\n", "============================================================\n", "\n", "Test Set Distribution:\n", "  Total examples: 3978\n", "  Deterministic (label=0): 1584 (39.8%)\n", "  Indeterministic (label=1): 2394 (60.2%)\n", "\n", "Component Scores:\n", "  Classification Accuracy: 0.9994 (99.94%)\n", "  Retrieval Accuracy (Top-1): 0.9994 (99.94%)\n", "  LLM Quality (BERTScore F1): 0.9126 (91.26%)\n", "\n", "------------------------------------------------------------\n", "METHOD 1: Simple Weighted Score\n", "------------------------------------------------------------\n", "  Formula: (P_det \u00d7 Retrieval) + (P_indet \u00d7 BERTScore)\n", "  Calculation:\n", "    Deterministic contribution: 0.398 \u00d7 0.9994 = 0.3980\n", "    Indeterministic contribution: 0.602 \u00d7 0.9126 = 0.5492\n", "\n", "  Hybrid Score: 0.9472 (94.72%)\n", "\n", "------------------------------------------------------------\n", "METHOD 2: Including Classification Accuracy\n", "------------------------------------------------------------\n", "  Formula: Classification \u00d7 [(P_det \u00d7 Retrieval) + (P_indet \u00d7 BERTScore)]\n", "  Calculation:\n", "    Base score: 0.9472\n", "    With classification: 0.9994 \u00d7 0.9472 = 0.9466\n", "\n", "  Hybrid Score: 0.9466 (94.66%)\n", "\n", "============================================================\n", "HYBRID SYSTEM SCORE SUMMARY\n", "============================================================\n", "\n", "Recommended Score (Method 1): 0.9472\n", "  Interpretation:\n", "    \u2022 Deterministic path (39.8% of queries): 99.94% accuracy\n", "    \u2022 Indeterministic path (60.2% of queries): 91.26% semantic similarity\n", "    \u2022 Overall weighted performance: 94.72%\n", "\n", "Conservative Score (Method 2): 0.9466\n", "  Factors in potential classification errors: 94.66%\n", "\n", "\u2713 Scores saved to /content/drive/MyDrive/NLP_Project/results//hybrid_system_score.json\n"]}]}]}